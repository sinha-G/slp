{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Import Libraries </h1>\n",
    "We import all the necessary libraries, including Optuna, PyTorch, and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Read Data </h1>\n",
    "We read the data saved in `data_processing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42768, 9, 1024)\n",
      "(42768,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# X  = np.load('../../data/training_inputs_cart_numpy_binary_1024.npy') # Stick input as cartesian coordinates.\n",
    "\n",
    "X  = np.load('C:/Users/grant/slp/data/training_inputs_cart_numpy_binary_1024.npy') # Stick input as cartesian coordinates.\n",
    "# X  = np.load('../data/training_inputs_polar_numpy_binary.npy') # Stick inputs as polar coordinates.\n",
    "# X  = np.load('../data/training_inputs_cart_numpy_binary.npy') # Stick input as cartesian coordinates.\n",
    "# X  = np.load('../data/training_inputs_polar_numpy_binary.npy') # Stick inputs as polar coordinates.\n",
    "# Load labels\n",
    "# y  = np.load('../../data/labes_is_sheik_numpy_binary_1024.npy')\n",
    "y  = np.load('C:/Users/grant/slp/data/labes_is_sheik_numpy_binary_1024.npy')\n",
    "# Load labels\n",
    "# y  = np.load('../data/labes_is_sheik_numpy_binary.npy')\n",
    "# Print shape to make sure we have what we want.\n",
    "print(X.shape)\n",
    "# print(X)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Splitting </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8554,)\n",
      "(25660,)\n",
      "(8554,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training + validation and holdout sets\n",
    "X_train_val, X_holdout, y_train_val, y_holdout = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Split training + validation set into separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "print(y_holdout.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Loader </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336\n"
     ]
    }
   ],
   "source": [
    "# Convert arrays into tensors and create dataset objects\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "holdout_dataset = TensorDataset(torch.tensor(X_holdout, dtype=torch.float32), torch.tensor(y_holdout, dtype=torch.float32))\n",
    "\n",
    "# Create data loaders\n",
    "num_batches = 32 # Can be tuned\n",
    "num_workers = 1 # Can be tuned\n",
    "\n",
    "batch_size = X.shape[0] // num_batches  # Can be tuned\n",
    "print(batch_size)\n",
    "# batch_size = 64  # Can be tuned\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    # Apply sigmoid and threshold at 0.5\n",
    "    # epsilon = 10 ** -44\n",
    "    preds = torch.sigmoid(outputs) >= 0.5\n",
    "    correct = (preds.squeeze().long() == labels.long()).float().sum()\n",
    "    return correct / labels.shape[0]\n",
    "\n",
    "# class myLoss(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, pos_weight=1):\n",
    "#       super().__init__()\n",
    "#       self.pos_weight = pos_weight\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "#       epsilon = 10 ** -44\n",
    "#       input = input.sigmoid().clamp(epsilon, 1 - epsilon)\n",
    "\n",
    "#       my_bce_loss = -1 * (self.pos_weight * target * torch.log(input)\n",
    "#                           + (1 - target) * torch.log(1 - input))\n",
    "#       add_loss = (target - 0.5) ** 2 * 4\n",
    "#       mean_loss = (my_bce_loss * add_loss).mean()\n",
    "#       return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> List the Parameters </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 3, 'conv_0_out_channels': 333, 'conv_0_kernel_size_power': 3, 'conv_0_dilation': 1, 'conv_0_out_length_reduction_exponent': 1, 'conv_0_bn': False, 'conv_0_activation': True, 'conv_1_out_channels': 488, 'conv_1_kernel_size_power': 4, 'conv_1_dilation': 1, 'conv_1_out_length_reduction_exponent': 0, 'conv_1_bn': False, 'conv_1_activation': True, 'conv_2_out_channels': 164, 'conv_2_kernel_size_power': 5, 'conv_2_dilation': 3, 'conv_2_out_length_reduction_exponent': 1, 'conv_2_bn': True, 'conv_2_activation': False, 'maxpool_kernel_exponent': 6, 'fc1_out_features': 64, 'fc2_out_features': 60}\n"
     ]
    }
   ],
   "source": [
    "params_str = \"num_conv_layers: 3, conv_0_out_channels: 333, conv_0_kernel_size_power: 3, conv_0_dilation: 1, conv_0_out_length_reduction_exponent: 1, conv_0_bn: False, conv_0_activation: True, conv_1_out_channels: 488, conv_1_kernel_size_power: 4, conv_1_dilation: 1, conv_1_out_length_reduction_exponent: 0, conv_1_bn: False, conv_1_activation: True, conv_2_out_channels: 164, conv_2_kernel_size_power: 5, conv_2_dilation: 3, conv_2_out_length_reduction_exponent: 1, conv_2_bn: True, conv_2_activation: False, maxpool_kernel_exponent: 6, fc1_out_features: 64, fc2_out_features: 60\"\n",
    "\n",
    "# Split the string by commas to get each parameter\n",
    "param_list = params_str.split(\", \")\n",
    "\n",
    "# Initialize an empty dictionary to store parameters\n",
    "params = {}\n",
    "\n",
    "# Loop through each item in the list and split by colon to get key-value pairs\n",
    "for param in param_list:\n",
    "    key, value = param.split(\": \")\n",
    "    # Convert numeric values to integers or floats, and 'True'/'False' to boolean\n",
    "    if value.isdigit():\n",
    "        value = int(value)  # Convert to integer if the value is digit\n",
    "    elif value.replace('.', '', 1).isdigit():\n",
    "        value = float(value)  # Convert to float if the value is a float\n",
    "    elif value == \"True\":\n",
    "        value = True  # Convert to boolean True\n",
    "    elif value == \"False\":\n",
    "        value = False  # Convert to boolean False\n",
    "    \n",
    "    # Add to dictionary\n",
    "    params[key] = value\n",
    "\n",
    "# Now you have a dictionary 'params' with all parameters\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define the Customizable Network </h1>\n",
    "We define the neural network architecture. We'll use Optuna to suggest hyperparameters for convolutional layers, optional ReLU activation, max pooling layers, and linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        # Fixed dropout rate (not tuned by Optuna)\n",
    "        dropout_rate = 0.5\n",
    "\n",
    "        # Convolutional layers setup\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.activations = []\n",
    "        self.poolings = []\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "\n",
    "        num_layers = params['num_conv_layers']\n",
    "        in_channels = 9  # Fixed input channel size\n",
    "\n",
    "        length_reduction_exporent_remaining = 5\n",
    "        in_length_exponent = 10\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            out_channels = params[f'conv_{i}_out_channels']\n",
    "            kernel_size_power = params[f'conv_{i}_kernel_size_power']\n",
    "            kernel_size = 2 * kernel_size_power + 1\n",
    "            dilation = params[f'conv_{i}_dilation']\n",
    "            out_length_reduction_exponent = params[f'conv_{i}_out_length_reduction_exponent']\n",
    "            stride = 2 ** out_length_reduction_exponent\n",
    "\n",
    "            length_reduction_exporent_remaining -= out_length_reduction_exponent\n",
    "            in_length_exponent -= out_length_reduction_exponent\n",
    "\n",
    "            if (out_length_reduction_exponent == 2) and (((dilation * kernel_size_power) % 2) == 1):\n",
    "                padding = dilation * kernel_size_power - 1\n",
    "            else:\n",
    "                padding = dilation * kernel_size_power\n",
    "\n",
    "            self.conv_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation))\n",
    "            in_channels = out_channels\n",
    "\n",
    "            pooling_type = params.get(f'layer_{i}_pooling_type', None)\n",
    "            if pooling_type is not None:\n",
    "                pool_kernal_size_exponent = out_length_reduction_exponent\n",
    "                if pooling_type == 1:\n",
    "                    self.poolings.append(nn.MaxPool1d(2 ** pool_kernal_size_exponent))\n",
    "                else:\n",
    "                    self.poolings.append(nn.AvgPool1d(2 ** pool_kernal_size_exponent))\n",
    "            else:\n",
    "                self.poolings.append(None)\n",
    "\n",
    "            use_bn = params[f'conv_{i}_bn']\n",
    "            self.bns.append(nn.BatchNorm1d(out_channels) if use_bn else None)\n",
    "\n",
    "            use_activation = params[f'conv_{i}_activation']\n",
    "            self.activations.append(use_activation)\n",
    "\n",
    "        kernel_exponent = params['maxpool_kernel_exponent']\n",
    "        kernel_size = 2 ** kernel_exponent\n",
    "        in_length_exponent -= kernel_exponent\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels * 2 ** in_length_exponent, params['fc1_out_features'])\n",
    "        self.fc1_dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, params['fc2_out_features'])\n",
    "        self.fc2_dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            x = conv_layer(x)\n",
    "            if self.bns[i] is not None:\n",
    "                x = self.bns[i](x)\n",
    "            if self.poolings[i] is not None:\n",
    "                x = self.poolings[i](x)\n",
    "            if self.activations[i]:\n",
    "                x = F.relu(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define the Objective Function </h1>\n",
    "We define the objective function for Optuna, which involves training and validating the model with the suggested hyperparameters to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 20/25 [08:22<02:05, 25.13s/it, Training Loss: 0.1836, Accuracy: 0.9417, Validation Loss: 0.6003, Accuracy: 0.7905]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:  96%|█████████▌| 24/25 [03:00<00:07,  7.54s/it, Training Loss: 0.1003, Accuracy: 0.9749, Validation Loss: 0.5954, Accuracy: 0.8380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Loss: 0.5568, Accuracy: 0.8448\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model with hyperparameters suggested by Optuna\n",
    "model = CustomNet(params).to(device)\n",
    "\n",
    "# print(f\"Trial {trial.number}:\")\n",
    "# print(model)\n",
    "\n",
    "# Load and prepare data (assuming X and y are already loaded)\n",
    "# Splitting, converting to TensorDataset, and DataLoader setup would go here\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # loss = criterion(outputs.squeeze(), labels)\n",
    "            loss = criterion(torch.sigmoid(outputs.squeeze()), labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def evaluate_holdout(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_accuracy\n",
    "            \n",
    "# Training loop with early stopping and tqdm progress bar\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "epochs_overfit = 0\n",
    "epochs = 25\n",
    "min_delta = 0.001\n",
    "min_overfit = .275\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=epochs, desc=\"Epochs\", position=0, leave=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
    "    \n",
    "    # Early Stopping check and progress bar update\n",
    "    if (val_loss + min_delta) < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if abs(train_loss - val_loss) < min_overfit:\n",
    "        epochs_overfit = 0\n",
    "    else:\n",
    "        epochs_overfit += 1\n",
    "\n",
    "    # Update progress bar\n",
    "    pbar.set_postfix_str(f\"Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    pbar.update(1)  # Move the progress bar by one epoch\n",
    "\n",
    "    # Check early stopping condition\n",
    "    if epochs_no_improve >= patience or epochs_overfit >= patience:\n",
    "        # pbar.write(f'Early stopping triggered at epoch {epoch + 1}')\n",
    "        pbar.close()  # Close the progress bar\n",
    "        break\n",
    "\n",
    "# Evaluate model on holdout set after training is complete (if necessary)\n",
    "holdout_loss, holdout_accuracy = evaluate_holdout(model, holdout_loader, criterion)\n",
    "# Set user attributes for final metrics\n",
    "# trial.set_user_attr('Training Loss', train_loss)\n",
    "# trial.set_user_attr('Training Accuracy', train_accuracy)\n",
    "# trial.set_user_attr('Validation Loss', val_loss)\n",
    "# trial.set_user_attr('Validation Accuracy', val_accuracy)\n",
    "# trial.set_user_attr('Holdout Loss', holdout_loss)\n",
    "# trial.set_user_attr('Holdout Accuracy', holdout_accuracy)\n",
    "\n",
    "print(f'Holdout Loss: {holdout_loss:.4f}, Accuracy: {holdout_accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define Callback Function </h1>\n",
    "We define a callback function that will be called by the Optuna study after each trial. This function will check if the current trial has a better value than the previous best and, if so, will save its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params_if_best(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        # Save the best parameters so far\n",
    "        print(f\"New best trial at trial {trial.number}: {trial.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Run the Optimization </h1>\n",
    "We create an Optuna study and then iterate the optimizer separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cryptography\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-22 08:59:52,340] A new study created in RDB with name: 2024-02-22 08:59:52 Classical CNN\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from datetime import datetime\n",
    "# import cryptography\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "current_datetime_string = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S \")\n",
    "\n",
    "study = optuna.create_study(study_name = current_datetime_string + \"Classical CNN\",\n",
    "                            direction=\"minimize\",\n",
    "                            storage = \"mysql+pymysql://root:MomentusPigs@localhost:3306/optuna_trials\")\n",
    "\n",
    "\n",
    "# storage_url = \"mysql+mysqlconnector://optuna_user:your_password@localhost/optuna_db\"\n",
    "# study = optuna.create_study(study_name=\"your_study_name\", storage=storage_url, load_if_exists=True)\n",
    "# storage_url = \"mysql+pymysql://root:MomentusPigs@localhost:33060/optuna_trials\"\n",
    "# study = optuna.create_study(direction=\"minimize\", storage=storage_url)\n",
    "\n",
    "# !optuna-dashboard mysql+pymysql://root:MomentusPigs@localhost/optuna_trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'objective' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[43mobjective\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3600\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[save_params_if_best])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Print the overall best hyperparameters\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial overall:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'objective' is not defined"
     ]
    }
   ],
   "source": [
    "study.optimize(objective, n_trials=1000, show_progress_bar=True, timeout=3600*10, callbacks=[save_params_if_best])\n",
    "\n",
    "# Print the overall best hyperparameters\n",
    "print(\"Best trial overall:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    table = PrettyTable(['Modules', 'Parameters'])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f'Total Trainable Params: {total_params}')\n",
    "    return total_params\n",
    "\n",
    "count_parameters(CustomNet(study.best_trial))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

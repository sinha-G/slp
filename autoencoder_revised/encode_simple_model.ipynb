{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on encoded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asses_model(\n",
    "    model_name, \n",
    "    y_pred, \n",
    "    y_test, \n",
    "    labels_order, \n",
    "    plot_roc=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Assess a classification model using multiple metrics, confusion matrices,\n",
    "    classification report, and optional ROC curve visualization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        The name of the model to display in plots.\n",
    "    y_pred : array-like\n",
    "        Model predictions.\n",
    "    y_test : array-like\n",
    "        True labels.\n",
    "    labels_order : list\n",
    "        Labels in a specific order you want for confusion matrices and bar charts.\n",
    "    plot_roc : bool, default = False\n",
    "        Whether to plot the ROC curves (for binary or multi-class classification).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Metrics: Accuracy & Cohen Kappa ---\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    print(f'Accuracy:          {accuracy:.4f}')\n",
    "    print(f'Cohen Kappa Score: {kappa:.4f}')\n",
    "\n",
    "    # --- 2. Classification Report ---\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=labels_order))\n",
    "\n",
    "    # --- 3. Over/Under Prediction Bar Chart ---\n",
    "    unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "    normalized_counts_pred = {\n",
    "        k: v / counts_test[np.where(unique_test == k)[0][0]] \n",
    "        for k, v in zip(unique_pred, counts_pred)\n",
    "        if k in unique_test\n",
    "    }\n",
    "\n",
    "    # Calculate the percent the model over or under predicted the labels \n",
    "    # using the specified label order\n",
    "    sorted_values = [\n",
    "        normalized_counts_pred[k] - 1 \n",
    "        if k in normalized_counts_pred \n",
    "        else 0 \n",
    "        for k in labels_order\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(2*len(labels_order), 8))\n",
    "    plt.bar(labels_order, sorted_values,\n",
    "            color=['green' if x > 0 else 'blue' for x in sorted_values])\n",
    "    plt.title(f'Percent Model {model_name} Over/Under Predicted Labels', fontsize=50)\n",
    "    plt.xlabel('Labels', fontsize=40)\n",
    "    plt.ylabel('Over/Under', fontsize=40)\n",
    "    # make the x-ticks bigger and vertical\n",
    "    plt.xticks(fontsize=30, rotation=90)\n",
    "    plt.yticks(fontsize=30)\n",
    "    # Center y-axis\n",
    "    max_extent = max(abs(min(sorted_values)), abs(max(sorted_values))) * 1.05\n",
    "    plt.ylim(-max_extent, max_extent)\n",
    "    plt.axhline(y=0, color='gray', linewidth=0.8)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 4. Confusion Matrices (Not Normalized, Normalized by True, Normalized by Pred) ---\n",
    "    for norm in [None, 'true', 'pred']:\n",
    "        plt.figure(figsize=(len(labels_order)+5, len(labels_order)+5))\n",
    "        ax = plt.gca()\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test, y_pred, \n",
    "            labels=labels_order,\n",
    "            normalize=norm, \n",
    "            xticks_rotation='vertical', \n",
    "            ax=ax\n",
    "        )\n",
    "        title_str = 'Not Normalized' if norm is None else f'Normalized by {norm}'\n",
    "        ax.set_title(f'{model_name} Confusion Matrix ({title_str})', fontsize=40)\n",
    "        ax.set_xlabel('Predicted Label', fontsize=30)\n",
    "        ax.set_ylabel('True Label', fontsize=30)\n",
    "        # make the y an x ticks bigger\n",
    "        ax.yaxis.set_tick_params(labelsize=20)\n",
    "        ax.xaxis.set_tick_params(labelsize=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- 5. (Optional) Plot ROC Curves ---\n",
    "    if plot_roc:\n",
    "        # This approach can work for both binary and multi-class problems\n",
    "        # For multi-class, we do one-vs-rest\n",
    "        try:\n",
    "            # Convert labels to one-hot encoding for multi-class\n",
    "            # If you already have binary labels (0,1) or one-hot encoding, adjust accordingly.\n",
    "            from sklearn.preprocessing import label_binarize\n",
    "            n_classes = len(labels_order)\n",
    "            y_test_binarized = label_binarize(y_test, classes=labels_order)\n",
    "            y_pred_binarized = label_binarize(y_pred, classes=labels_order)\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            for i in range(n_classes):\n",
    "                fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_binarized[:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, \n",
    "                         label=f'Class {labels_order[i]} (AUC = {roc_auc:.2f})')\n",
    "            \n",
    "            # Micro-average ROC curve\n",
    "            fpr_micro, tpr_micro, _ = roc_curve(\n",
    "                y_test_binarized.ravel(), \n",
    "                y_pred_binarized.ravel()\n",
    "            )\n",
    "            roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "            plt.plot(fpr_micro, tpr_micro,\n",
    "                     label=f'Micro-average (AUC = {roc_auc_micro:.2f})',\n",
    "                     linestyle='--', color='black')\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=30)\n",
    "            plt.ylabel('True Positive Rate', fontsize=30)\n",
    "            plt.title(f'{model_name} ROC Curves', fontsize=40)\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot ROC curves: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = ['ranked','public','mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True],\n",
    "}\n",
    "player_features = {\n",
    "    'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "\n",
    "# We will not be training with a label.\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    'feature': ['character_name']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/autoencoder_revised/../slp_package/input_dataset.py:113: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage_name</th>\n",
       "      <th>num_players</th>\n",
       "      <th>conclusive</th>\n",
       "      <th>player_character_name</th>\n",
       "      <th>player_type_name</th>\n",
       "      <th>opposing_player_type_name</th>\n",
       "      <th>player_inputs_np_sub_path</th>\n",
       "      <th>length</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\FALCO\\727e819f-8cb3-4c3f-bf0a-ceefa9e41c...</td>\n",
       "      <td>5606</td>\n",
       "      <td>FALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\FALCO\\76fe3db5-60de-46bb-8f0d-80d48822a8...</td>\n",
       "      <td>5754</td>\n",
       "      <td>FALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POKEMON_STADIUM</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>MARTH</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\MARTH\\7e6b417f-249d-4629-b6dc-2fe1d95d8f...</td>\n",
       "      <td>6213</td>\n",
       "      <td>MARTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOUNTAIN_OF_DREAMS</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FOX</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\FOX\\32305eaf-71d8-46e5-a8a1-2c7c890a9baf...</td>\n",
       "      <td>7621</td>\n",
       "      <td>FOX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\FALCO\\a5396c32-6f2c-4b88-8582-f8b875bb55...</td>\n",
       "      <td>7840</td>\n",
       "      <td>FALCO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stage_name  num_players conclusive player_character_name  \\\n",
       "0   FINAL_DESTINATION            2       True                 FALCO   \n",
       "1   FINAL_DESTINATION            2       True                 FALCO   \n",
       "2     POKEMON_STADIUM            2       True                 MARTH   \n",
       "3  FOUNTAIN_OF_DREAMS            2       True                   FOX   \n",
       "4   FINAL_DESTINATION            2       True                 FALCO   \n",
       "\n",
       "  player_type_name opposing_player_type_name  \\\n",
       "0            HUMAN                     HUMAN   \n",
       "1            HUMAN                     HUMAN   \n",
       "2            HUMAN                     HUMAN   \n",
       "3            HUMAN                     HUMAN   \n",
       "4            HUMAN                     HUMAN   \n",
       "\n",
       "                           player_inputs_np_sub_path  length labels  \n",
       "0  mango\\FALCO\\727e819f-8cb3-4c3f-bf0a-ceefa9e41c...    5606  FALCO  \n",
       "1  mango\\FALCO\\76fe3db5-60de-46bb-8f0d-80d48822a8...    5754  FALCO  \n",
       "2  mango\\MARTH\\7e6b417f-249d-4629-b6dc-2fe1d95d8f...    6213  MARTH  \n",
       "3  mango\\FOX\\32305eaf-71d8-46e5-a8a1-2c7c890a9baf...    7621    FOX  \n",
       "4  mango\\FALCO\\a5396c32-6f2c-4b88-8582-f8b875bb55...    7840  FALCO  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "dataset.dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOX               103069\n",
      "FALCO              90719\n",
      "MARTH              53728\n",
      "CAPTAIN_FALCON     38006\n",
      "SHEIK              27623\n",
      "PEACH              17438\n",
      "JIGGLYPUFF         16374\n",
      "SAMUS               9524\n",
      "ICE_CLIMBERS        6849\n",
      "GANONDORF           6655\n",
      "YOSHI               5725\n",
      "LUIGI               5230\n",
      "DR_MARIO            4202\n",
      "PIKACHU             4096\n",
      "LINK                2502\n",
      "NESS                2306\n",
      "DONKEY_KONG         2026\n",
      "GAME_AND_WATCH      1967\n",
      "MEWTWO              1775\n",
      "MARIO               1713\n",
      "YOUNG_LINK          1447\n",
      "ROY                 1272\n",
      "BOWSER               940\n",
      "KIRBY                556\n",
      "PICHU                230\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.dataset['labels'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Label   Count  Shift\n",
      "0              FOX  102551  15075\n",
      "1            FALCO   90263  12994\n",
      "2            MARTH   53538   8570\n",
      "3   CAPTAIN_FALCON   37820   5406\n",
      "4            SHEIK   27536   4950\n",
      "5            PEACH   17367   3383\n",
      "6       JIGGLYPUFF   16214   3002\n",
      "7            SAMUS    9489   2037\n",
      "8     ICE_CLIMBERS    6820   1347\n",
      "9        GANONDORF    6611   1013\n",
      "10           YOSHI    5704   1022\n",
      "11           LUIGI    5210    971\n",
      "12        DR_MARIO    4177    761\n",
      "13         PIKACHU    4067    762\n",
      "14            LINK    2489    479\n",
      "15            NESS    2291    523\n",
      "16     DONKEY_KONG    2009    363\n",
      "17  GAME_AND_WATCH    1949    288\n",
      "18          MEWTWO    1758    407\n",
      "19           MARIO    1710    327\n",
      "20      YOUNG_LINK    1430    282\n",
      "21             ROY    1262    223\n",
      "22          BOWSER     934    191\n",
      "23           KIRBY     531    106\n",
      "24           PICHU     227     41\n"
     ]
    }
   ],
   "source": [
    "labels_order =  dataset.number_of_segments_per_game(3600,40000)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           player_inputs_np_sub_path  length  num_segments  \\\n",
      "0  public\\FALCO\\c4923e7a-eb2b-4d7a-844d-0642caa17...   16592             1   \n",
      "1  ranked\\FALCO\\34c40adf-7386-4603-9c9e-ee2f8e2ba...   16590             1   \n",
      "2  ranked\\FALCO\\db3afa43-f3cf-4cba-94ce-105f6670b...   16569             1   \n",
      "3  ranked\\FALCO\\e7d480eb-7a28-4741-b49e-917544a11...   16564             1   \n",
      "4  public\\FALCO\\9f3fda37-6f5f-4db7-aeee-516bc89e8...   16540             1   \n",
      "\n",
      "  labels  encoded_labels  \n",
      "0  FALCO               4  \n",
      "1  FALCO               4  \n",
      "2  FALCO               4  \n",
      "3  FALCO               4  \n",
      "4  FALCO               4  \n"
     ]
    }
   ],
   "source": [
    "train_df, test_df  = dataset.train_test_split_dataframes(test_ratio = .20, val = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 'FALCO', 14: 'MARTH', 5: 'FOX', 1: 'CAPTAIN_FALCON', 22: 'SHEIK', 9: 'JIGGLYPUFF', 3: 'DR_MARIO', 23: 'YOSHI', 19: 'PIKACHU', 21: 'SAMUS', 8: 'ICE_CLIMBERS', 13: 'MARIO', 12: 'LUIGI', 17: 'PEACH', 0: 'BOWSER', 2: 'DONKEY_KONG', 20: 'ROY', 7: 'GANONDORF', 18: 'PICHU', 11: 'LINK', 16: 'NESS', 24: 'YOUNG_LINK', 10: 'KIRBY', 6: 'GAME_AND_WATCH', 15: 'MEWTWO'}\n"
     ]
    }
   ],
   "source": [
    "labels_unique = train_df['labels'].unique()\n",
    "encoded_labels_unique = train_df['encoded_labels'].unique()\n",
    "label_decoder = zip(labels_unique, encoded_labels_unique)\n",
    "label_decoder = dict(zip(encoded_labels_unique, labels_unique)) \n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2048, 25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, encoder, criterion, optimizer, loaders, device, num_epochs=1):\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        \n",
    "        for inputs, labels in train_loader_tqdm:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            encoded_gpu = encoder(inputs)\n",
    "            # Resets the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "            outputs = model(encoded_gpu)\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            \n",
    "            # Scales loss and calls backward() to create scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "\n",
    "            # Update progress\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_loader_tqdm.set_postfix(loss=(train_loss / total), accuracy=(100.0 * train_correct / total))\n",
    "            \n",
    "            \n",
    "def predict(model, encoder, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and returns predictions as a numpy array,\n",
    "    displaying a progress bar during the evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        device (torch.device): The device to evaluate on.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Numpy array of predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm(total=len(test_loader), desc=\"Evaluating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            encoded_gpu = encoder(inputs)\n",
    "            outputs = model(encoded_gpu)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())  # Collecting predictions\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()  # Ensure the progress bar is closed after the loop\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading and optionally transforming game segments from compressed NumPy files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must include the following columns:\n",
    "          - 'player_inputs_np_sub_path': file paths to the compressed NumPy files\n",
    "          - 'encoded_labels': integer-encoded labels\n",
    "          - 'segment_start_index': start index for each segment\n",
    "          - 'segment_length': length of each segment in frames\n",
    "    transform : bool, default=False\n",
    "        If True, applies a specific transformation to each loaded segment (e.g., scaling analog inputs).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=False):\n",
    "        self.file_paths = df['player_inputs_np_sub_path'].to_numpy()\n",
    "        self.encoded_labels = df['encoded_labels'].to_numpy()\n",
    "        self.segment_start_index = df['segment_start_index'].to_numpy()\n",
    "        self.segment_length = df['segment_length'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "        # Optional: you can store a shape attribute to document the shape \n",
    "        # of data that __getitem__ will return. \n",
    "        # We'll initialize it to None and fill it when the first item is fetched.\n",
    "        self.sample_shape = None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample (and possibly label) from the dataset at index 'idx'.\n",
    "\n",
    "        In this custom dataset:\n",
    "          1. We open the compressed file corresponding to self.file_paths[idx].\n",
    "          2. We slice out the segment using self.segment_start_index[idx] and\n",
    "             self.segment_length[idx].\n",
    "          3. If transform=True, we apply additional transformations (shifting, scaling, etc.).\n",
    "          4. We return a PyTorch tensor containing the processed segment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the sample to be fetched.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor representing the selected segment, after optional transformations.\n",
    "        \"\"\"\n",
    "        # Load the uncompressed file\n",
    "        file_path = self.file_paths[idx].replace('\\\\', '/')\n",
    "        with gzip.open('/workspace/melee_project_data/input_np/' + file_path, 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "\n",
    "        # Determine slice boundaries\n",
    "        start = int(self.segment_start_index[idx])\n",
    "        end = start + int(self.segment_length[idx])\n",
    "\n",
    "        # Extract the segment\n",
    "        segment = segment[:, start:end]\n",
    "\n",
    "        # Apply transformations if requested\n",
    "        if self.transform:\n",
    "            # Example transformation: shape = (9+4, 3600) for some reason\n",
    "            transformed = np.zeros((9 + 4, segment.shape[1]))\n",
    "\n",
    "            # 1) Shift and scale analog inputs to [0, 1]\n",
    "            analog_transformed = np.copy(segment[0:4])\n",
    "            analog_transformed[analog_transformed > 0] -= 0.2875 + 0.0125\n",
    "            analog_transformed[analog_transformed < 0] += 0.2875 - 0.0125\n",
    "            analog_transformed *= 0.5 / 0.725\n",
    "            analog_transformed += 0.5\n",
    "            transformed[0:4] = analog_transformed\n",
    "\n",
    "            # 2) Mark positions where analog inputs are zero\n",
    "            transformed[4:8] += (segment[:4] == 0)\n",
    "\n",
    "            # # Possible additional transformations:\n",
    "            # # 3) Some custom “transition” measure on last 5 rows\n",
    "            # prepend = np.expand_dims(segment[-5:, 0], axis=1)\n",
    "            # transitions = np.abs(np.diff(segment[-5:], axis=1, prepend=prepend))\n",
    "            # transformed[8:13] += transitions\n",
    "\n",
    "            # 4) Add button inputs\n",
    "            transformed[-5:] += segment[-5:]\n",
    "\n",
    "        else:\n",
    "            # If not transforming, produce something simpler (9 x 60)\n",
    "            transformed = np.zeros((9, segment.shape[1]))\n",
    "\n",
    "            # 1) Shift and scale analog inputs to [0, 1]\n",
    "            analog_transformed = np.copy(segment[0:4])\n",
    "            analog_transformed[analog_transformed > 0] -= 0.2875 + 0.0125\n",
    "            analog_transformed[analog_transformed < 0] += 0.2875 - 0.0125\n",
    "            analog_transformed *= 0.5 / 0.725\n",
    "            analog_transformed += 0.5\n",
    "            transformed[0:4] = analog_transformed\n",
    "\n",
    "            # 2) Transform the Trigger to 0/1\n",
    "            transformed[-5] += (segment[-5] > 0.5)\n",
    "\n",
    "            # 3) The last 4 rows become button inputs\n",
    "            transformed[-4:] += segment[-4:]\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        segment_tensor = torch.from_numpy(transformed).float()\n",
    "\n",
    "        # Optionally store the shape of the output the first time __getitem__ is called\n",
    "        if self.sample_shape is None:\n",
    "            self.sample_shape = segment_tensor.shape\n",
    "\n",
    "        return segment_tensor\n",
    "\n",
    "\n",
    "def prepare_data_loaders(train_df, test_df, batch_size, num_workers,  transform = True):\n",
    "    \"\"\"\n",
    "    Creates DataLoader objects for training and testing sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df : pd.DataFrame\n",
    "    test_df : pd.DataFrame\n",
    "    batch_size : int\n",
    "    num_workers : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict of DataLoader\n",
    "        'train' -> training DataLoader\n",
    "        'test' -> testing DataLoader\n",
    "    \"\"\"\n",
    "    train_dataset = TrainingDataset(train_df, transform=transform)\n",
    "    test_dataset = TrainingDataset(test_df, transform=transform)\n",
    "\n",
    "    loaders = {\n",
    "        'train': DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        ),\n",
    "        'test': DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "    }\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 3600])\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# trasform = True adds binary features corresponding to when the analog inputs are 0.\n",
    "transform = True\n",
    "# bce_scale is a tunable parameter that scales the binary cross-entropy loss.\n",
    "bce_scale = 100\n",
    "# weighted = True weights the loss function to account for the imbalance of the button being pressed.\n",
    "weighted = True\n",
    "\n",
    "loaders = prepare_data_loaders(train_df, test_df, batch_size=16, num_workers=20,  transform=transform)\n",
    "# Grab one item (segment tensor) from the train dataset\n",
    "train_dataset = loaders['train'].dataset\n",
    "first_item = train_dataset[0]\n",
    "print(first_item.size())\n",
    "channels = first_item.size(0)\n",
    "print(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Convolutional_Autoencoder_Model import ResNet_Encoder\n",
    "import torch_tensorrt\n",
    "\n",
    "encoder = ResNet_Encoder(channels).to('cuda')\n",
    "encoder.eval()\n",
    "# encoder = torch.compile(encoder, backend=\"torch_tensorrt\",mode = 'max-autotune')\n",
    "encoder.eval()\n",
    "\n",
    "model = Model().to('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/50000 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 173, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      7\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predict(model, loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, encoder, criterion, optimizer, loaders, device, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m train_loader_tqdm \u001b[38;5;241m=\u001b[39m tqdm(loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader_tqdm:\n\u001b[1;32m     12\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     encoded_gpu \u001b[38;5;241m=\u001b[39m encoder(inputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 121, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 173, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "epochs = 1\n",
    "train_model(model, encoder, criterion, optimizer, loaders, 'cuda', epochs)\n",
    "y_pred = predict(model, loaders['test'], 'cuda' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

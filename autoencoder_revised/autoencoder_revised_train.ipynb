{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised Autoencoder\n",
    "In this notebook we attempt to train an autoencoder on the input data for one minute segments. Two main challenges are that the input data is a mix of analog and digital signals and that buttons are not pressed much more than they are pressed. This necessitaies a custom loss function.\n",
    "\n",
    "## Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training and test sets\n",
    "We use the InputDataset class to create the training and test sets. Unlike the classification with ResNet, we do not create a balacened dataset (with respect to the character the player is playing).  We still need to specify the labels, but they are not used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = ['ranked','public','mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True],\n",
    "}\n",
    "player_features = {\n",
    "    'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "\n",
    "# We will not be training with a label.\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    'feature': ['character_name']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/autoencoder_revised/../slp_package/input_dataset.py:113: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage_name</th>\n",
       "      <th>num_players</th>\n",
       "      <th>conclusive</th>\n",
       "      <th>player_character_name</th>\n",
       "      <th>player_type_name</th>\n",
       "      <th>opposing_player_type_name</th>\n",
       "      <th>player_inputs_np_sub_path</th>\n",
       "      <th>length</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\FALCO\\727e819f-8cb3-4c3f-bf0a-ceefa9e41c...</td>\n",
       "      <td>5606</td>\n",
       "      <td>FALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\FALCO\\76fe3db5-60de-46bb-8f0d-80d48822a8...</td>\n",
       "      <td>5754</td>\n",
       "      <td>FALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POKEMON_STADIUM</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>MARTH</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\MARTH\\7e6b417f-249d-4629-b6dc-2fe1d95d8f...</td>\n",
       "      <td>6213</td>\n",
       "      <td>MARTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOUNTAIN_OF_DREAMS</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FOX</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\FOX\\32305eaf-71d8-46e5-a8a1-2c7c890a9baf...</td>\n",
       "      <td>7621</td>\n",
       "      <td>FOX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\FALCO\\a5396c32-6f2c-4b88-8582-f8b875bb55...</td>\n",
       "      <td>7840</td>\n",
       "      <td>FALCO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stage_name  num_players conclusive player_character_name  \\\n",
       "0   FINAL_DESTINATION            2       True                 FALCO   \n",
       "1   FINAL_DESTINATION            2       True                 FALCO   \n",
       "2     POKEMON_STADIUM            2       True                 MARTH   \n",
       "3  FOUNTAIN_OF_DREAMS            2       True                   FOX   \n",
       "4   FINAL_DESTINATION            2       True                 FALCO   \n",
       "\n",
       "  player_type_name opposing_player_type_name  \\\n",
       "0            HUMAN                     HUMAN   \n",
       "1            HUMAN                     HUMAN   \n",
       "2            HUMAN                     HUMAN   \n",
       "3            HUMAN                     HUMAN   \n",
       "4            HUMAN                     HUMAN   \n",
       "\n",
       "                           player_inputs_np_sub_path  length labels  \n",
       "0  mango\\FALCO\\727e819f-8cb3-4c3f-bf0a-ceefa9e41c...    5606  FALCO  \n",
       "1  mango\\FALCO\\76fe3db5-60de-46bb-8f0d-80d48822a8...    5754  FALCO  \n",
       "2  mango\\MARTH\\7e6b417f-249d-4629-b6dc-2fe1d95d8f...    6213  MARTH  \n",
       "3  mango\\FOX\\32305eaf-71d8-46e5-a8a1-2c7c890a9baf...    7621    FOX  \n",
       "4  mango\\FALCO\\a5396c32-6f2c-4b88-8582-f8b875bb55...    7840  FALCO  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "dataset.dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify the segment length and the shift we wish to take. We choose a shift of 1800 frames meaning the segments overlap by half. The training and test set do not contain segments from the same game, this avoids data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1472456, 8)\n",
      "(81775, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_inputs_np_sub_path</th>\n",
       "      <th>length</th>\n",
       "      <th>num_segments</th>\n",
       "      <th>labels</th>\n",
       "      <th>encoded_labels</th>\n",
       "      <th>segment_index</th>\n",
       "      <th>segment_start_index</th>\n",
       "      <th>segment_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...</td>\n",
       "      <td>11040</td>\n",
       "      <td>5</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...</td>\n",
       "      <td>11040</td>\n",
       "      <td>5</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1800</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...</td>\n",
       "      <td>11040</td>\n",
       "      <td>5</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3600</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...</td>\n",
       "      <td>11040</td>\n",
       "      <td>5</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5400</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...</td>\n",
       "      <td>11040</td>\n",
       "      <td>5</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7200</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           player_inputs_np_sub_path  length  num_segments  \\\n",
       "0  ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...   11040             5   \n",
       "1  ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...   11040             5   \n",
       "2  ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...   11040             5   \n",
       "3  ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...   11040             5   \n",
       "4  ranked\\FALCO\\e34756b0-46b5-4db2-a3c4-453aec014...   11040             5   \n",
       "\n",
       "  labels  encoded_labels  segment_index  segment_start_index  segment_length  \n",
       "0  FALCO               4              0                    0            3600  \n",
       "1  FALCO               4              1                 1800            3600  \n",
       "2  FALCO               4              2                 3600            3600  \n",
       "3  FALCO               4              3                 5400            3600  \n",
       "4  FALCO               4              4                 7200            3600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_length = 3600\n",
    "shift = 1800\n",
    "\n",
    "train_df, test_df = dataset.all_segments_train_test_split_dataframes(segment_length,shift=shift, proportion_of_segments=1, test_ratio = .1, val = False)\n",
    "# porportion = 1\n",
    "# train_df = train_df.sample(frac=porportion, random_state = 42)\n",
    "porportion = .5\n",
    "test_df = test_df.sample(frac=porportion, random_state = 42)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "We have a mix of binary and semi-continuous data.\n",
    "\n",
    "### The Analog Inputs\n",
    "The analog sticks take values between $[-1, -0.2875]\\cup\\{0\\}\\cup[0.2875, 1]$ in increments of .0125. There is a deadzone around 0. The values are not uniformly distributed, there is more values at the edges of the circle. The distribution of the values is shown below (we remove the (0,0) value so that the other values show up on the plots).\n",
    "![Analog Sticks](stick_hist.png)\n",
    "We transform the analog inputs in the TrainingDataset class to evenly spaced in the interval $[-1, 1]$.\n",
    "```python\n",
    " # 1) Shift and scale analog inputs to [0, 1]\n",
    "analog_transformed = np.copy(segment[0:4])\n",
    "analog_transformed[analog_transformed > 0] -= 0.2875 + 0.0125\n",
    "analog_transformed[analog_transformed < 0] += 0.2875 - 0.0125\n",
    "analog_transformed *= 0.5 / 0.725\n",
    "analog_transformed += 0.5\n",
    "transformed[0:4] = analog_transformed\n",
    "\n",
    "```\n",
    "When we asses the model's performance, we care that the model gets within the bins of the analog inputs or how many bins we are away from the target.\n",
    "```python\n",
    "integer_stick_targets = np.round(target[:,0:4] / 0.008620689655172415 ).astype(np.int32)\n",
    "integer_stick_pred = np.round(pred[:,0:4] / 0.008620689655172415).astype(np.int32)\n",
    "```\n",
    "Because the model has the most trouble predicting the J-Stick values (followed by the C-Stick values), we engineer a feature that is 1 if the stick value is zero on a particular axis (this is optional).\n",
    "```python\n",
    "# 2) Mark positions where analog inputs are zero\n",
    "transformed[4:8] += (segment[:4] == 0)\n",
    "```\n",
    "### The Digital Inputs\n",
    "The digital inputs are binary, but the buttons are not pressed much more than they are pressed. We do not do perform any transformations on those inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading and optionally transforming game segments from compressed NumPy files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must include the following columns:\n",
    "          - 'player_inputs_np_sub_path': file paths to the compressed NumPy files\n",
    "          - 'encoded_labels': integer-encoded labels\n",
    "          - 'segment_start_index': start index for each segment\n",
    "          - 'segment_length': length of each segment in frames\n",
    "    transform : bool, default=False\n",
    "        If True, applies a specific transformation to each loaded segment (e.g., scaling analog inputs).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=False):\n",
    "        self.file_paths = df['player_inputs_np_sub_path'].to_numpy()\n",
    "        self.encoded_labels = df['encoded_labels'].to_numpy()\n",
    "        self.segment_start_index = df['segment_start_index'].to_numpy()\n",
    "        self.segment_length = df['segment_length'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "        # Optional: you can store a shape attribute to document the shape \n",
    "        # of data that __getitem__ will return. \n",
    "        # We'll initialize it to None and fill it when the first item is fetched.\n",
    "        self.sample_shape = None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample (and possibly label) from the dataset at index 'idx'.\n",
    "\n",
    "        In this custom dataset:\n",
    "          1. We open the compressed file corresponding to self.file_paths[idx].\n",
    "          2. We slice out the segment using self.segment_start_index[idx] and\n",
    "             self.segment_length[idx].\n",
    "          3. If transform=True, we apply additional transformations (shifting, scaling, etc.).\n",
    "          4. We return a PyTorch tensor containing the processed segment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the sample to be fetched.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor representing the selected segment, after optional transformations.\n",
    "        \"\"\"\n",
    "        # Load the uncompressed file\n",
    "        file_path = self.file_paths[idx].replace('\\\\', '/')\n",
    "        with gzip.open('/workspace/melee_project_data/input_np/' + file_path, 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "\n",
    "        # Determine slice boundaries\n",
    "        start = int(self.segment_start_index[idx])\n",
    "        end = start + int(self.segment_length[idx])\n",
    "\n",
    "        # Extract the segment\n",
    "        segment = segment[:, start:end]\n",
    "\n",
    "        # Apply transformations if requested\n",
    "        if self.transform:\n",
    "            # Example transformation: shape = (9+4, 3600) for some reason\n",
    "            transformed = np.zeros((9 + 4, int(self.segment_length[idx])))\n",
    "\n",
    "            # 1) Shift and scale analog inputs to [0, 1]\n",
    "            analog_transformed = np.copy(segment[0:4])\n",
    "            analog_transformed[analog_transformed > 0] -= 0.2875 + 0.0125\n",
    "            analog_transformed[analog_transformed < 0] += 0.2875 - 0.0125\n",
    "            analog_transformed *= 0.5 / 0.725\n",
    "            analog_transformed += 0.5\n",
    "            transformed[0:4] = analog_transformed\n",
    "\n",
    "            # 2) Mark positions where analog inputs are zero\n",
    "            transformed[4:8] += (segment[:4] == 0)\n",
    "\n",
    "            # # Possible additional transformations:\n",
    "            # # 3) Some custom “transition” measure on last 5 rows\n",
    "            # prepend = np.expand_dims(segment[-5:, 0], axis=1)\n",
    "            # transitions = np.abs(np.diff(segment[-5:], axis=1, prepend=prepend))\n",
    "            # transformed[8:13] += transitions\n",
    "\n",
    "            # 4) Add button inputs\n",
    "            transformed[-5:] += segment[-5:]\n",
    "\n",
    "        else:\n",
    "            # If not transforming, produce something simpler (9 x 60)\n",
    "            transformed = np.zeros((9, int(self.segment_length[idx])))\n",
    "\n",
    "            # 1) Shift and scale analog inputs to [0, 1]\n",
    "            analog_transformed = np.copy(segment[0:4])\n",
    "            analog_transformed[analog_transformed > 0] -= 0.2875 + 0.0125\n",
    "            analog_transformed[analog_transformed < 0] += 0.2875 - 0.0125\n",
    "            analog_transformed *= 0.5 / 0.725\n",
    "            analog_transformed += 0.5\n",
    "            transformed[0:4] = analog_transformed\n",
    "\n",
    "            # 2) Transform the Trigger to 0/1\n",
    "            transformed[-5] += (segment[-5] > 0.5)\n",
    "\n",
    "            # 3) The last 4 rows become button inputs\n",
    "            transformed[-4:] += segment[-4:]\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        segment_tensor = torch.from_numpy(transformed).float()\n",
    "\n",
    "        # Optionally store the shape of the output the first time __getitem__ is called\n",
    "        if self.sample_shape is None:\n",
    "            self.sample_shape = segment_tensor.shape\n",
    "\n",
    "        return segment_tensor\n",
    "\n",
    "\n",
    "def prepare_data_loaders(train_df, test_df, batch_size, num_workers,  transform = True):\n",
    "    \"\"\"\n",
    "    Creates DataLoader objects for training and testing sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df : pd.DataFrame\n",
    "    test_df : pd.DataFrame\n",
    "    batch_size : int\n",
    "    num_workers : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict of DataLoader\n",
    "        'train' -> training DataLoader\n",
    "        'test' -> testing DataLoader\n",
    "    \"\"\"\n",
    "    train_dataset = TrainingDataset(train_df, transform=transform)\n",
    "    test_dataset = TrainingDataset(test_df, transform=transform)\n",
    "\n",
    "    loaders = {\n",
    "        'train': DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        ),\n",
    "        'test': DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "    }\n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predict\n",
    "Our training loop includes a progress bar that updates every five minutes of traning. We display the batches per second (this is useful for comparinig how much of a difference compiling the model makes). The progress bar keeps track of the loss for each of over the previous 5 minutes as well as the best loss the model achived. It has a patience that keeps track of the number of 5 minute intervals that the loss has not improved.\n",
    "\n",
    "We also display the maximum and minimum of both the gradient and the parameters. This is a holdover from debugging the model where we learned that we cannot use mixed precision traning to train the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_virtual_epochs(model, criterion, optimizer, loaders, device, channels, segment_length, num_epochs=1, bce_scale=100):\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=15, factor=0.1)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    vepoch_total = 0\n",
    "    vepoch_loss_sum = 0\n",
    "    best_vepoch_loss = float('inf')\n",
    "    early_stopping_patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        virtual_epoch_start_time = time.time()\n",
    "\n",
    "        # Initialize variables for tracking gradient and parameter stats\n",
    "        grad_max = float('-inf')\n",
    "        grad_min = float('inf')\n",
    "        param_max = float('-inf')\n",
    "        param_min = float('inf')\n",
    "\n",
    "        for batch_number, target_cpu in enumerate(train_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output_gpu = model(target_gpu)\n",
    "            loss = criterion(output_gpu, target_gpu) / (channels * segment_length * target_cpu.size(0))\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # Track max and min of gradients\n",
    "            batch_grad_max = max((p.grad.max().item() for p in model.parameters() if p.grad is not None), default=grad_max)\n",
    "            batch_grad_min = min((p.grad.min().item() for p in model.parameters() if p.grad is not None), default=grad_min)\n",
    "            grad_max = max(grad_max, batch_grad_max)\n",
    "            grad_min = min(grad_min, batch_grad_min)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            vepoch_total += target_cpu.size(0)\n",
    "            vepoch_loss_sum += loss.item() * target_cpu.size(0)\n",
    "\n",
    "            if time.time() - virtual_epoch_start_time > 60*5:\n",
    "                vepoch_loss = vepoch_loss_sum / vepoch_total\n",
    "                if best_vepoch_loss > vepoch_loss:\n",
    "                    best_vepoch_loss = vepoch_loss\n",
    "                else:\n",
    "                    early_stopping_patience += 1\n",
    "\n",
    "                # Calculate max and min of model parameters at the end of the virtual epoch\n",
    "                param_max = max(p.data.max().item() for p in model.parameters())\n",
    "                param_min = min(p.data.min().item() for p in model.parameters())\n",
    "\n",
    "                train_loader_tqdm.set_postfix(\n",
    "                    Best=f'{best_vepoch_loss * bce_scale:.10f}',\n",
    "                    Vepoch=f'{vepoch_loss * bce_scale:.10f}',\n",
    "                    patience=early_stopping_patience,\n",
    "                    Grad_Max=grad_max,\n",
    "                    Grad_Min=grad_min,\n",
    "                    Param_Max=param_max,\n",
    "                    Param_Min=param_min\n",
    "                )\n",
    "                # print('Grad Max:', grad_max, ' Grad Min:', grad_min)\n",
    "                virtual_epoch_start_time = time.time()\n",
    "                vepoch_total = 0\n",
    "                vepoch_loss_sum = 0\n",
    "                grad_max = float('-inf')  # Reset for next virtual epoch\n",
    "                grad_min = float('inf')   # Reset for next virtual epoch\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def predict(model, loaders, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loader_tqdm = tqdm(loaders[loader], unit='batch')\n",
    "        \n",
    "        for _, target_cpu in enumerate(eval_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            output_gpu = model(target_gpu)\n",
    "            # output_gpu = torch.sigmoid(output_gpu)\n",
    "            \n",
    "            predictions.append(torch.sigmoid(output_gpu).cpu().numpy())\n",
    "            targets.append(target_cpu.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    return predictions, targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Function Explanation\n",
    "\n",
    "Our dataset contains a mixture of analog and binary signals, so we need a loss function that can handle both types effectively. In the code snippet, we define a `CustomLoss` class that does the following:\n",
    "\n",
    "1. **Analog Channels (first 4 channels) with MSE-like Objective**  \n",
    "   - We want our network to reconstruct the analog stick values in a way that maps closely back to the original controller bins.  \n",
    "   - Because the model outputs logits that get fed through a sigmoid for the analog channels, we first apply:\n",
    "     ```python\n",
    "     torch.sigmoid(pred[:, 0:4, :])\n",
    "     ```\n",
    "     to constrain predictions to the range \\([0, 1]\\).  \n",
    "   - We then compare that to the ground-truth analog values (also in \\([0, 1]\\)), but we scale the difference by a factor \\(\\tfrac{1}{5 \\times 0.0086206896}\\). This factor can be interpreted as an approximation of the “bin width” for the analog sticks and a further manual scaling to ensure the MSE term is on a comparable scale to the binary loss term.\n",
    "   - Finally, we take the squared difference and feed it to a BCE-like criterion (with `zeros` as target). This construction effectively pushes the scaled difference toward zero. Although slightly unorthodox in its implementation (since we typically see `nn.MSELoss` directly), the outcome is similar: small differences between predictions and targets lead to a smaller loss, encouraging precise analog reconstruction.\n",
    "   \n",
    "   We apply log to the mse term for the anolog channels to make the loss less sensitive to small errors. We see in the gragh below that $x^2$ (green) has does not level off near zero, but $\\log(x^2)$ (blue) does. We prefer the leveling off because we do not want the model to be penalized too much for small errors (once the model gets within $0.008620689655172415$ of the target, we round to the target).\n",
    "\n",
    "   ![Log MSE](quadratic_vs_log_quadratic.png)\n",
    "\n",
    "2. **Binary Channels (remaining channels) with BCE**  \n",
    "   - The rest of the channels are binary (buttons pressed or not). We use a binary cross-entropy loss to measure how well the model predicts these button states:\n",
    "     ```python\n",
    "     self.BCE_buttons(pred[:, 4:, :], target[:, 4:, :])\n",
    "     ```\n",
    "   - Because some buttons are pressed much more (or less) frequently than others, we use `pos_weight` for class-imbalance correction if `weighted=True`. This gives the model a stronger penalty for misclassifying the minority (pressed) cases, preventing it from trivially predicting zeros.\n",
    "\n",
    "3. **Combining the Losses**  \n",
    "   - We sum the analog-channel loss and the binary-channel loss into a single scalar.  \n",
    "   - The final loss ensures the model learns to reconstruct both the analog inputs (through the scaled MSE-like term) and the binary inputs (through the BCE term).  \n",
    "   - The hyperparameter `bce_scale` (or an equivalent division factor) can be adjusted to balance the importance of reconstructing analog vs. binary channels.\n",
    "\n",
    "Overall, this custom loss function is designed to tackle two key challenges: (1) making sure the model can accurately generate analog outputs that match the discrete bins of the controller sticks, and (2) dealing with significant class imbalance in the binary button-press signals. By scaling and weighting different components of the loss, we aim to encourage the network to pay attention to both the analog and digital elements of the signal without allowing one to dominate the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, bce_scale=100, transform=False, weighted=False, channels=13, segment_length=3600):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "        # Fraction of times each button is pressed in your sample\n",
    "        buttons_sample_mean = [\n",
    "            0.16908772957310006,  # TRIGGER_LOGICAL\n",
    "            0.008974353071937505, # Z\n",
    "            0.060945588829374495, # A\n",
    "            0.04591526858731047,  # B\n",
    "            0.09663690337362206   # X_or_Y\n",
    "        ]\n",
    "        # If transform == True, you also have additional ones for jstick/cstick?\n",
    "        trigger_logical_sample_mean = [\n",
    "            0.45849791926398437,  # JSTICK_X_LOGICAL\n",
    "            0.6879025510132348,   # JSTICK_Y_LOGICAL\n",
    "            0.9726537459234259,   # CSTICK_X_LOGICAL\n",
    "            0.971675825912117     # CSTICK_Y_LOGICAL\n",
    "        ]\n",
    "\n",
    "        # Create pos_weight or bce_weights depending on your logic\n",
    "        if transform:\n",
    "            # Merge your two sets if needed\n",
    "            sample_means = trigger_logical_sample_mean + buttons_sample_mean\n",
    "        else:\n",
    "            sample_means = buttons_sample_mean\n",
    "\n",
    "        # pos_weight for each dimension: (1 - p) / p\n",
    "        pos_weight_vals = np.zeros((channels-4, segment_length))\n",
    "        for i, mean in enumerate(sample_means):\n",
    "            p_pos = mean\n",
    "            p_neg = 1.0 - mean\n",
    "            \n",
    "            pos_weight_vals[i,:] += p_neg / p_pos\n",
    "        pos_weight_tensor = torch.tensor(pos_weight_vals, dtype=torch.float, device='cuda')\n",
    "\n",
    "        if weighted:\n",
    "            # Use pos_weight instead of 'weight'\n",
    "            self.BCE_buttons = nn.BCEWithLogitsLoss(reduction='sum', pos_weight=pos_weight_tensor)\n",
    "        else:\n",
    "            self.BCE_buttons = nn.BCELoss(reduction='sum')\n",
    "\n",
    "        # Save the other components\n",
    "        self.bce_scale = bce_scale\n",
    "        self.BCE_sticks = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        pred, target shape: (B, Channels, T)\n",
    "        We'll assume:\n",
    "          - pred[:, 0:4, :] are analog predictions (MSE)\n",
    "          - pred[:, 4:, :] are button predictions (BCE)\n",
    "        \"\"\"\n",
    "        # 1) MSE for first 4 analog channels\n",
    "        mse_loss = ((torch.sigmoid(pred[:, 0:4, :])- target[:, 0:4, :]) / (5 * 0.008620689623058)).pow(2) / 2\n",
    "        zeros = torch.zeros_like(mse_loss)\n",
    "        bce_loss_sticks = self.BCE_sticks(mse_loss, zeros)\n",
    "        # mse_loss_cstick = self.MSE(pred[:, 2:4, :], target[:, 2:4, :]) \n",
    "        # 2) BCE for the rest\n",
    "        bce_loss_buttons = self.BCE_buttons(pred[:, 4:, :], target[:, 4:, :])\n",
    "\n",
    "        # Scale & return combined\n",
    "        return  bce_loss_buttons + bce_loss_sticks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# trasform = True adds binary features corresponding to when the analog inputs are 0.\n",
    "transform = True\n",
    "# bce_scale is a tunable parameter that scales the binary cross-entropy loss.\n",
    "bce_scale = 1\n",
    "# weighted = True weights the loss function to account for the imbalance of the button being pressed.\n",
    "weighted = True\n",
    "\n",
    "loaders = prepare_data_loaders(train_df, test_df, batch_size=16, num_workers=20,  transform=transform)\n",
    "# Grab one item (segment tensor) from the train dataset\n",
    "train_dataset = loaders['train'].dataset\n",
    "first_item = train_dataset[0]\n",
    "channels = first_item.size(0)\n",
    "\n",
    "from Convolutional_Autoencoder_Model import ResNet_Autoencoder\n",
    "# Initialize the model\n",
    "model = ResNet_Autoencoder(channels)\n",
    "state_dict = torch.load('/workspace/melee_project_data/autoencoder_models/autoencoder_revised_one_epoch_4.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.cuda()\n",
    "model = torch.compile(model, mode='max-autotune')\n",
    "# With the size of an input we can get a model summary.\n",
    "summary(model, input_size=(channels, segment_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We train the model for an epoch (which takes about three hours), evaluate it, then train another epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 92029/92029 [3:09:59<00:00,  8.07batch/s, Best=0.4978289099, Grad_Max=4.41, Grad_Min=-6.17, Param_Max=3.56, Param_Min=-2.35, Vepoch=0.4978289099, patience=23]  \n"
     ]
    }
   ],
   "source": [
    "criterion = CustomLoss(bce_scale=bce_scale, transform=transform, weighted=weighted, channels=channels, segment_length=segment_length)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 1\n",
    "\n",
    "# This seems to sometimes help\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Train the model\n",
    "train_model_with_virtual_epochs(model, criterion, optimizer, loaders, 'cuda', channels, segment_length, num_epochs, bce_scale=bce_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "We need to save the model in its original form so that we can load it later. If we just saved the state dictionoary of the compiled model, we would not be able to load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = model._orig_mod\n",
    "torch.save(orig_model.state_dict(), '/workspace/melee_project_data/autoencoder_models/autoencoder_revised_one_epoch_5.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop the notebook\n",
    "We make sure to stop the notebook here becauese the next cells use a lot of memory and we do not want to run them by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Stop running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# stop it from running all\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStop running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Stop running"
     ]
    }
   ],
   "source": [
    "# stop it from running all\n",
    "raise Exception('Stop running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see that the model loeads correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Convolutional_Autoencoder_Model import ResNet_Autoencoder\n",
    "# Initialize the model\n",
    "model_2 = ResNet_Autoencoder(channels)\n",
    "state_dict_2 = torch.load('/workspace/melee_project_data/autoencoder_models/autoencoder_revised_one_epoch_5.pt')\n",
    "model_2.load_state_dict(state_dict_2)\n",
    "model_2.to('cuda')\n",
    "model_2 = torch.compile(model_2, mode = 'max-autotune')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5111/5111 [02:28<00:00, 34.45batch/s]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pred, target = predict(model_2, loaders, 'test','cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the Stick predictions\n",
    "The transforemed stick values are in the range $[0, 1]$, but only take values in increments of roughly $0.00862$. We round the predictions to the nearest permissible value and calculate the percentage of frames that are correct to within a certain number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>How Close</th>\n",
       "      <th>JSTICK_X</th>\n",
       "      <th>JSTICK_Y</th>\n",
       "      <th>CSTICK_X</th>\n",
       "      <th>CSTICK_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21.846934</td>\n",
       "      <td>30.849162</td>\n",
       "      <td>63.738467</td>\n",
       "      <td>53.315810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.235865</td>\n",
       "      <td>51.514800</td>\n",
       "      <td>87.480318</td>\n",
       "      <td>82.347296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>47.925924</td>\n",
       "      <td>59.925596</td>\n",
       "      <td>92.453506</td>\n",
       "      <td>89.021015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>55.149839</td>\n",
       "      <td>65.521103</td>\n",
       "      <td>94.542510</td>\n",
       "      <td>91.803883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>60.622391</td>\n",
       "      <td>69.701577</td>\n",
       "      <td>95.611314</td>\n",
       "      <td>93.420068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>64.945117</td>\n",
       "      <td>73.027633</td>\n",
       "      <td>96.227636</td>\n",
       "      <td>94.466899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>68.510912</td>\n",
       "      <td>75.825144</td>\n",
       "      <td>96.615769</td>\n",
       "      <td>95.182737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>71.553068</td>\n",
       "      <td>78.244224</td>\n",
       "      <td>96.876054</td>\n",
       "      <td>95.696722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>74.206838</td>\n",
       "      <td>80.364655</td>\n",
       "      <td>97.060998</td>\n",
       "      <td>96.083443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>76.563239</td>\n",
       "      <td>82.240707</td>\n",
       "      <td>97.198743</td>\n",
       "      <td>96.383779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   How Close   JSTICK_X   JSTICK_Y   CSTICK_X   CSTICK_Y\n",
       "0          0  21.846934  30.849162  63.738467  53.315810\n",
       "1          1  38.235865  51.514800  87.480318  82.347296\n",
       "2          2  47.925924  59.925596  92.453506  89.021015\n",
       "3          3  55.149839  65.521103  94.542510  91.803883\n",
       "4          4  60.622391  69.701577  95.611314  93.420068\n",
       "5          5  64.945117  73.027633  96.227636  94.466899\n",
       "6          6  68.510912  75.825144  96.615769  95.182737\n",
       "7          7  71.553068  78.244224  96.876054  95.696722\n",
       "8          8  74.206838  80.364655  97.060998  96.083443\n",
       "9          9  76.563239  82.240707  97.198743  96.383779"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = np.unique(target[:,0])\n",
    "unique_diff = np.diff(unique)\n",
    "scale = np.mean(unique_diff[1:])\n",
    "\n",
    "integer_stick_targets = np.round(target[:,0:4] / scale ).astype(np.int16)\n",
    "integer_stick_pred = np.round(pred[:,0:4] / scale).astype(np.int16)\n",
    "\n",
    "\n",
    "\n",
    "n = 10\n",
    "\n",
    "buttons = ['JSTICK_X', 'JSTICK_Y', 'CSTICK_X', 'CSTICK_Y']\n",
    "# buttons = ['X_or_Y']\n",
    "\n",
    "\n",
    "stick_accuracy_df = pd.DataFrame(np.arange(n,dtype=np.int16),columns=['How Close'])\n",
    "# print(summary_df)\n",
    "\n",
    "\n",
    "for j in range(4):\n",
    "    unique, counts = np.unique(integer_stick_pred[:,j] - integer_stick_targets[:,j], return_counts=True)\n",
    "    data = []\n",
    "    num = np.sum(counts)\n",
    "    for i in range(n):\n",
    "        mask = np.abs(unique) <= i\n",
    "        data += [np.sum(counts[mask]) / num * 100]\n",
    "    stick_accuracy_df[buttons[j]] = data\n",
    "        \n",
    "stick_accuracy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check to see how accurate the model is at predicting the stick being zero. We engineered a binary feature that is 1 if the stick value is zeor, but here we check to see if the model predicts the stick value and not the binary feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95.10578993851693, 97.6362474948198, 99.72547980569992, 99.66317877645301]\n"
     ]
    }
   ],
   "source": [
    "target_stick_is_zero = ((integer_stick_targets == 0)*1).astype(np.int16)\n",
    "pred_stick_is_zero = ((integer_stick_pred == 0)*1).astype(np.int16)\n",
    "\n",
    "zero_accuracy = []\n",
    "# find the accuracy of the model when the stick is zero\n",
    "for j in range(4):\n",
    "    diff = np.abs(target_stick_is_zero[:,j] - pred_stick_is_zero[:,j]) \n",
    "    data = []\n",
    "    num_correct = np.sum(diff == 0)\n",
    "    zero_accuracy.append(num_correct / np.prod(diff.shape) * 100)\n",
    "print(zero_accuracy)\n",
    "# zero_accuracy_df = pd.DataFrame(columns=buttons, data=[zero_accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point the notebook is taking up a lot of memory so we delete some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del integer_stick_targets\n",
    "del integer_stick_pred\n",
    "del target_stick_is_zero\n",
    "del pred_stick_is_zero\n",
    "del diff\n",
    "# del train_df\n",
    "# del train_dataset\n",
    "# del pred\n",
    "# del pred_buttons\n",
    "# del target_buttons\n",
    "# del target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the accuracy of the button values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Button</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Acc of 0</th>\n",
       "      <th>Acc of 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRIGGER_LOGICAL</td>\n",
       "      <td>88.519695</td>\n",
       "      <td>90.179586</td>\n",
       "      <td>95.243926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z</td>\n",
       "      <td>98.780531</td>\n",
       "      <td>98.769754</td>\n",
       "      <td>99.966871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>93.515164</td>\n",
       "      <td>93.133870</td>\n",
       "      <td>99.312793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>95.820327</td>\n",
       "      <td>95.658194</td>\n",
       "      <td>99.278211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X_or_Y</td>\n",
       "      <td>87.057540</td>\n",
       "      <td>86.287204</td>\n",
       "      <td>94.053059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Button   Accuracy   Acc of 0   Acc of 1\n",
       "0  TRIGGER_LOGICAL  88.519695  90.179586  95.243926\n",
       "1                Z  98.780531  98.769754  99.966871\n",
       "2                A  93.515164  93.133870  99.312793\n",
       "3                B  95.820327  95.658194  99.278211\n",
       "4           X_or_Y  87.057540  86.287204  94.053059"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buttons = [ 'TRIGGER_LOGICAL', 'Z', 'A', 'B', 'X_or_Y']\n",
    "\n",
    "# Initializing the DataFrame\n",
    "button_accuracy_df = pd.DataFrame(columns=['Button', 'Accuracy', 'Acc of 0', 'Acc of 1'])\n",
    "\n",
    "target_buttons = target[:, 4 + 4 * transform:]\n",
    "pred_buttons = pred[:, 4 + 4 * transform:] > 0.5\n",
    "total = np.prod(target_buttons.shape[0]*target_buttons.shape[2])\n",
    "\n",
    "# Computing accuracies and filling the DataFrame\n",
    "rows = []  # List to hold row data\n",
    "\n",
    "for i, button in enumerate(buttons):\n",
    "    correct_predictions = np.sum(target_buttons[:, i] == pred_buttons[:, i])\n",
    "    correct_zeros = np.sum((target_buttons[:, i] == 0) & (pred_buttons[:, i] == 0))\n",
    "    correct_ones = np.sum((target_buttons[:, i] == 1) & (pred_buttons[:, i] == 1))\n",
    "\n",
    "    accuracy = correct_predictions / total * 100\n",
    "    acc_of_0 = correct_zeros / np.sum(target_buttons[:, i] == 0) * 100 if np.sum(target_buttons[:, i] == 0) > 0 else 0\n",
    "    acc_of_1 = correct_ones / np.sum(target_buttons[:, i] == 1) * 100 if np.sum(target_buttons[:, i] == 1) > 0 else 0\n",
    "\n",
    "    rows.append({\n",
    "        'Button': button,\n",
    "        'Accuracy': accuracy,\n",
    "        'Acc of 0': acc_of_0,\n",
    "        'Acc of 1': acc_of_1\n",
    "    })\n",
    "\n",
    "# Use concat to add all new rows to the DataFrame at once\n",
    "button_accuracy_df = pd.concat([button_accuracy_df, pd.DataFrame(rows)], ignore_index=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "button_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

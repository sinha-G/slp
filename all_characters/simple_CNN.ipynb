{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(9, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.fc1 = nn.LazyLinear(128)  # Adjust the dimensions based on your input size\n",
    "#         self.fc2 = nn.Linear(128, 5)  # We have 5 classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(9, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        # Assuming the sequential data is being halved by the pooling layers\n",
    "        # Update the following line if the number of features after pooling changes\n",
    "        self.fc1 = nn.LazyLinear(128)  # Adjust the dimensions based on your input size\n",
    "        self.fc2 = nn.Linear(128, 5)  # We have 5 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Custom Dataset Class: </h2>\n",
    "Create a custom Dataset class in PyTorch. This class will handle loading individual segments from disk and performing any necessary preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "class GameSegmentDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        \"\"\"\n",
    "        file_paths: List of paths to the numpy files\n",
    "        labels: List of labels corresponding to each file\n",
    "        transform: Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # segment = np.load(self.file_paths[idx])\n",
    "        with gzip.open(self.file_paths[idx], 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "\n",
    "        if self.transform:\n",
    "            segment = self.transform(segment)\n",
    "            \n",
    "        return torch.from_numpy(segment).float(), self.labels[idx]\n",
    "            \n",
    "    # def __getitem__(self, idx):\n",
    "    #     try:\n",
    "    #         with gzip.open(self.file_paths[idx], 'rb') as f:\n",
    "    #             segment = np.load(f)\n",
    "    #         # Apply transform if any\n",
    "    #         if self.transform:\n",
    "    #             segment = self.transform(segment)\n",
    "    #         return torch.from_numpy(segment).float(), self.labels[idx]\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error loading gzip file at index {idx}: {e}\")\n",
    "    #         raise\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Splitting: </h2>\n",
    "Before initializing the DataLoader, you need to split your data into training, validation, and holdout sets. Make sure to stratify the split to maintain the proportion of each character class across the sets. You can use scikit-learn's train_test_split function for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Path where the files were saved\n",
    "save_path = 'C:/Users/jaspa/Grant ML/slp/data/'\n",
    "\n",
    "# Load file_paths\n",
    "with open(os.path.join(save_path, 'file_paths.pkl'), 'rb') as f:\n",
    "    file_paths = pickle.load(f)\n",
    "\n",
    "print(file_paths)\n",
    "\n",
    "# Load label_list\n",
    "with open(os.path.join(save_path, 'label_list.pkl'), 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "print(labels)\n",
    "# Assuming file_paths and labels are lists containing your data and labels\n",
    "file_paths_train, file_paths_temp, labels_train, labels_temp = train_test_split(file_paths, labels, test_size=0.3, stratify=labels)\n",
    "file_paths_val, file_paths_test, labels_val, labels_test = train_test_split(file_paths_temp, labels_temp, test_size=0.5, stratify=labels_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Initializing DataLoaders: </h2>\n",
    "With your data split and your custom Dataset defined, you can now initialize the DataLoader for each set. The DataLoader will handle loading the data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize the datasets\n",
    "train_dataset = GameSegmentDataset(file_paths_train, labels_train)\n",
    "val_dataset = GameSegmentDataset(file_paths_val, labels_val)\n",
    "test_dataset = GameSegmentDataset(file_paths_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data loaders\n",
    "batch_size = 2**9  # Adjust based on your system's capabilities\n",
    "num_workers = 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Define the Training Loop and Evaluate Accuracy </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with progress bar\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "    \n",
    "\n",
    "    for inputs, labels in train_loader_tqdm:\n",
    "        # Move data to appropriate device (e.g., GPU if available)\n",
    "        # print(inputs.shape)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_loader_tqdm.set_postfix(loss=(train_loss / total), accuracy=(100.0 * train_correct / total))\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print(f'\\nTest Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asses_model(model_name, y_pred, y_test, labels_order):\n",
    "    print()\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_pred, y_test)\n",
    "\n",
    "    # Print accuracy and Cohen Kappa score with explanations\n",
    "    print(f'Accuracy of {model_name}: {accuracy:.4f}')\n",
    "    print(f'Cohen Kappa Score of {model_name}: {kappa:.4f}')\n",
    "\n",
    "    # Calculate the normalized predicted label count\n",
    "    unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "    normalized_counts_pred = {k: v / counts_test[np.where(unique_test == k)[0][0]] for k, v in zip(unique_pred, counts_pred)}\n",
    "    \n",
    "    # Calculate the percent the model over or under predicted the labels using the specified label order\n",
    "    sorted_values = [normalized_counts_pred[k] - 1 if k in normalized_counts_pred else 0 for k in labels_order]\n",
    "\n",
    "    # Plotting the percent the model over or under predicted the labels\n",
    "    plt.figure(figsize=(2*len(labels_order), 4))\n",
    "    plt.bar(labels_order, sorted_values, color=['green' if x > 0 else 'blue' for x in sorted_values])\n",
    "    plt.title(f'Percent Model {model_name} Over or Under Predicted Labels')\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Percent Over/Under Prediction')\n",
    "    \n",
    "    # Center y-axis and set equal extension above and below\n",
    "    max_extent = max(abs(min(sorted_values)), abs(max(sorted_values))) * 1.05\n",
    "    plt.ylim(-max_extent, max_extent)\n",
    "    plt.axhline(y=0, color='gray', linewidth=0.8)\n",
    "    plt.show()\n",
    "\n",
    "    # Display each confusion matrix on its own row\n",
    "    for norm in [None, 'true', 'pred']:\n",
    "        plt.figure(figsize=(len(labels_order)+5, len(labels_order)+5))\n",
    "        ax = plt.gca()\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test, y_pred, normalize=norm, ax=ax,\n",
    "            xticks_rotation='vertical', labels=labels_order\n",
    "        )\n",
    "        ax.title.set_text(f'{model_name} Confusion Matrix ({\"Not Normalized\" if norm is None else \"Normalized by \" + norm})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "# def asses_model(model_name, y_pred, y_test, labels_order):\n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     kappa = cohen_kappa_score(y_pred, y_test)\n",
    "\n",
    "#     # Print accuracy and Cohen Kappa score\n",
    "#     print(f'Accuracy of {model_name}: {accuracy:.4f}')\n",
    "#     print(f'Cohen Kappa Score of {model_name}: {kappa:.4f}')\n",
    "\n",
    "#     # Calculate the normalized predicted label count\n",
    "#     unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "#     unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "#     normalized_counts_pred = {k: v / counts_test[np.where(unique_test == k)[0][0]] for k, v in zip(unique_pred, counts_pred) if k in unique_test}\n",
    "    \n",
    "#     # Calculate the percent the model over or under predicted the labels\n",
    "#     sorted_values = [normalized_counts_pred.get(k, 0) - 1 for k in labels_order]\n",
    "\n",
    "#     # Plotting the percent the model over or under predicted the labels\n",
    "#     plt.figure(figsize=(2*len(labels_order), 4))\n",
    "#     plt.bar(labels_order, sorted_values, color=['green' if x > 0 else 'blue' for x in sorted_values])\n",
    "#     plt.title(f'Percent {model_name} Over or Under Predicted Labels')\n",
    "#     plt.xlabel('Labels')\n",
    "#     plt.ylabel('Percent Over/Under Prediction')\n",
    "\n",
    "#     # Center y-axis and set equal extension above and below\n",
    "#     max_extent = max(abs(min(sorted_values)), abs(max(sorted_values))) * 1.05\n",
    "#     plt.ylim(-max_extent, max_extent)\n",
    "#     plt.axhline(y=0, color='gray', linewidth=0.8)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Display each confusion matrix on its own row\n",
    "#     for norm in [None, 'true', 'pred']:\n",
    "#         plt.figure(figsize=(len(labels_order)+5, len(labels_order)+5))\n",
    "#         ax = plt.gca()\n",
    "#         ConfusionMatrixDisplay.from_predictions(\n",
    "#             y_test, y_pred, normalize=norm, ax=ax,\n",
    "#             display_labels=labels_order,\n",
    "#             xticks_rotation='vertical'\n",
    "#         )\n",
    "#         ax.title.set_text(f'{model_name} Confusion Matrix ({\"Not Normalized\" if norm is None else \"Normalized by \" + norm})')\n",
    "#         ax.set_xlabel('Predicted labels')\n",
    "#         ax.set_ylabel('True labels')\n",
    "#         ax.set_xticklabels(labels_order)  # Explicitly setting x-axis labels\n",
    "#         ax.set_yticklabels(labels_order[::-1])  # Setting y-axis labels to reverse order\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import accuracy_score, cohen_kappa_score, ConfusionMatrixDisplay\n",
    "\n",
    "# def asses_model(model_name, y_pred, y_test, labels_order, label_decoder):\n",
    "#     \"\"\"\n",
    "#     Evaluate and visualize the model's performance with decoded labels.\n",
    "\n",
    "#     Args:\n",
    "#         model_name (str): Name of the model for display purposes.\n",
    "#         y_pred (np.ndarray): Encoded predictions from the model.\n",
    "#         y_test (np.ndarray): Encoded true labels.\n",
    "#         labels_order (list): Ordered list of labels for display.\n",
    "#         label_decoder (dict): Dictionary to decode labels from encoded labels to string labels.\n",
    "#     \"\"\"\n",
    "#     # Decode predictions and true labels\n",
    "#     decoded_y_pred = [label_decoder.get(label, \"Unknown\") for label in y_pred]\n",
    "#     decoded_y_test = [label_decoder.get(label, \"Unknown\") for label in y_test]\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(decoded_y_test, decoded_y_pred)\n",
    "#     kappa = cohen_kappa_score(decoded_y_pred, decoded_y_test)\n",
    "#     print(f'Accuracy of {model_name}: {accuracy:.4f}')\n",
    "#     print(f'Cohen Kappa Score of {model_name}: {kappa:.4f}')\n",
    "\n",
    "#     # Normalize the predicted label counts\n",
    "#     unique_pred, counts_pred = np.unique(decoded_y_pred, return_counts=True)\n",
    "#     unique_test, counts_test = np.unique(decoded_y_test, return_counts=True)\n",
    "#     normalized_counts_pred = {k: v / counts_test[np.where(unique_test == k)[0][0]] for k, v in zip(unique_pred, counts_pred) if k in unique_test}\n",
    "    \n",
    "#     # Calculate and plot the percent the model over or under predicted the labels\n",
    "#     sorted_values = [normalized_counts_pred.get(k, 0) - 1 for k in labels_order]\n",
    "#     plt.figure(figsize=(2*len(labels_order), 4))\n",
    "#     plt.bar(labels_order, sorted_values, color=['green' if x > 0 else 'blue' for x in sorted_values])\n",
    "#     plt.title(f'Percent {model_name} Over or Under Predicted Labels')\n",
    "#     plt.xlabel('Labels')\n",
    "#     plt.ylabel('Percent Over/Under Prediction')\n",
    "#     max_extent = max(abs(min(sorted_values)), abs(max(sorted_values))) * 1.05\n",
    "#     plt.ylim(-max_extent, max_extent)\n",
    "#     plt.axhline(y=0, color='gray', linewidth=0.8)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Display each confusion matrix\n",
    "#     for norm in [None, 'true', 'pred']:\n",
    "#         plt.figure(figsize=(len(labels_order)+5, len(labels_order)+5))\n",
    "#         ax = plt.gca()\n",
    "#         ConfusionMatrixDisplay.from_predictions(\n",
    "#             decoded_y_test, decoded_y_pred, normalize=norm, ax=ax,\n",
    "#             display_labels=labels_order,\n",
    "#             xticks_rotation='vertical'\n",
    "#         )\n",
    "#         ax.title.set_text(f'{model_name} Confusion Matrix ({\"Not Normalized\" if norm is None else \"Normalized by \" + norm})')\n",
    "#         ax.set_xlabel('Predicted labels')\n",
    "#         ax.set_ylabel('True labels')\n",
    "#         ax.set_xticklabels(labels_order)\n",
    "#         ax.set_yticklabels(labels_order[::-1])\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = ['ranked','public','mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FALCO'],\n",
    "    # 'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'PEACH', 'LUIGI', 'DONKEY_KONG'],\n",
    "    'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We classify 5 characters on competitive stages\n",
    "\n",
    "source_data = ['ranked']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    'character_name': ['FOX'],\n",
    "    'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0'],\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['opposing_player'], # Can be 'general', 'player', 'opposing_player'\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "\n",
    "print(dataset.dataset['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_order =  dataset.number_of_segments_per_game(12,100000)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df  = dataset.train_test_split_dataframes(test_ratio = .20, val = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unique = train_df['labels'].unique()\n",
    "encoded_labels_unique = train_df['encoded_labels'].unique()\n",
    "label_decoder = zip(labels_unique, encoded_labels_unique)\n",
    "label_decoder = dict(zip(encoded_labels_unique, labels_unique)) \n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ResNet_Model import ResNet50, ResNet101\n",
    "# from ResNet_Model_Relative import ResNet50\n",
    "from ResNet_Model_Relative_2 import ResNet50\n",
    "\n",
    "model = ResNet50(num_classes=20, channels=9).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dummy input with the correct shape\n",
    "# dummy_input = torch.randn(1, 9, 4096)  # batch size of 1, 9 channels, 4096 length\n",
    "# try:\n",
    "#     with torch.no_grad():\n",
    "#         model(dummy_input.to(model.device))  # Ensure the dummy input is on the same device as the model\n",
    "# except Exception as e:\n",
    "#     print(\"Error during test forward pass: \", str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(9, 2**12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.compile(model, mode = 'default').to('cuda')\n",
    "model = torch.compile(model, mode='max-autotune')\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = slp_pytorch_functions.prepare_data_loaders_no_val(train_df, test_df, 32*2, 16)\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "slp_pytorch_functions.train_model(model, criterion,optimizer, loaders, 'cuda', 2 )\n",
    "y_pred = slp_pytorch_functions.predict(model, loaders['test'], 'cuda' )\n",
    "# y_pred = slp_pytorch_functions.predict(model, loaders['train'], 'cuda' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [3,4,6,3]\n",
    "# print(1 + 2 * (x[0] * 4 + x[1] * 8 + x[2] * 16 + x[3] * 32))\n",
    "# print('Memory of ResNet50:', (1 + 2 * (x[0] * 4 + x[1] * 8 + x[2] * 16 + x[3] * 32)) / 60, 'seconds')\n",
    "# x = [3,4,23,3]\n",
    "# print(1 + 2 * (x[0] * 4 + x[1] * 8 + x[2] * 16 + x[3] * 32))\n",
    "# print('Memory of ResNet101:', (1 + 2 * (x[0] * 4 + x[1] * 8 + x[2] * 16 + x[3] * 32)) / 60, 'seconds')\n",
    "# x = [3,8,36,3]\n",
    "# print(1 + 2 * (x[0] * 4 + x[1] * 8 + x[2] * 16 + x[3] * 32))\n",
    "# print('Memory of ResNet152:', (1 + 2 * (x[0] * 4 + x[1] * 8 + x[2] * 16 + x[3] * 32)) / 60, 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [3, 4, 4]\n",
    "# plus = 3 + 2\n",
    "# multiplier = 4\n",
    "# for val in x:\n",
    "#     plus += multiplier * val\n",
    "#     multiplier *= 2\n",
    "# print(1 + 2 * plus)\n",
    "    \n",
    "# print(1 + 2 * (x[0] * 4 + x[1] * 8 + x[2] * 16 + x[3] * 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_pytorch_functions.train_model(model, criterion,optimizer, loaders, 'cuda', 2 )\n",
    "y_pred = slp_pytorch_functions.predict(model, loaders['test'], 'cuda' )\n",
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_pytorch_functions.train_model(model, criterion,optimizer, loaders, 'cuda', 2 )\n",
    "y_pred = slp_pytorch_functions.predict(model, loaders['test'], 'cuda' )\n",
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_pytorch_functions.train_model(model, criterion,optimizer, loaders, 'cuda', 2 )\n",
    "y_pred = slp_pytorch_functions.predict(model, loaders['test'], 'cuda' )\n",
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

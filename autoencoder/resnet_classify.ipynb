{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "from slp_package.slp_functions import create_merged_game_data_df\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_merged_game_data_df(['ranked','mango','public'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['player_1_display_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].sum() / (60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asses_model(model_name, y_pred, y_test, labels_order):\n",
    "    print()\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_pred, y_test)\n",
    "\n",
    "    # Print accuracy and Cohen Kappa score with explanations\n",
    "    print(f'Accuracy of {model_name}: {accuracy:.4f}')\n",
    "    print(f'Cohen Kappa Score of {model_name}: {kappa:.4f}')\n",
    "\n",
    "    # Calculate the normalized predicted label count\n",
    "    unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "    normalized_counts_pred = {k: v / counts_test[np.where(unique_test == k)[0][0]] for k, v in zip(unique_pred, counts_pred)}\n",
    "    \n",
    "    # Calculate the percent the model over or under predicted the labels using the specified label order\n",
    "    sorted_values = [normalized_counts_pred[k] - 1 if k in normalized_counts_pred else 0 for k in labels_order]\n",
    "\n",
    "    # Plotting the percent the model over or under predicted the labels\n",
    "    plt.figure(figsize=(2*len(labels_order), 4))\n",
    "    plt.bar(labels_order, sorted_values, color=['green' if x > 0 else 'blue' for x in sorted_values])\n",
    "    plt.title(f'Percent Model {model_name} Over or Under Predicted Labels')\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Percent Over/Under Prediction')\n",
    "    \n",
    "    # Center y-axis and set equal extension above and below\n",
    "    max_extent = max(abs(min(sorted_values)), abs(max(sorted_values))) * 1.05\n",
    "    plt.ylim(-max_extent, max_extent)\n",
    "    plt.axhline(y=0, color='gray', linewidth=0.8)\n",
    "    plt.show()\n",
    "\n",
    "    # Display each confusion matrix on its own row\n",
    "    for norm in [None, 'true', 'pred']:\n",
    "        plt.figure(figsize=(len(labels_order)+5, len(labels_order)+5))\n",
    "        ax = plt.gca()\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test, y_pred, normalize=norm, ax=ax,\n",
    "            xticks_rotation='vertical', labels=labels_order\n",
    "        )\n",
    "        ax.title.set_text(f'{model_name} Confusion Matrix ({\"Not Normalized\" if norm is None else \"Normalized by \" + norm})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = ['ranked','public','mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FALCO'],\n",
    "    # 'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'PEACH', 'LUIGI', 'DONKEY_KONG'],\n",
    "    'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_data = ['ranked','public']\n",
    "\n",
    "# general_features = {\n",
    "#     'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "#     'num_players': [2],\n",
    "#     'conclusive': [True]\n",
    "# }\n",
    "# player_features = {\n",
    "#     # 'netplay_code': ['MANG#0'],\n",
    "#     # 'character_name': ['PICHU'],\n",
    "#     # 'character_name': ['PIKACHU'],\n",
    "#     # 'character_name': ['PIKACHU','PICHU'],\n",
    "#     # 'character_name': ['FOX','FALCO'],\n",
    "#     'character_name': ['FOX','FALCO','PIKACHU','PICHU'],\n",
    "#     # 'display_name': ['Platinum Player','Master Player', 'Diamond Player']\n",
    "\n",
    "    \n",
    "# }\n",
    "# opposing_player_features = {\n",
    "#     # 'character_name': ['MARTH'],\n",
    "#     # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "#     'type_name': ['HUMAN']\n",
    "# }\n",
    "# label_info = {\n",
    "#     'source': ['player'], # Can be 'general', 'player\n",
    "#     # 'feature': ['netplay_code']\n",
    "#     'feature': ['character_name']\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We classify opponent's characters on competitive stages\n",
    "\n",
    "# source_data = ['ranked']\n",
    "\n",
    "# general_features = {\n",
    "#     'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "#     'num_players': [2],\n",
    "#     'conclusive': [True]\n",
    "# }\n",
    "# player_features = {\n",
    "#     # 'netplay_code': ['MANG#0'],\n",
    "#     'character_name': ['FOX'],\n",
    "#     'type_name': ['HUMAN']\n",
    "    \n",
    "# }\n",
    "# opposing_player_features = {\n",
    "#     'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "#     # 'netplay_code': ['KOD#0', 'ZAIN#0'],\n",
    "#     'type_name': ['HUMAN']\n",
    "# }\n",
    "# label_info = {\n",
    "#     'source': ['opposing_player'], # Can be 'general', 'player', 'opposing_player'\n",
    "#     # 'feature': ['netplay_code']\n",
    "#     'feature': ['character_name']\n",
    "# }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "\n",
    "print(dataset.dataset['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_order =  dataset.number_of_segments_per_game(10,2000)\n",
    "print(2**10)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df  = dataset.train_test_split_dataframes(test_ratio = .20, val = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unique = train_df['labels'].unique()\n",
    "encoded_labels_unique = train_df['encoded_labels'].unique()\n",
    "label_decoder = zip(labels_unique, encoded_labels_unique)\n",
    "label_decoder = dict(zip(encoded_labels_unique, labels_unique)) \n",
    "print(label_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet_Model import ResNet50, ResNet101\n",
    "# from ResNet_Model_Relative import ResNet50\n",
    "# from ResNet_Model_Relative_2 import ResNet50\n",
    "\n",
    "model = ResNet50(num_classes=4, channels=9).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dummy input with the correct shape\n",
    "# dummy_input = torch.randn(1, 9, 4096)  # batch size of 1, 9 channels, 4096 length\n",
    "# try:\n",
    "#     with torch.no_grad():\n",
    "#         model(dummy_input.to(model.device))  # Ensure the dummy input is on the same device as the model\n",
    "# except Exception as e:\n",
    "#     print(\"Error during test forward pass: \", str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(9, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.compile(model, mode = 'default').to('cuda')\n",
    "# model = torch.compile(model, mode='max-autotune')\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = slp_pytorch_functions.prepare_data_loaders_no_val(train_df, test_df, 32, 16)\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "slp_pytorch_functions.train_model(model, criterion,optimizer, loaders, 'cuda', 2 )\n",
    "y_pred = slp_pytorch_functions.predict(model, loaders['test'], 'cuda' )\n",
    "# y_pred = slp_pytorch_functions.predict(model, loaders['train'], 'cuda' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [3, 4, 4]\n",
    "plus = 3 + 2\n",
    "multiplier = 4\n",
    "for val in x:\n",
    "    plus += multiplier * val\n",
    "    multiplier *= 2\n",
    "print(1 + 2 * plus)\n",
    "    \n",
    "# print(1 + 2 * (x[0] * 4 + x[1] * 8 + x[2] * 16 + x[3] * 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_pytorch_functions.train_model(model, criterion,optimizer, loaders, 'cuda', 2 )\n",
    "y_pred = slp_pytorch_functions.predict(model, loaders['test'], 'cuda' )\n",
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_pytorch_functions.train_model(model, criterion,optimizer, loaders, 'cuda', 2 )\n",
    "y_pred = slp_pytorch_functions.predict(model, loaders['test'], 'cuda' )\n",
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_pytorch_functions.train_model(model, criterion,optimizer, loaders, 'cuda', 2 )\n",
    "y_pred = slp_pytorch_functions.predict(model, loaders['test'], 'cuda' )\n",
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asses_model('ResNet-50', np.array([label_decoder.get(item, \"Unknown\") for item in y_pred]), np.array(test_df['labels']), labels_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

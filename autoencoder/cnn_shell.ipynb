{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = ['ranked','public']\n",
    "\n",
    "general_features = {\n",
    "    # 'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'PEACH', 'LUIGI', 'DONKEY_KONG'],\n",
    "    # 'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/autoencoder/../slp_package/input_dataset.py:95: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOX               101341\n",
      "FALCO              79412\n",
      "MARTH              50788\n",
      "CAPTAIN_FALCON     39858\n",
      "SHEIK              29972\n",
      "PEACH              19338\n",
      "JIGGLYPUFF         17390\n",
      "SAMUS              10499\n",
      "ICE_CLIMBERS        7696\n",
      "GANONDORF           7524\n",
      "LUIGI               5777\n",
      "YOSHI               5567\n",
      "PIKACHU             4488\n",
      "DR_MARIO            4150\n",
      "LINK                2693\n",
      "NESS                2554\n",
      "GAME_AND_WATCH      2196\n",
      "DONKEY_KONG         2183\n",
      "MEWTWO              1883\n",
      "BOWSER               994\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "\n",
    "print(dataset.dataset['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Label   Count   Shift\n",
      "0              FOX  100351  282403\n",
      "1            FALCO   78612  219519\n",
      "2            MARTH   50387  154283\n",
      "3   CAPTAIN_FALCON   39443  106456\n",
      "4            SHEIK   29811  102660\n",
      "5            PEACH   19215   72021\n",
      "6       JIGGLYPUFF   17103   61146\n",
      "7            SAMUS   10438   43361\n",
      "8     ICE_CLIMBERS    7649   28907\n",
      "9        GANONDORF    7448   21670\n",
      "10           LUIGI    5737   20492\n",
      "11           YOSHI    5537   19333\n",
      "12         PIKACHU    4446   15860\n",
      "13        DR_MARIO    4114   14173\n",
      "14            LINK    2678    9791\n",
      "15            NESS    2536   11125\n",
      "16  GAME_AND_WATCH    2171    5949\n",
      "17     DONKEY_KONG    2156    7413\n",
      "18          MEWTWO    1863    8306\n",
      "19          BOWSER     987    3891\n"
     ]
    }
   ],
   "source": [
    "labels_order =  dataset.number_of_segments_per_game(12,2000)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df  = dataset.train_test_split_dataframes(test_ratio = .20, val = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_inputs_np_sub_path</th>\n",
       "      <th>labels</th>\n",
       "      <th>encoded_labels</th>\n",
       "      <th>segment_start_index</th>\n",
       "      <th>segment_index</th>\n",
       "      <th>segment_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public\\FOX\\0080ac84-c059-426a-83b1-3b71efc4881...</td>\n",
       "      <td>FOX</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public\\FOX\\61719d70-d256-48c3-9583-293c47ceffb...</td>\n",
       "      <td>FOX</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public\\FOX\\88b39d55-8f2e-4c2e-8d7c-8d45c276df9...</td>\n",
       "      <td>FOX</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public\\FOX\\6c9db7b1-0207-470a-954a-94d05b5e81c...</td>\n",
       "      <td>FOX</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public\\FOX\\2dfd4f0f-2ade-4535-84a5-8f87c5b3e75...</td>\n",
       "      <td>FOX</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           player_inputs_np_sub_path labels  encoded_labels  \\\n",
       "0  public\\FOX\\0080ac84-c059-426a-83b1-3b71efc4881...    FOX               5   \n",
       "1  public\\FOX\\61719d70-d256-48c3-9583-293c47ceffb...    FOX               5   \n",
       "2  public\\FOX\\88b39d55-8f2e-4c2e-8d7c-8d45c276df9...    FOX               5   \n",
       "3  public\\FOX\\6c9db7b1-0207-470a-954a-94d05b5e81c...    FOX               5   \n",
       "4  public\\FOX\\2dfd4f0f-2ade-4535-84a5-8f87c5b3e75...    FOX               5   \n",
       "\n",
       "   segment_start_index  segment_index  segment_length  \n",
       "0                    0              0            4096  \n",
       "1                    0              0            4096  \n",
       "2                    0              0            4096  \n",
       "3                    0              0            4096  \n",
       "4                    0              0            4096  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        dropout = 0\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.LazyConv1d(128, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(128, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.LazyConv1d(64, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(64, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.LazyConv1d(32, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(32, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.LazyConv1d(16, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(16, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Upsampling and Conv1d to gradually increase dimensions back to original\n",
    "            nn.ConvTranspose1d(16, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.ConvTranspose1d(16, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.ConvTranspose1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv1d(128, 9, kernel_size=9, stride=1, padding=4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the model.\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Get a batch of data to see the size if we want that information. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading game segments from compressed numpy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.file_paths = df['player_inputs_np_sub_path'].to_numpy()\n",
    "        self.encoded_labels = df['encoded_labels'].to_numpy()\n",
    "        self.segment_start_index = df['segment_start_index'].to_numpy()\n",
    "        # self.segment_index = df['segment_index'].to_numpy()\n",
    "        self.segment_length = df['segment_length'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads and returns a sample from the dataset at the specified index.\"\"\"\n",
    "        with gzip.open('/workspace/melee_project_data/input_np/' + self.file_paths[idx].replace('\\\\','/'), 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "\n",
    "        if self.transform:\n",
    "            segment = self.transform(segment)\n",
    "        \n",
    "        # Start and end of the segment\n",
    "        segment_start = self.segment_start_index[idx]\n",
    "        segment_end = self.segment_start_index[idx] + self.segment_length[idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        segment_tensor = torch.from_numpy(segment[:,segment_start:segment_end]).float()\n",
    "        # label_tensor = torch.tensor(self.encoded_labels[idx], dtype=torch.long)\n",
    "        return segment_tensor#, label_tensor\n",
    "    \n",
    "def prepare_data_loaders(train_df, test_df, batch_size, num_workers):\n",
    "    # Initialize datasets\n",
    "    train_dataset = TrainingDataset(train_df)\n",
    "    # val_dataset = TrainingDataset(file_paths_val, labels_val)\n",
    "    test_dataset = TrainingDataset(test_df)\n",
    "\n",
    "    # Initialize data loaders\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True,persistent_workers=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=2**9, num_workers=num_workers, shuffle=True, pin_memory=True,persistent_workers=True),\n",
    "        # 'val': DataLoader(val_dataset, batch_size=2**9, num_workers=num_workers, shuffle=False, pin_memory=True,persistent_workers=True)\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "\n",
    "\n",
    "''' Get a batch of data to see the size if we want that information. ''' \n",
    "# data_loader_iterator = iter(loaders['train'])\n",
    "# first_batch = next(data_loader_iterator)\n",
    "# print(first_batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, loaders, device, num_epochs=1):\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total = 0\n",
    "        train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        \n",
    "        for batch_number, target_cpu in enumerate(train_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            \n",
    "            # Resets the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Runs the forward pass with autocasting.\n",
    "            with autocast():\n",
    "                output_gpu = model(target_gpu)\n",
    "                loss = criterion(output_gpu, target_gpu)\n",
    "            \n",
    "            # Scales loss and calls backward() to create scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "            # otherwise, optimizer.step() is skipped.\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "\n",
    "            # Update progress\n",
    "            train_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix(loss=f'{train_loss / (batch_number + 1):.4f}')\n",
    "\n",
    "\n",
    "def evaluate_model(model, criterion, loaders, loader, device):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        eval_loader_tqdm = tqdm(loaders[loader], unit = 'batch')\n",
    "        \n",
    "        for batch_number, target_cpu in enumerate(eval_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            output_gpu = model(target_gpu)\n",
    "            \n",
    "            eval_loss += criterion(output_gpu, target_gpu).item()\n",
    "            eval_loader_tqdm.set_postfix(loss=f'{eval_loss / (batch_number + 1):.4f}') \n",
    "            \n",
    "    print(f'Evaluated Loss: {eval_loss / len(loaders[loader]):.6f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 128, 4096]          10,496\n",
      "              ReLU-2            [-1, 128, 4096]               0\n",
      "            Conv1d-3            [-1, 128, 4096]         147,584\n",
      "              ReLU-4            [-1, 128, 4096]               0\n",
      "         MaxPool1d-5            [-1, 128, 2048]               0\n",
      "           Dropout-6            [-1, 128, 2048]               0\n",
      "            Conv1d-7             [-1, 64, 2048]          73,792\n",
      "              ReLU-8             [-1, 64, 2048]               0\n",
      "            Conv1d-9             [-1, 64, 2048]          36,928\n",
      "             ReLU-10             [-1, 64, 2048]               0\n",
      "        MaxPool1d-11             [-1, 64, 1024]               0\n",
      "          Dropout-12             [-1, 64, 1024]               0\n",
      "           Conv1d-13             [-1, 32, 1024]          18,464\n",
      "             ReLU-14             [-1, 32, 1024]               0\n",
      "           Conv1d-15             [-1, 32, 1024]           9,248\n",
      "             ReLU-16             [-1, 32, 1024]               0\n",
      "        MaxPool1d-17              [-1, 32, 512]               0\n",
      "          Dropout-18              [-1, 32, 512]               0\n",
      "           Conv1d-19              [-1, 16, 512]           4,624\n",
      "             ReLU-20              [-1, 16, 512]               0\n",
      "           Conv1d-21              [-1, 16, 512]           2,320\n",
      "             ReLU-22              [-1, 16, 512]               0\n",
      "        MaxPool1d-23              [-1, 16, 256]               0\n",
      "          Dropout-24              [-1, 16, 256]               0\n",
      "  ConvTranspose1d-25              [-1, 16, 512]           1,040\n",
      "             ReLU-26              [-1, 16, 512]               0\n",
      "           Conv1d-27              [-1, 16, 512]           2,320\n",
      "             ReLU-28              [-1, 16, 512]               0\n",
      "          Dropout-29              [-1, 16, 512]               0\n",
      "  ConvTranspose1d-30             [-1, 32, 1024]           2,080\n",
      "             ReLU-31             [-1, 32, 1024]               0\n",
      "           Conv1d-32             [-1, 32, 1024]           9,248\n",
      "             ReLU-33             [-1, 32, 1024]               0\n",
      "          Dropout-34             [-1, 32, 1024]               0\n",
      "  ConvTranspose1d-35             [-1, 64, 2048]           8,256\n",
      "             ReLU-36             [-1, 64, 2048]               0\n",
      "           Conv1d-37             [-1, 64, 2048]          36,928\n",
      "             ReLU-38             [-1, 64, 2048]               0\n",
      "          Dropout-39             [-1, 64, 2048]               0\n",
      "  ConvTranspose1d-40            [-1, 128, 4096]          32,896\n",
      "             ReLU-41            [-1, 128, 4096]               0\n",
      "           Conv1d-42            [-1, 128, 4096]         147,584\n",
      "             ReLU-43            [-1, 128, 4096]               0\n",
      "          Dropout-44            [-1, 128, 4096]               0\n",
      "           Conv1d-45              [-1, 9, 4096]          10,377\n",
      "================================================================\n",
      "Total params: 554,185\n",
      "Trainable params: 554,185\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 53.41\n",
      "Params size (MB): 2.11\n",
      "Estimated Total Size (MB): 55.66\n",
      "----------------------------------------------------------------\n",
      "Target shape: torch.Size([9, 4096])\n",
      "Output shape: torch.Size([9, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Autoencoder().to('cuda')\n",
    "\n",
    "# With the size of an input we can get a model summary.\n",
    "summary(model, input_size=(9, 4096))\n",
    "\n",
    "# Check that the output shape and target shape match\n",
    "training_example = torch.rand(9, 2 ** 12).to('cuda')\n",
    "print('Target shape:', training_example.shape)\n",
    "model.eval()\n",
    "output = model(training_example)\n",
    "print('Output shape:', output.shape)\n",
    "\n",
    "## Optionally compile the model\n",
    "# import torch_tensorrt\n",
    "# model = torch.compile(model, mode = 'default')\n",
    "# model = torch.compile(model,mode = 'max-autotune')\n",
    "# model = torch.compile(model, backend=\"torch_tensorrt\")\n",
    "# model = torch.compile(model, backend=\"torch_tensorrt\",mode = 'max-autotune')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  53%|█████▎    | 1059/2000 [00:06<00:05, 161.14batch/s, loss=0.1100]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Again, this sometimes seems to help\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, loaders, device, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Update progress\u001b[39;00m\n\u001b[1;32m     33\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrain_loader_tqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_postfix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrain_loss\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_number\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1431\u001b[0m, in \u001b[0;36mtqdm.set_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostfix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m postfix[key]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m   1429\u001b[0m                          \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m postfix\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh:\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1151\u001b[0m, in \u001b[0;36mtqdm.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_meter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:631\u001b[0m, in \u001b[0;36mtqdm.format_meter\u001b[0;34m(n, total, elapsed, ncols, prefix, ascii, unit, unit_scale, rate, bar_format, postfix, unit_divisor, initial, colour, **extra_kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m     bar_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[38;5;132;01m{r_bar}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m full_bar \u001b[38;5;241m=\u001b[39m FormatReplace()\n\u001b[0;32m--> 631\u001b[0m nobar \u001b[38;5;241m=\u001b[39m \u001b[43mbar_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mformat_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m full_bar\u001b[38;5;241m.\u001b[39mformat_called:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nobar  \u001b[38;5;66;03m# no `{bar}`; nothing else to do\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/utils.py:112\u001b[0m, in \u001b[0;36mFormatReplace.__format__\u001b[0;34m(self, _)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace \u001b[38;5;241m=\u001b[39m replace\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, _):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_called \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pepare data loaders\n",
    "batch_size = 16\n",
    "num_workers = 16\n",
    "loaders = prepare_data_loaders(train_df, test_df, batch_size, num_workers)\n",
    "\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 2\n",
    "\n",
    "# This seems to sometimes help\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)\n",
    "print(f'Batch Size: {batch_size}, Training time: {time.time() - start_time:.2f}')\n",
    "\n",
    "# Again, this sometimes seems to help\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Evaluate the trained model\n",
    "evaluate_model(model, criterion, loaders, 'test', 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

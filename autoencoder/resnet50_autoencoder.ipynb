{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = ['ranked','public','mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FALCO'],\n",
    "    'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'PEACH', 'LUIGI', 'DONKEY_KONG'],\n",
    "    # 'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/autoencoder/../slp_package/input_dataset.py:95: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOX               103744\n",
      "FALCO              90727\n",
      "MARTH              53731\n",
      "CAPTAIN_FALCON     38024\n",
      "SHEIK              27623\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "\n",
    "print(dataset.dataset['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Label   Count   Shift\n",
      "0             FOX  102522  552695\n",
      "1           FALCO   89892  475132\n",
      "2           MARTH   53342  316278\n",
      "3  CAPTAIN_FALCON   37633  197554\n",
      "4           SHEIK   27478  184374\n"
     ]
    }
   ],
   "source": [
    "labels_order =  dataset.number_of_segments_per_game(12,6000)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df  = dataset.train_test_split_dataframes(test_ratio = .20, val = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_inputs_np_sub_path</th>\n",
       "      <th>labels</th>\n",
       "      <th>encoded_labels</th>\n",
       "      <th>segment_start_index</th>\n",
       "      <th>segment_index</th>\n",
       "      <th>segment_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public\\FALCO\\bee06d45-fca6-437f-969a-901efa166...</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mango\\FALCO\\a24ef3f0-ab56-47e6-af18-5905aa43af...</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mango\\FALCO\\24b523a3-18da-4ba2-a986-d0c99b6228...</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mango\\FALCO\\60e0d81b-e0bd-420c-8fce-fe2b11645c...</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mango\\FALCO\\44e0962b-fdf7-4a16-acbe-61b5e5d609...</td>\n",
       "      <td>FALCO</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           player_inputs_np_sub_path labels  encoded_labels  \\\n",
       "0  public\\FALCO\\bee06d45-fca6-437f-969a-901efa166...  FALCO               1   \n",
       "1  mango\\FALCO\\a24ef3f0-ab56-47e6-af18-5905aa43af...  FALCO               1   \n",
       "2  mango\\FALCO\\24b523a3-18da-4ba2-a986-d0c99b6228...  FALCO               1   \n",
       "3  mango\\FALCO\\60e0d81b-e0bd-420c-8fce-fe2b11645c...  FALCO               1   \n",
       "4  mango\\FALCO\\44e0962b-fdf7-4a16-acbe-61b5e5d609...  FALCO               1   \n",
       "\n",
       "   segment_start_index  segment_index  segment_length  \n",
       "0                    0              0            4096  \n",
       "1                    0              0            4096  \n",
       "2                    0              0            4096  \n",
       "3                    0              0            4096  \n",
       "4                    0              0            4096  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        dropout = .2\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.LazyConv1d(9*16, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(9*16, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.LazyConv1d(9*8, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(9*8, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.LazyConv1d(9*4, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(9*4, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # nn.LazyConv1d(32, kernel_size = 9, stride = 1, padding = 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.LazyConv1d(32, kernel_size = 9, stride = 1, padding = 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool1d(kernel_size = 2),\n",
    "            # nn.Dropout(dropout),\n",
    "            \n",
    "            # nn.LazyConv1d(16, kernel_size = 9, stride = 1, padding = 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.LazyConv1d(16, kernel_size = 9, stride = 1, padding = 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool1d(kernel_size = 2),\n",
    "            # nn.Dropout(dropout),\n",
    "            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Upsampling and Conv1d to gradually increase dimensions back to original\n",
    "            \n",
    "            # nn.ConvTranspose1d(16, 16, kernel_size=4, stride=2, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv1d(16, 16, kernel_size=9, stride=1, padding=4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(dropout),\n",
    "            \n",
    "            nn.ConvTranspose1d(9*4, 9*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(9*4, 9*4, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.ConvTranspose1d(9*4, 9*8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(9*8, 9*8, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.ConvTranspose1d(9*8, 9*16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(9*16, 9*16, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv1d(9*16, 9, kernel_size=9, stride=1, padding=4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the model.\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder2, self).__init__()\n",
    "        dropout = .2\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.LazyConv1d(9*2, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(9*2, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 4),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.LazyConv1d(9*4, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(9*4, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 4),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.LazyConv1d(9*8, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(9*8, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 4),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.LazyConv1d(9*16, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv1d(9*16, kernel_size = 9, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 4),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # nn.LazyConv1d(16, kernel_size = 9, stride = 1, padding = 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.LazyConv1d(16, kernel_size = 9, stride = 1, padding = 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool1d(kernel_size = 2),\n",
    "            # nn.Dropout(dropout),\n",
    "            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Upsampling and Conv1d to gradually increase dimensions back to original\n",
    "            \n",
    "            nn.ConvTranspose1d(9*16, 9*16, kernel_size=9, stride=4, padding=3,output_padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(9*16, 9*16, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.ConvTranspose1d(9*16,9*8, kernel_size=9, stride=4, padding=3,output_padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(9*8, 9*8, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.ConvTranspose1d(9*8, 9*4, kernel_size=9, stride=4, padding=3,output_padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(9*4, 9*4, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.ConvTranspose1d(9*4, 9*2, kernel_size=9, stride=4, padding=3,output_padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(9*2, 9*2, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv1d(9*2, 9, kernel_size=9, stride=1, padding=4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the model.\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading game segments from compressed numpy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.file_paths = df['player_inputs_np_sub_path'].to_numpy()\n",
    "        self.encoded_labels = df['encoded_labels'].to_numpy()\n",
    "        self.segment_start_index = df['segment_start_index'].to_numpy()\n",
    "        # self.segment_index = df['segment_index'].to_numpy()\n",
    "        self.segment_length = df['segment_length'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads and returns a sample from the dataset at the specified index.\"\"\"\n",
    "        with gzip.open('/workspace/melee_project_data/input_np/' + self.file_paths[idx].replace('\\\\','/'), 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "\n",
    "        if self.transform:\n",
    "            segment = self.transform(segment)\n",
    "        \n",
    "        # Start and end of the segment\n",
    "        segment_start = self.segment_start_index[idx]\n",
    "        segment_end = self.segment_start_index[idx] + self.segment_length[idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        segment_tensor = torch.from_numpy(segment[:,segment_start:segment_end]).float()\n",
    "        # label_tensor = torch.tensor(self.encoded_labels[idx], dtype=torch.long)\n",
    "        return segment_tensor#, label_tensor\n",
    "    \n",
    "def prepare_data_loaders(train_df, test_df, batch_size, num_workers):\n",
    "    # Initialize datasets\n",
    "    train_dataset = TrainingDataset(train_df)\n",
    "    # val_dataset = TrainingDataset(file_paths_val, labels_val)\n",
    "    test_dataset = TrainingDataset(test_df)\n",
    "\n",
    "    # Initialize data loaders\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True,persistent_workers=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True,persistent_workers=True),\n",
    "        # 'val': DataLoader(val_dataset, batch_size=2**9, num_workers=num_workers, shuffle=False, pin_memory=True,persistent_workers=True)\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "\n",
    "\n",
    "# ''' Get a batch of data to see the size if we want that information. ''' \n",
    "# data_loader_iterator = iter(loaders['train'])\n",
    "# first_batch = next(data_loader_iterator)\n",
    "# print(first_batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, loaders, device, num_epochs=1):\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        \n",
    "        for batch_number, target_cpu in enumerate(train_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            \n",
    "            # Resets the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Runs the forward pass with autocasting.\n",
    "            with autocast():\n",
    "                output_gpu = model(target_gpu)\n",
    "                loss = criterion(output_gpu, target_gpu)\n",
    "            \n",
    "            # Scales loss and calls backward() to create scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Before calling step(), check for inf or NaN values in the gradients\n",
    "            if any(torch.isinf(p.grad).any() or torch.isnan(p.grad).any() for p in model.parameters() if p.grad is not None):\n",
    "                print(\"Warning: inf or NaN values in gradients!\")\n",
    "                \n",
    "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "            # otherwise, optimizer.step() is skipped.\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "\n",
    "            # Update progress\n",
    "            train_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix(loss=f'{train_loss / (batch_number + 1):.4f}')\n",
    "\n",
    "\n",
    "def evaluate_model(model, criterion, loaders, loader, device):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        eval_loader_tqdm = tqdm(loaders[loader], unit = 'batch',)\n",
    "        \n",
    "        for batch_number, target_cpu in enumerate(eval_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            output_gpu = model(target_gpu)\n",
    "            \n",
    "            eval_loss += criterion(output_gpu, target_gpu).item()\n",
    "            eval_loader_tqdm.set_postfix(loss=f'{eval_loss / (batch_number + 1):.4f}') \n",
    "            \n",
    "    print(f'Evaluated Loss: {eval_loss / len(loaders[loader]):.6f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 18, 4096]           1,476\n",
      "              ReLU-2             [-1, 18, 4096]               0\n",
      "            Conv1d-3             [-1, 18, 4096]           2,934\n",
      "              ReLU-4             [-1, 18, 4096]               0\n",
      "         MaxPool1d-5             [-1, 18, 1024]               0\n",
      "           Dropout-6             [-1, 18, 1024]               0\n",
      "            Conv1d-7             [-1, 36, 1024]           5,868\n",
      "              ReLU-8             [-1, 36, 1024]               0\n",
      "            Conv1d-9             [-1, 36, 1024]          11,700\n",
      "             ReLU-10             [-1, 36, 1024]               0\n",
      "        MaxPool1d-11              [-1, 36, 256]               0\n",
      "          Dropout-12              [-1, 36, 256]               0\n",
      "           Conv1d-13              [-1, 72, 256]          23,400\n",
      "             ReLU-14              [-1, 72, 256]               0\n",
      "           Conv1d-15              [-1, 72, 256]          46,728\n",
      "             ReLU-16              [-1, 72, 256]               0\n",
      "        MaxPool1d-17               [-1, 72, 64]               0\n",
      "          Dropout-18               [-1, 72, 64]               0\n",
      "           Conv1d-19              [-1, 144, 64]          93,456\n",
      "             ReLU-20              [-1, 144, 64]               0\n",
      "           Conv1d-21              [-1, 144, 64]         186,768\n",
      "             ReLU-22              [-1, 144, 64]               0\n",
      "        MaxPool1d-23              [-1, 144, 16]               0\n",
      "          Dropout-24              [-1, 144, 16]               0\n",
      "  ConvTranspose1d-25              [-1, 144, 64]         186,768\n",
      "             ReLU-26              [-1, 144, 64]               0\n",
      "           Conv1d-27              [-1, 144, 64]         186,768\n",
      "             ReLU-28              [-1, 144, 64]               0\n",
      "          Dropout-29              [-1, 144, 64]               0\n",
      "  ConvTranspose1d-30              [-1, 72, 256]          93,384\n",
      "             ReLU-31              [-1, 72, 256]               0\n",
      "           Conv1d-32              [-1, 72, 256]          46,728\n",
      "             ReLU-33              [-1, 72, 256]               0\n",
      "          Dropout-34              [-1, 72, 256]               0\n",
      "  ConvTranspose1d-35             [-1, 36, 1024]          23,364\n",
      "             ReLU-36             [-1, 36, 1024]               0\n",
      "           Conv1d-37             [-1, 36, 1024]          11,700\n",
      "             ReLU-38             [-1, 36, 1024]               0\n",
      "          Dropout-39             [-1, 36, 1024]               0\n",
      "  ConvTranspose1d-40             [-1, 18, 4096]           5,850\n",
      "             ReLU-41             [-1, 18, 4096]               0\n",
      "           Conv1d-42             [-1, 18, 4096]           2,934\n",
      "             ReLU-43             [-1, 18, 4096]               0\n",
      "          Dropout-44             [-1, 18, 4096]               0\n",
      "           Conv1d-45              [-1, 9, 4096]           1,467\n",
      "================================================================\n",
      "Total params: 931,293\n",
      "Trainable params: 931,293\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 10.30\n",
      "Params size (MB): 3.55\n",
      "Estimated Total Size (MB): 13.99\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Build model\n",
    "model = Autoencoder2().to('cuda')\n",
    "\n",
    "# With the size of an input we can get a model summary.\n",
    "summary(model, input_size=(9, 4096))\n",
    "\n",
    "# Check that the output shape and target shape match\n",
    "# training_example = torch.rand(9, 2 ** 12).to('cuda')\n",
    "# print('Target shape:', training_example.shape)\n",
    "# model.eval()\n",
    "# output = model(training_example)\n",
    "# print('Output shape:', output.shape)\n",
    "\n",
    "## Optionally compile the model\n",
    "# import torch_tensorrt\n",
    "model = torch.compile(model, mode = 'default')\n",
    "# model = torch.compile(model,mode = 'max-autotune')\n",
    "# model = torch.compile(model, backend=\"torch_tensorrt\")\n",
    "# model = torch.compile(model, backend=\"torch_tensorrt\",mode = 'max-autotune')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 125/125 [00:01<00:00, 102.19batch/s, loss=0.0833]\n",
      "Epoch 2/10: 100%|██████████| 125/125 [00:01<00:00, 119.27batch/s, loss=0.0825]\n",
      "Epoch 3/10: 100%|██████████| 125/125 [00:01<00:00, 124.40batch/s, loss=0.0821]\n",
      "Epoch 4/10: 100%|██████████| 125/125 [00:00<00:00, 125.35batch/s, loss=0.0819]\n",
      "Epoch 5/10: 100%|██████████| 125/125 [00:00<00:00, 129.04batch/s, loss=0.0815]\n",
      "Epoch 6/10: 100%|██████████| 125/125 [00:01<00:00, 124.00batch/s, loss=0.0812]\n",
      "Epoch 7/10: 100%|██████████| 125/125 [00:01<00:00, 122.80batch/s, loss=0.0809]\n",
      "Epoch 8/10: 100%|██████████| 125/125 [00:01<00:00, 124.98batch/s, loss=0.0807]\n",
      "Epoch 9/10: 100%|██████████| 125/125 [00:00<00:00, 128.13batch/s, loss=0.0804]\n",
      "Epoch 10/10: 100%|██████████| 125/125 [00:01<00:00, 124.31batch/s, loss=0.0801]\n",
      "100%|██████████| 32/32 [00:00<00:00, 90.65batch/s, loss=0.0775]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Loss: 0.077498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Pepare data loaders\n",
    "batch_size =  32\n",
    "num_workers = 16\n",
    "loaders = prepare_data_loaders(train_df, test_df, batch_size, num_workers)\n",
    "\n",
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# # This seems to sometimes help\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "# start_time = time.time()\n",
    "train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)\n",
    "# print(f'Batch Size: {batch_size}, Training time: {time.time() - start_time:.2f}')\n",
    "\n",
    "# Again, this sometimes seems to help\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Evaluate the trained model\n",
    "evaluate_model(model, criterion, loaders, 'test', 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Import Libraries </h1>\n",
    "We import all the necessary libraries, including Optuna, PyTorch, and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Read Data </h1>\n",
    "We read the data saved in `data_processing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42768, 9, 1024)\n",
      "[[[0.         0.         1.         ... 0.00714286 0.         0.98750001]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [3.14159265 0.98750001 0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.98750001 0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         1.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [1.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [1.         3.14159265 1.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         1.         ... 0.         0.         0.        ]\n",
      "  [0.         0.98750001 0.         ... 0.         1.         0.05714286]\n",
      "  [2.84772332 0.99255036 0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         1.87667521 0.99624293 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.         0.         ... 0.02142857 3.14159265 0.97500002]\n",
      "  [0.         0.         0.         ... 0.         0.         0.02857143]\n",
      "  [3.14159265 0.98750001 0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.02857143 3.14159265 0.98750001 ... 0.98433035 0.         0.        ]\n",
      "  [0.         0.         1.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.03571429 3.14159265 0.98750001]\n",
      "  [0.         0.         0.         ... 0.         0.         1.        ]\n",
      "  [3.14159265 0.98750001 0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         1.         0.        ]\n",
      "  [0.02857143 1.57079633 0.98750001 ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.01428571 ... 0.99812325 0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.02857143 0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.02142857]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.98750001 0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.01428571 3.14159265 0.98750001 ... 0.         0.         0.        ]\n",
      "  [0.         0.         1.         ... 0.         0.         0.        ]]]\n",
      "(42768,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X  = np.load('../data/training_inputs_cart_numpy_binary_1024.npy') # Stick input as cartesian coordinates.\n",
    "# X  = np.load('../data/training_inputs_polar_numpy_binary.npy') # Stick inputs as polar coordinates.\n",
    "# X  = np.load('../data/training_inputs_cart_numpy_binary.npy') # Stick input as cartesian coordinates.\n",
    "# X  = np.load('../data/training_inputs_polar_numpy_binary.npy') # Stick inputs as polar coordinates.\n",
    "# Load labels\n",
    "y  = np.load('../data/labes_is_sheik_numpy_binary_1024.npy')\n",
    "# Load labels\n",
    "# y  = np.load('../data/labes_is_sheik_numpy_binary.npy')\n",
    "# Print shape to make sure we have what we want.\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Splitting </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8554,)\n",
      "(25660,)\n",
      "(8554,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training + validation and holdout sets\n",
    "X_train_val, X_holdout, y_train_val, y_holdout = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Split training + validation set into separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "print(y_holdout.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Loader </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336\n"
     ]
    }
   ],
   "source": [
    "# Convert arrays into tensors and create dataset objects\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "holdout_dataset = TensorDataset(torch.tensor(X_holdout, dtype=torch.float32), torch.tensor(y_holdout, dtype=torch.float32))\n",
    "\n",
    "# Create data loaders\n",
    "num_batches = 32 # Can be tuned\n",
    "num_workers = 1 # Can be tuned\n",
    "\n",
    "batch_size = X.shape[0] // num_batches  # Can be tuned\n",
    "print(batch_size)\n",
    "# batch_size = 64  # Can be tuned\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    # Apply sigmoid and threshold at 0.5\n",
    "    epsilon = 10 ** -44\n",
    "    preds = torch.sigmoid(outputs) >= 0.5\n",
    "    correct = (preds.squeeze().long() == labels.long()).float().sum()\n",
    "    return correct / labels.shape[0]\n",
    "\n",
    "# class myLoss(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, pos_weight=1):\n",
    "#       super().__init__()\n",
    "#       self.pos_weight = pos_weight\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "#       epsilon = 10 ** -44\n",
    "#       input = input.sigmoid().clamp(epsilon, 1 - epsilon)\n",
    "\n",
    "#       my_bce_loss = -1 * (self.pos_weight * target * torch.log(input)\n",
    "#                           + (1 - target) * torch.log(1 - input))\n",
    "#       add_loss = (target - 0.5) ** 2 * 4\n",
    "#       mean_loss = (my_bce_loss * add_loss).mean()\n",
    "#       return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define the Customizable Network </h1>\n",
    "We define the neural network architecture. We'll use Optuna to suggest hyperparameters for convolutional layers, optional ReLU activation, max pooling layers, and linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, trial):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        # Fixed dropout rate (not tuned by Optuna)\n",
    "        dropout_rate = 0.5\n",
    "\n",
    "        # Convolutional layers setup\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.activations = []\n",
    "        self.dropouts = nn.ModuleList()\n",
    "\n",
    "        num_layers = trial.suggest_int(f\"num_conv_layers\", 3, 5)\n",
    "        in_channels = 9  # Fixed input channel size\n",
    "\n",
    "        \n",
    "        \n",
    "        ######################################################################################################\n",
    "        # In length is 2 ** 10\n",
    "        # Padding is set up so that the out length is always reduced by 1 / 2 ** out_length_reduction_exponent\n",
    "        # The length of a kernel is: kernel + (dilation - 1) * (kernel_size - 1)\n",
    "        # The max lenght of a kernel is 25 which is kernel_size = 7 and dilation = 4\n",
    "        # The in lenght can never be less than 25\n",
    "        # Since the in lenght is always a power of 2, the in lenght can be no less than 2 ** 5 = 32,\n",
    "        # we need to make sure not to reduce the in lenght too much, we keep track of\n",
    "        # how much we can still reduce the length by using length_reduction_power_left which is set to 5.\n",
    "        ######################################################################################################\n",
    "        length_reduction_exporent_remaining = 5\n",
    "        in_length_exponent = 10\n",
    "        for i in range(num_layers):  # Convolutional layers\n",
    "            ###########################\n",
    "            # In length is a power of 2\n",
    "            ###########################\n",
    "            out_channels = trial.suggest_int(f\"conv_{i}_out_channels\", 16, 128)\n",
    "            # kernel_size = trial.suggest_int(f\"conv_{i}_kernel_size\", 3, 7, step=2)\n",
    "            k = trial.suggest_int(f\"conv_{i}_kernel_size_power\", 1, 3)  # can safely change 3 to be anything\n",
    "            kernel_size = 2 * k + 1\n",
    "            dilation = trial.suggest_int(f\"conv_{i}_dilation\", 1, 4)\n",
    "            out_length_reduction_exponent = trial.suggest_int(f\"conv_{i}_out_length_reduction_exponent\", 0, min(2,length_reduction_exporent_remaining))\n",
    "            # Keep track of how much reducing we still can do\n",
    "            length_reduction_exporent_remaining -= out_length_reduction_exponent\n",
    "            in_length_exponent -= out_length_reduction_exponent\n",
    "            # Set stride\n",
    "            stride = 2 ** out_length_reduction_exponent\n",
    "            # Padding is chosen so that out length is a power of 2\n",
    "            # there is a floor in the formula. If we want to use more than 2 for out_length_reduction_exponent, we neen do caluclate the cases\n",
    "            if (out_length_reduction_exponent == 2) and (((dilation * k) % 2) == 1):\n",
    "                padding = dilation * k - 1\n",
    "            else:\n",
    "                padding = dilation * k\n",
    "                \n",
    "            self.conv_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size,stride, padding, dilation=dilation))\n",
    "            in_channels = out_channels  # Update in_channels for the next layer\n",
    "\n",
    "            # Optional ReLU activation\n",
    "            use_activation = trial.suggest_categorical(f\"conv_{i}_activation\", [True, False])\n",
    "            self.activations.append(use_activation)\n",
    "\n",
    "            # Add dropout after each activation\n",
    "            self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        \n",
    "        # Max pooling layer\n",
    "        # The kernel can be a power of two, up to the in lenght\n",
    "        # In length of the output will be 2 ** out_length_exponent\n",
    "        # and lenght can be 1, 2, 4, 8, 16, 32\n",
    "        \n",
    "        kernel_exponent = trial.suggest_int(f\"maxpool_kernel_exponent\",length_reduction_exporent_remaining , in_length_exponent)\n",
    "        kernel_size = 2 ** kernel_exponent\n",
    "        in_length_exponent -= kernel_exponent\n",
    "        \n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        '''# Optional additional convolutional layer\n",
    "        self.use_conv4 = trial.suggest_categorical(\"use_conv4\", [True, False])\n",
    "        if self.use_conv4:\n",
    "            self.conv4 = nn.Conv1d(in_channels, \n",
    "                                   trial.suggest_int(\"conv4_out_channels\", 32, 128), \n",
    "                                   trial.suggest_int(\"conv4_kernel_size\", 3, 7, step=2), \n",
    "                                   stride=trial.suggest_int(\"conv4_stride\", 1, 2),\n",
    "                                   dilation=trial.suggest_int(\"conv4_dilation\", 1, 4))\n",
    "            self.use_conv4_activation = trial.suggest_categorical(\"conv4_activation\", [True, False])\n",
    "            in_channels = self.conv4.out_channels  # Update in_channels in case this layer is used\n",
    "            self.conv4_dropout = nn.Dropout(dropout_rate)  # Dropout after optional conv4\n",
    "\n",
    "        # Optional second max pooling layer\n",
    "        self.use_pool2 = trial.suggest_categorical(\"use_pool2\", [True, False])\n",
    "        if self.use_pool2:\n",
    "            self.pool2 = nn.MaxPool1d(kernel_size=trial.suggest_int(\"pool2_kernel\", 2, 4),\n",
    "                                      stride=trial.suggest_int(\"pool2_stride\", 2, 4))\n",
    "        '''\n",
    "        # The length right now should be 2 ** in_length_exponent, so we can be exact in our first lineal layer\n",
    "        self.fc1 = nn.Linear(out_channels * 2 ** in_length_exponent, trial.suggest_int(\"fc1_out_features\", 64, 256))\n",
    "        # self.fc1 = nn.LazyLinear(trial.suggest_int(\"fc1_out_features\", 64, 256))\n",
    "        self.fc1_dropout = nn.Dropout(dropout_rate)  # Dropout after fc1\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, trial.suggest_int(\"fc2_out_features\", 32, 128))\n",
    "        self.fc2_dropout = nn.Dropout(dropout_rate)  # Dropout after fc2\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, 1)  # Output layer with 1 unit for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with optional ReLU and fixed dropout\n",
    "        for i, (conv_layer, dropout) in enumerate(zip(self.conv_layers, self.dropouts)):\n",
    "            x = conv_layer(x)\n",
    "            if self.activations[i]:\n",
    "                x = F.relu(x)\n",
    "            x = dropout(x)\n",
    "\n",
    "        # Optional max pooling after conv layers\n",
    "        # if self.use_pool1:\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Optional additional conv layer with optional ReLU and fixed dropout\n",
    "        '''if self.use_conv4:\n",
    "            x = self.conv4(x)\n",
    "            if self.use_conv4_activation:\n",
    "                x = F.relu(x)\n",
    "            x = self.conv4_dropout(x)\n",
    "\n",
    "        # Optional second max pooling layer\n",
    "        if self.use_pool2:\n",
    "            x = self.pool2(x)\n",
    "        '''\n",
    "        # Flatten for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_dropout(x)\n",
    "        x = self.fc3(x)  # Output without activation for BCEWithLogitsLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomNet(nn.Module):\n",
    "#     def __init__(self, trial):\n",
    "#         super(CustomNet, self).__init__()\n",
    "\n",
    "#         # Fixed dropout rate (not tuned by Optuna)\n",
    "#         dropout_rate = 0.5\n",
    "\n",
    "#         # Convolutional layers setup\n",
    "#         self.conv_layers = nn.ModuleList()\n",
    "#         self.activations = []\n",
    "#         self.dropouts = nn.ModuleList()\n",
    "\n",
    "#         num_layers = trial.suggest_int(f\"num_conv_layers\", 1, 5)\n",
    "#         in_channels = 9  # Fixed input channel size\n",
    "\n",
    "#         for i in range(num_layers):  # Convolutional layers\n",
    "#             out_channels = trial.suggest_int(f\"conv_{i}_out_channels\", 16, 64)\n",
    "#             kernel_size = trial.suggest_int(f\"conv_{i}_kernel_size\", 3, 7, step=2)\n",
    "#             dilation = trial.suggest_int(f\"conv_{i}_dilation\", 1, 4)\n",
    "#             stride = trial.suggest_int(f\"conv_{i}_stride\", 1, 4)\n",
    "#             self.conv_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, stride, dilation=dilation))\n",
    "#             in_channels = out_channels  # Update in_channels for the next layer\n",
    "\n",
    "#             # Optional ReLU activation\n",
    "#             use_activation = trial.suggest_categorical(f\"conv_{i}_activation\", [True, False])\n",
    "#             self.activations.append(use_activation)\n",
    "\n",
    "#             # Add dropout after each activation\n",
    "#             self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "#         # Max pooling layer\n",
    "#         #self.use_pool1 = trial.suggest_categorical(\"use_pool1\", [True, False])\n",
    "#         #if self.use_pool1:\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=trial.suggest_int(\"pool1_kernel\", 2, 4),\n",
    "#                                     stride=trial.suggest_int(\"pool1_stride\", 2, 4))\n",
    "\n",
    "#         '''# Optional additional convolutional layer\n",
    "#         self.use_conv4 = trial.suggest_categorical(\"use_conv4\", [True, False])\n",
    "#         if self.use_conv4:\n",
    "#             self.conv4 = nn.Conv1d(in_channels, \n",
    "#                                    trial.suggest_int(\"conv4_out_channels\", 32, 128), \n",
    "#                                    trial.suggest_int(\"conv4_kernel_size\", 3, 7, step=2), \n",
    "#                                    stride=trial.suggest_int(\"conv4_stride\", 1, 2),\n",
    "#                                    dilation=trial.suggest_int(\"conv4_dilation\", 1, 4))\n",
    "#             self.use_conv4_activation = trial.suggest_categorical(\"conv4_activation\", [True, False])\n",
    "#             in_channels = self.conv4.out_channels  # Update in_channels in case this layer is used\n",
    "#             self.conv4_dropout = nn.Dropout(dropout_rate)  # Dropout after optional conv4\n",
    "\n",
    "#         # Optional second max pooling layer\n",
    "#         self.use_pool2 = trial.suggest_categorical(\"use_pool2\", [True, False])\n",
    "#         if self.use_pool2:\n",
    "#             self.pool2 = nn.MaxPool1d(kernel_size=trial.suggest_int(\"pool2_kernel\", 2, 4),\n",
    "#                                       stride=trial.suggest_int(\"pool2_stride\", 2, 4))\n",
    "#         '''\n",
    "#         # Fully connected layers setup\n",
    "#         self.fc1 = nn.LazyLinear(trial.suggest_int(\"fc1_out_features\", 64, 256))\n",
    "#         self.fc1_dropout = nn.Dropout(dropout_rate)  # Dropout after fc1\n",
    "#         self.fc2 = nn.Linear(self.fc1.out_features, trial.suggest_int(\"fc2_out_features\", 32, 128))\n",
    "#         self.fc2_dropout = nn.Dropout(dropout_rate)  # Dropout after fc2\n",
    "#         self.fc3 = nn.Linear(self.fc2.out_features, 1)  # Output layer with 1 unit for binary classification\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Apply convolutional layers with optional ReLU and fixed dropout\n",
    "#         for i, (conv_layer, dropout) in enumerate(zip(self.conv_layers, self.dropouts)):\n",
    "#             x = conv_layer(x)\n",
    "#             if self.activations[i]:\n",
    "#                 x = F.relu(x)\n",
    "#             x = dropout(x)\n",
    "\n",
    "#         # Optional max pooling after conv layers\n",
    "#         # if self.use_pool1:\n",
    "#         x = self.pool1(x)\n",
    "\n",
    "#         # Optional additional conv layer with optional ReLU and fixed dropout\n",
    "#         '''if self.use_conv4:\n",
    "#             x = self.conv4(x)\n",
    "#             if self.use_conv4_activation:\n",
    "#                 x = F.relu(x)\n",
    "#             x = self.conv4_dropout(x)\n",
    "\n",
    "#         # Optional second max pooling layer\n",
    "#         if self.use_pool2:\n",
    "#             x = self.pool2(x)\n",
    "#         '''\n",
    "#         # Flatten for fully connected layers\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc1_dropout(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc2_dropout(x)\n",
    "#         x = self.fc3(x)  # Output without activation for BCEWithLogitsLoss\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define the Objective Function </h1>\n",
    "We define the objective function for Optuna, which involves training and validating the model with the suggested hyperparameters to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model with hyperparameters suggested by Optuna\n",
    "    model = CustomNet(trial).to(device)\n",
    "\n",
    "    # Load and prepare data (assuming X and y are already loaded)\n",
    "    # Splitting, converting to TensorDataset, and DataLoader setup would go here\n",
    "\n",
    "    # Define the optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    eps = .0000000001\n",
    "    # criterion = myLoss\n",
    "    def train_epoch(model, dataloader, optimizer, criterion):\n",
    "        eps = .0000000001\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze() + eps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += calculate_accuracy(outputs + eps, labels) * inputs.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "        return epoch_loss, epoch_accuracy\n",
    "\n",
    "    def validate_epoch(model, dataloader, criterion):\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs) + eps\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "                \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "        return epoch_loss, epoch_accuracy\n",
    "\n",
    "    def evaluate_holdout(model, dataloader, criterion):\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        with torch.no_grad():  # No gradients needed\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "                \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "        return epoch_loss, epoch_accuracy\n",
    "                \n",
    "    # Early stopping parameters\n",
    "    patience = 10  # Number of epochs to wait for improvement before stopping\n",
    "    min_delta = 0.001  # Minimum change to qualify as an improvement\n",
    "    min_overfit = .2\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    epochs_overfit = 0\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    epochs = 250\n",
    "    # epochs = 2\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "\n",
    "        # Early Stopping check\n",
    "        if (val_loss + min_delta) < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if abs(train_loss - val_loss) < min_overfit:\n",
    "            epochs_overfit = 0\n",
    "        else:\n",
    "            epochs_overfit += 1\n",
    "\n",
    "        # Print progress every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            tqdm.write(f'Epoch {epoch+1}/{epochs} - Duration: {epoch_duration:.2f}s - Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f} - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Check early stopping condition\n",
    "        if (epochs_no_improve >= patience) or (epochs_overfit >= patience):\n",
    "            tqdm.write(f'Early stopping triggered at epoch {epoch + 1}')\n",
    "            # holdout_loss, holdout_accuracy = evaluate_holdout(model, holdout_loader, criterion)\n",
    "            # print(f'Holdout Loss: {holdout_loss:.4f}, Accuracy: {holdout_accuracy:.4f}')\n",
    "            break\n",
    "\n",
    "    # Evaluate model on holdout set after training is complete (if necessary)\n",
    "    holdout_loss, holdout_accuracy = evaluate_holdout(model, holdout_loader, criterion)\n",
    "    print(f'Holdout Loss: {holdout_loss:.4f}, Accuracy: {holdout_accuracy:.4f}')\n",
    "    \n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     # Device configuration\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # Initialize the model with hyperparameters suggested by Optuna\n",
    "#     model = CustomNet(trial).to(device)\n",
    "\n",
    "#     # Load and prepare data (assuming X and y are already loaded)\n",
    "#     # Splitting, converting to TensorDataset, and DataLoader setup would go here\n",
    "\n",
    "#     # Define the optimizer and criterion\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     def train_epoch(model, dataloader, optimizer, criterion):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         running_accuracy = 0.0\n",
    "#         for inputs, labels in dataloader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs.squeeze(), labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "            \n",
    "#         epoch_loss = running_loss / len(dataloader.dataset)\n",
    "#         epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "#         return epoch_loss, epoch_accuracy\n",
    "\n",
    "#     def validate_epoch(model, dataloader, criterion):\n",
    "#         model.eval()\n",
    "#         running_loss = 0.0\n",
    "#         running_accuracy = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in dataloader:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "                \n",
    "#         epoch_loss = running_loss / len(dataloader.dataset)\n",
    "#         epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "#         return epoch_loss, epoch_accuracy\n",
    "\n",
    "#     def evaluate_holdout(model, dataloader, criterion):\n",
    "#         model.eval()  # Set model to evaluation mode\n",
    "#         running_loss = 0.0\n",
    "#         running_accuracy = 0.0\n",
    "#         with torch.no_grad():  # No gradients needed\n",
    "#             for inputs, labels in dataloader:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "                \n",
    "#         epoch_loss = running_loss / len(dataloader.dataset)\n",
    "#         epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "#         return epoch_loss, epoch_accuracy\n",
    "                \n",
    "#     # Early stopping parameters\n",
    "#     patience = 10  # Number of epochs to wait for improvement before stopping\n",
    "#     min_delta = 0.001  # Minimum change to qualify as an improvement\n",
    "#     best_val_loss = float('inf')\n",
    "#     epochs_no_improve = 0\n",
    "\n",
    "#     # Training loop with early stopping\n",
    "#     epochs = 250\n",
    "#     for epoch in range(epochs):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion)\n",
    "#         val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
    "        \n",
    "#         end_time = time.time()\n",
    "#         epoch_duration = end_time - start_time\n",
    "\n",
    "#         # Early Stopping check\n",
    "#         if (val_loss + min_delta) < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             epochs_no_improve = 0\n",
    "#         else:\n",
    "#             epochs_no_improve += 1\n",
    "\n",
    "#         # Print progress every 5 epochs\n",
    "#         if (epoch + 1) % 5 == 0:\n",
    "#             tqdm.write(f'Epoch {epoch+1}/{epochs} - Duration: {epoch_duration:.2f}s - Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f} - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "#         # Check early stopping condition\n",
    "#         if epochs_no_improve >= patience:\n",
    "#             tqdm.write(f'Early stopping triggered at epoch {epoch + 1}')\n",
    "#             # holdout_loss, holdout_accuracy = evaluate_holdout(model, holdout_loader, criterion)\n",
    "#             # print(f'Holdout Loss: {holdout_loss:.4f}, Accuracy: {holdout_accuracy:.4f}')\n",
    "#             break\n",
    "\n",
    "#     # Evaluate model on holdout set after training is complete (if necessary)\n",
    "#     holdout_loss, holdout_accuracy = evaluate_holdout(model, holdout_loader, criterion)\n",
    "#     print(f'Holdout Loss: {holdout_loss:.4f}, Accuracy: {holdout_accuracy:.4f}')\n",
    "    \n",
    "#     return best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define Callback Function </h1>\n",
    "We define a callback function that will be called by the Optuna study after each trial. This function will check if the current trial has a better value than the previous best and, if so, will save its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params_if_best(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        # Save the best parameters so far\n",
    "        print(f\"New best trial at trial {trial.number}:\")\n",
    "        print(f\"  Value: {trial.value}\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Run the Optimization </h1>\n",
    "We create an Optuna study and then iterate the optimizer separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-16 11:29:00,843] A new study created in memory with name: no-name-0620926b-6405-49eb-b112-216c82d1b570\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062a8851778a4c8cb295ea52fe5deceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/250 - Duration: 3.83s - Training Loss: nan, Accuracy: 0.5000 - Validation Loss: nan, Accuracy: 0.5000\n",
      "[W 2024-02-16 11:29:23,657] Trial 0 failed with parameters: {'num_conv_layers': 3, 'conv_0_out_channels': 21, 'conv_0_kernel_size_power': 1, 'conv_0_dilation': 1, 'conv_0_out_length_reduction_exponent': 1, 'conv_0_activation': True, 'conv_1_out_channels': 38, 'conv_1_kernel_size_power': 2, 'conv_1_dilation': 4, 'conv_1_out_length_reduction_exponent': 1, 'conv_1_activation': False, 'conv_2_out_channels': 124, 'conv_2_kernel_size_power': 3, 'conv_2_dilation': 4, 'conv_2_out_length_reduction_exponent': 2, 'conv_2_activation': False, 'maxpool_kernel_exponent': 3, 'fc1_out_features': 209, 'fc2_out_features': 114} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jaspa\\AppData\\Local\\Temp\\ipykernel_20544\\3387909719.py\", line 85, in objective\n",
      "    val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jaspa\\AppData\\Local\\Temp\\ipykernel_20544\\3387909719.py\", line 41, in validate_epoch\n",
      "    for inputs, labels in dataloader:\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1329, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1295, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1133, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\queues.py\", line 113, in get\n",
      "    if not self._poll(timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\connection.py\", line 256, in poll\n",
      "    return self._poll(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\connection.py\", line 329, in _poll\n",
      "    return bool(wait([self], timeout))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\connection.py\", line 1066, in wait\n",
      "    ready_handles = _exhaustive_wait(waithandle_to_obj.keys(), timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\connection.py\", line 998, in _exhaustive_wait\n",
      "    res = _winapi.WaitForMultipleObjects(L, False, timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-02-16 11:29:23,658] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msave_params_if_best\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Print the overall best hyperparameters\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial overall:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[32], line 85\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     82\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     84\u001b[0m train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion)\n\u001b[1;32m---> 85\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     88\u001b[0m epoch_duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[32], line 41\u001b[0m, in \u001b[0;36mobjective.<locals>.validate_epoch\u001b[1;34m(model, dataloader, criterion)\u001b[0m\n\u001b[0;32m     39\u001b[0m running_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\connection.py:329\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    327\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\connection.py:1066\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m   1063\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m   1064\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1066\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\multiprocessing\\connection.py:998\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    996\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 998\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "study.optimize(objective, n_trials=1, show_progress_bar=True, timeout=3600*3, callbacks=[save_params_if_best])\n",
    "\n",
    "# Print the overall best hyperparameters\n",
    "print(\"Best trial overall:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempted to use an uninitialized parameter in <method 'numel' of 'torch._C.TensorBase' objects>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Trainable Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_param\n\u001b[1;32m---> 13\u001b[0m \u001b[43mcount_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCustomNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36mcount_parameters\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, parameter \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter\u001b[38;5;241m.\u001b[39mrequires_grad: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     table\u001b[38;5;241m.\u001b[39madd_row([name, params])\n\u001b[0;32m      8\u001b[0m     total_params\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mparams\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\parameter.py:158\u001b[0m, in \u001b[0;36mUninitializedTensorMixin.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempted to use an uninitialized parameter in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis error happens when you are using a `LazyModule` or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicitly manipulating `torch.nn.parameter.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects. When using LazyModules Call `forward` with a dummy batch \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto initialize the parameters before calling torch functions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Attempted to use an uninitialized parameter in <method 'numel' of 'torch._C.TensorBase' objects>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    table = PrettyTable(['Modules', 'Parameters'])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f'Total Trainable Params: {total_params}')\n",
    "    return total_param\n",
    "\n",
    "count_parameters(CustomNet(trial))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

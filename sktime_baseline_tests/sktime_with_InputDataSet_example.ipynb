{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> InputDataSet Class and sktime example </h1>\n",
    "First we import packages and check we have access to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "from sktime.classification.deep_learning.resnet import ResNetClassifier\n",
    "from sktime.classification.deep_learning import InceptionTimeClassifier\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Define functions </h2>\n",
    "A function to asses the models we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def asses_model(model_name, y_pred, y_test):\n",
    "#     print()\n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     kappa = cohen_kappa_score(y_pred, y_test)\n",
    "\n",
    "#     # Print accuracy and Cohen Kappa score with explanations\n",
    "#     print(f'Accuracy of {model_name}: {accuracy:.4f}')\n",
    "#     # Cohen Kappa Score measures the agreement between the predicted and actual labels adjusted for chance.\n",
    "#     print(f'Cohen Kappa Score of {model_name}: {kappa:.4f}')\n",
    "\n",
    "#     # Calculate and sort the normalized predicted label count\n",
    "#     unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "#     unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "#     normalized_counts_pred = {k: v / counts_test[np.where(unique_test == k)[0][0]] for k, v in zip(unique_pred, counts_pred)}\n",
    "    \n",
    "#     # Sort by the number of predictions\n",
    "#     sorted_keys = sorted(normalized_counts_pred, key=normalized_counts_pred.get, reverse=True)\n",
    "#     sorted_values = [normalized_counts_pred[k] - 1 for k in sorted_keys]\n",
    "\n",
    "#     # Plotting the percent the model over or under predicted the labels\n",
    "#     plt.figure(figsize=(2*unique_pred.shape[0], 4))\n",
    "#     bars = plt.bar(sorted_keys, sorted_values, color=['green' if x > 0 else 'Blue' for x in sorted_values])\n",
    "#     plt.title(f'Percent Model {model_name} Over or Under Predicted Labels')\n",
    "#     plt.xlabel('Labels')\n",
    "#     plt.ylabel('Percent Over/Under Prediction')\n",
    "    \n",
    "#     # Center y-axis and set equal extension above and below\n",
    "#     max_extent = max(abs(min(sorted_values)), abs(max(sorted_values)))* 1.05\n",
    "#     plt.ylim(-max_extent, max_extent)\n",
    "#     plt.axhline(y=0, color='gray', linewidth=0.8)  # Add line at y=0\n",
    "#     plt.show()\n",
    "\n",
    "#     # Display confusion matrices sorted by frequency of the prediction\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "#     sorted_labels = sorted(unique_pred, key=lambda x: -counts_pred[np.where(unique_pred == x)[0][0]])\n",
    "#     for i, norm in enumerate([None, 'true', 'pred']):\n",
    "#         ax = axes[i]\n",
    "#         ConfusionMatrixDisplay.from_predictions(\n",
    "#             y_test, y_pred, normalize=norm, ax=ax,\n",
    "#             xticks_rotation='vertical', labels=sorted_labels\n",
    "#         )\n",
    "#         ax.title.set_text(f'{model_name} Confusion Matrix ({\"Not Normalized\" if norm is None else \"Normalized by \" + norm})')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "# def asses_model(model_name, y_pred, y_test, labels_order):\n",
    "#     print()\n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     kappa = cohen_kappa_score(y_pred, y_test)\n",
    "\n",
    "#     # Print accuracy and Cohen Kappa score with explanations\n",
    "#     print(f'Accuracy of {model_name}: {accuracy:.4f}')\n",
    "#     print(f'Cohen Kappa Score of {model_name}: {kappa:.4f}')\n",
    "\n",
    "#     # Calculate the normalized predicted label count\n",
    "#     unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "#     unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "#     normalized_counts_pred = {k: v / counts_test[np.where(unique_test == k)[0][0]] for k, v in zip(unique_pred, counts_pred)}\n",
    "    \n",
    "#     # Calculate the percent the model over or under predicted the labels using the specified label order\n",
    "#     sorted_values = [normalized_counts_pred[k] - 1 if k in normalized_counts_pred else 0 for k in labels_order]\n",
    "\n",
    "#     # Plotting the percent the model over or under predicted the labels\n",
    "#     plt.figure(figsize=(2*len(labels_order), 4))\n",
    "#     plt.bar(labels_order, sorted_values, color=['green' if x > 0 else 'blue' for x in sorted_values])\n",
    "#     plt.title(f'Percent Model {model_name} Over or Under Predicted Labels')\n",
    "#     plt.xlabel('Labels')\n",
    "#     plt.ylabel('Percent Over/Under Prediction')\n",
    "    \n",
    "#     # Center y-axis and set equal extension above and below\n",
    "#     max_extent = max(abs(min(sorted_values)), abs(max(sorted_values))) * 1.05\n",
    "#     plt.ylim(-max_extent, max_extent)\n",
    "#     plt.axhline(y=0, color='gray', linewidth=0.8)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Display confusion matrices using the specified label order\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "#     for i, norm in enumerate([None, 'true', 'pred']):\n",
    "#         ax = axes[i]\n",
    "#         ConfusionMatrixDisplay.from_predictions(\n",
    "#             y_test, y_pred, normalize=norm, ax=ax,\n",
    "#             xticks_rotation='vertical', labels=labels_order\n",
    "#         )\n",
    "#         ax.title.set_text(f'{model_name} Confusion Matrix ({\"Not Normalized\" if norm is None else \"Normalized by \" + norm})')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "def asses_model(model_name, y_pred, y_test, labels_order):\n",
    "    print()\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_pred, y_test)\n",
    "\n",
    "    # Print accuracy and Cohen Kappa score with explanations\n",
    "    print(f'Accuracy of {model_name}: {accuracy:.4f}')\n",
    "    print(f'Cohen Kappa Score of {model_name}: {kappa:.4f}')\n",
    "\n",
    "    # Calculate the normalized predicted label count\n",
    "    unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "    normalized_counts_pred = {k: v / counts_test[np.where(unique_test == k)[0][0]] for k, v in zip(unique_pred, counts_pred)}\n",
    "    \n",
    "    # Calculate the percent the model over or under predicted the labels using the specified label order\n",
    "    sorted_values = [normalized_counts_pred[k] - 1 if k in normalized_counts_pred else 0 for k in labels_order]\n",
    "\n",
    "    # Plotting the percent the model over or under predicted the labels\n",
    "    plt.figure(figsize=(2*len(labels_order), 4))\n",
    "    plt.bar(labels_order, sorted_values, color=['green' if x > 0 else 'blue' for x in sorted_values])\n",
    "    plt.title(f'Percent Model {model_name} Over or Under Predicted Labels')\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Percent Over/Under Prediction')\n",
    "    \n",
    "    # Center y-axis and set equal extension above and below\n",
    "    max_extent = max(abs(min(sorted_values)), abs(max(sorted_values))) * 1.05\n",
    "    plt.ylim(-max_extent, max_extent)\n",
    "    plt.axhline(y=0, color='gray', linewidth=0.8)\n",
    "    plt.show()\n",
    "\n",
    "    # Display each confusion matrix on its own row\n",
    "    for norm in [None, 'true', 'pred']:\n",
    "        plt.figure(figsize=(len(labels_order), len(labels_order)))\n",
    "        ax = plt.gca()\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test, y_pred, normalize=norm, ax=ax,\n",
    "            xticks_rotation='vertical', labels=labels_order\n",
    "        )\n",
    "        ax.title.set_text(f'{model_name} Confusion Matrix ({\"Not Normalized\" if norm is None else \"Normalized by \" + norm})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Define Features </h2>\n",
    "Here we define the parameters for the InputDataSet object we will create. It includes the source of the data we want and a list of features we will restrict ourselves to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We classify 5 characters on competitive stages\n",
    "\n",
    "source_data = ['public','ranked']\n",
    "\n",
    "general_features = {\n",
    "    # 'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We classify 5 characters on competitive stages\n",
    "\n",
    "# source_data = ['ranked']\n",
    "\n",
    "# general_features = {\n",
    "#     'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "#     'num_players': [2],\n",
    "#     'conclusive': [True]\n",
    "# }\n",
    "# player_features = {\n",
    "#     # 'netplay_code': ['MANG#0'],\n",
    "#     'character_name': ['FOX'],\n",
    "#     'type_name': ['HUMAN']\n",
    "    \n",
    "# }\n",
    "# opposing_player_features = {\n",
    "#     'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "#     # 'netplay_code': ['KOD#0', 'ZAIN#0'],\n",
    "#     'type_name': ['HUMAN']\n",
    "# }\n",
    "# label_info = {\n",
    "#     'source': ['opposing_player'], # Can be 'general', 'player', 'opposing_player'\n",
    "#     # 'feature': ['netplay_code']\n",
    "#     'feature': ['character_name']\n",
    "# }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We classify 5 characters on competitive stages\n",
    "\n",
    "# source_data = ['ranked']\n",
    "\n",
    "# general_features = {\n",
    "#     'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "#     'num_players': [2],\n",
    "#     'conclusive': [True]\n",
    "# }\n",
    "# player_features = {\n",
    "#     # 'netplay_code': ['MANG#0'],\n",
    "#     'character_name': ['FALCO'],\n",
    "#     'type_name': ['HUMAN']\n",
    "    \n",
    "# }\n",
    "# opposing_player_features = {\n",
    "#     'character_name': ['FOX', 'JIGGLYPUFF'],\n",
    "#     # 'netplay_code': ['KOD#0', 'ZAIN#0'],\n",
    "#     'type_name': ['HUMAN']\n",
    "# }\n",
    "# label_info = {\n",
    "#     'source': ['opposing_player'], # Can be 'general', 'player', 'opposing_player'\n",
    "#     # 'feature': ['netplay_code']\n",
    "#     'feature': ['character_name']\n",
    "# }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We classify 5 characters on competitive stages\n",
    "\n",
    "# source_data = ['mango']\n",
    "\n",
    "# general_features = {\n",
    "#     'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "#     'num_players': [2],\n",
    "#     'conclusive': [True]\n",
    "# }\n",
    "# player_features = {\n",
    "#     'netplay_code': ['MANG#0'],\n",
    "#     'character_name': ['FALCO'],\n",
    "#     'type_name': ['HUMAN']\n",
    "    \n",
    "# }\n",
    "# opposing_player_features = {\n",
    "#     'character_name': ['FOX', 'MARTH'],\n",
    "#     # 'netplay_code': ['KOD#0', 'ZAIN#0'],\n",
    "#     'type_name': ['HUMAN']\n",
    "# }\n",
    "# label_info = {\n",
    "#     'source': ['opposing_player'], # Can be 'general', 'player', 'opposing_player'\n",
    "#     # 'feature': ['netplay_code']\n",
    "#     'feature': ['character_name']\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create Dataset Object </h2>\n",
    "We create the dataset object and check the number of games for each label in our data and print the first several rows of the dataset to make sure it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "\n",
    "print(dataset.dataset['labels'].value_counts())\n",
    "# print(list(dataset.dataset['labels'].unique()))\n",
    "# dataset.dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Call number_of_segments_per_game() </h2>\n",
    "We set the length of the segments and the number of segments per label that we want in the dataset we are going to prepare. In the column 'Count' we see the number of games we have for each label (this is less than the value counts earlier because we discard games that are too short). In the column 'Shift' we see how much each segment will be shifted by. If the shift is less than the length of the segments, the data we prepare will contain overlaping segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_order =  dataset.number_of_segments_per_game(10, 50000)\n",
    "# labels_order =  dataset.number_of_segments_per_game(11, 10000)\n",
    "# labels_order =  dataset.number_of_segments_per_game(11, 100)\n",
    "labels_order =  dataset.number_of_segments_per_game(12,5000)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Split data and prepare it for the model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.number_of_segments_per_game(10, 10000)\n",
    "X_train, X_test, y_train, y_test  = dataset.train_test_split_numpy(test_ratio = .20, val = False)\n",
    "# Convert your data to float16\n",
    "# X_train = X_train.astype('float16')\n",
    "# y_train = y_train.astype('float16')\n",
    "# X_test = X_test.astype('float16')\n",
    "# y_test = y_test.astype('float16')\n",
    "print()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train sktime model </h2>\n",
    "Now that we have prepared our dataset, we train a model with sktime. After we train the model we calculate its score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 30\n",
    "epochs = 10\n",
    "model_name = 'ResNetClassifier'\n",
    "# import keras\n",
    "# from keras.metrics import \n",
    "# \n",
    "resnet_model = ResNetClassifier(verbose = 1, n_epochs=epochs, batch_size=batch_size, loss = 'categorical_crossentropy', random_state=42, optimizer='Adam') #Cohen Kappa Score of ResNetClassifier: 0.8340\n",
    "# resnet_model = ResNetClassifier(verbose = 1, n_epochs=epochs, batch_size=batch_size, loss = 'categorical_crossentropy', random_state=42, optimizer='Adamax') #Cohen Kappa Score of ResNetClassifier: 0.7940\n",
    "\n",
    "resnet_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = resnet_model.predict(X_test)\n",
    "asses_model(model_name, y_pred, y_test,labels_order)\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = resnet_model.predict(X_test)\n",
    "asses_model(model_name, y_pred, y_test,labels_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 2\n",
    "model_name = 'InceptionTimeClassifier'\n",
    "\n",
    "inceptiontime_model = InceptionTimeClassifier(n_epochs=epochs, batch_size=batch_size,  random_state=42, verbose=True, loss='categorical_crossentropy')\n",
    "inceptiontime_model.fit(X_train, y_train) \n",
    "y_pred = inceptiontime_model.predict(X_test) \n",
    "asses_model(model_name, y_pred, y_test,labels_order)\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'MiniRocketClassifier'\n",
    "# rocket_model = RocketClassifier(num_kernels=1000, n_jobs=-1, random_state=42, rocket_transform = 'minirocket') \n",
    "# rocket_model.fit(X_train, y_train) \n",
    "# y_pred = rocket_model.predict(X_test) \n",
    "# asses_model(model_name, y_pred, y_test,labels_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2  # You might want to increase this gradually to find the sweet spot\n",
    "batch_size = 32  # A smaller batch size than the default to help with generalization\n",
    "kernel_size = 20  # Smaller than default, assuming it still captures relevant features in your data\n",
    "n_filters = 16  # Fewer filters to reduce model complexity\n",
    "depth = 4  # Reduced depth to simplify the model\n",
    "\n",
    "inceptiontime_model = InceptionTimeClassifier(\n",
    "    n_epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    kernel_size=kernel_size,\n",
    "    n_filters=n_filters,\n",
    "    use_residual=True,  # Keep using residual connections, but this can be toggled for experimentation\n",
    "    use_bottleneck=True,  # Keep the bottleneck, but consider adjusting the size if overfitting persists\n",
    "    bottleneck_size=32,  # You might want to reduce this if the model is still overfitting\n",
    "    depth=depth,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "    loss='categorical_crossentropy'\n",
    ")\n",
    "\n",
    "inceptiontime_model.fit(X_train, y_train) \n",
    "y_pred = inceptiontime_model.predict(X_test) \n",
    "asses_model(model_name, y_pred, y_test, labels_order)\n",
    "tf.compat.v1.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

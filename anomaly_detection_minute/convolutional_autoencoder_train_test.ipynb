{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = ['ranked','public','mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True],\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FALCO'],\n",
    "    'character_name': ['PIKACHU'],\n",
    "    # 'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'PEACH', 'LUIGI', 'DONKEY_KONG'],\n",
    "    'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIKACHU    4096\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/anomaly_detection_minute/../slp_package/input_dataset.py:95: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage_name</th>\n",
       "      <th>num_players</th>\n",
       "      <th>conclusive</th>\n",
       "      <th>player_character_name</th>\n",
       "      <th>player_type_name</th>\n",
       "      <th>opposing_player_type_name</th>\n",
       "      <th>player_inputs_np_sub_path</th>\n",
       "      <th>length</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\PIKACHU\\daa0e300-d731-42ea-a634-28c9db71...</td>\n",
       "      <td>7244</td>\n",
       "      <td>PIKACHU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POKEMON_STADIUM</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\PIKACHU\\142d032c-4d4a-4056-b9a9-aca37c88...</td>\n",
       "      <td>7384</td>\n",
       "      <td>PIKACHU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOSHIS_STORY</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\PIKACHU\\3a508695-3da7-464a-b228-88d0f051...</td>\n",
       "      <td>14390</td>\n",
       "      <td>PIKACHU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOUNTAIN_OF_DREAMS</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\PIKACHU\\1be3107a-58a6-4f48-80c0-191cf926...</td>\n",
       "      <td>11363</td>\n",
       "      <td>PIKACHU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BATTLEFIELD</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\PIKACHU\\0b446825-d7e3-405d-9b67-33a80ba1...</td>\n",
       "      <td>9797</td>\n",
       "      <td>PIKACHU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stage_name  num_players conclusive player_character_name  \\\n",
       "0   FINAL_DESTINATION            2       True               PIKACHU   \n",
       "1     POKEMON_STADIUM            2       True               PIKACHU   \n",
       "2        YOSHIS_STORY            2       True               PIKACHU   \n",
       "3  FOUNTAIN_OF_DREAMS            2       True               PIKACHU   \n",
       "4         BATTLEFIELD            2       True               PIKACHU   \n",
       "\n",
       "  player_type_name opposing_player_type_name  \\\n",
       "0            HUMAN                     HUMAN   \n",
       "1            HUMAN                     HUMAN   \n",
       "2            HUMAN                     HUMAN   \n",
       "3            HUMAN                     HUMAN   \n",
       "4            HUMAN                     HUMAN   \n",
       "\n",
       "                           player_inputs_np_sub_path  length   labels  \n",
       "0  mango\\PIKACHU\\daa0e300-d731-42ea-a634-28c9db71...    7244  PIKACHU  \n",
       "1  mango\\PIKACHU\\142d032c-4d4a-4056-b9a9-aca37c88...    7384  PIKACHU  \n",
       "2  mango\\PIKACHU\\3a508695-3da7-464a-b228-88d0f051...   14390  PIKACHU  \n",
       "3  mango\\PIKACHU\\1be3107a-58a6-4f48-80c0-191cf926...   11363  PIKACHU  \n",
       "4  mango\\PIKACHU\\0b446825-d7e3-405d-9b67-33a80ba1...    9797  PIKACHU  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "\n",
    "print(dataset.dataset['labels'].value_counts())\n",
    "dataset.dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label  Count  Shift\n",
      "0  PIKACHU   4067   2032\n"
     ]
    }
   ],
   "source": [
    "labels_order =  dataset.number_of_segments_per_game(3600,15000)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           player_inputs_np_sub_path  length  num_segments  \\\n",
      "0  public\\PIKACHU\\683b5d15-bc36-4c91-8503-aff1dee...    9693             3   \n",
      "1  ranked\\PIKACHU\\5d30f16a-a1cb-44c7-b815-e03724e...   11724             4   \n",
      "2  public\\PIKACHU\\c337441e-e286-4703-ae91-16c32d0...   11724             4   \n",
      "3  ranked\\PIKACHU\\d881f727-966a-44e1-a9c6-d4d37ee...    7650             2   \n",
      "4  ranked\\PIKACHU\\5b9fa78a-09a3-4c75-9901-3da6bed...   13746             5   \n",
      "\n",
      "    labels  encoded_labels  \n",
      "0  PIKACHU               0  \n",
      "1  PIKACHU               0  \n",
      "2  PIKACHU               0  \n",
      "3  PIKACHU               0  \n",
      "4  PIKACHU               0  \n"
     ]
    }
   ],
   "source": [
    "train_df, test_df  = dataset.train_test_split_dataframes(test_ratio = .20, val = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = dataset.all_segments_train_test_split_dataframes(3600, proportion_of_segments=1, test_ratio = .2, val = False)\n",
    "# porportion = .8\n",
    "# train_df = train_df.sample(frac=porportion, random_state = 42)\n",
    "porportion = .05\n",
    "test_df = test_df.sample(frac=porportion, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8408, 6)\n",
      "(105, 6)\n",
      "0.01233407729355104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_inputs_np_sub_path</th>\n",
       "      <th>labels</th>\n",
       "      <th>encoded_labels</th>\n",
       "      <th>segment_start_index</th>\n",
       "      <th>segment_index</th>\n",
       "      <th>segment_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ranked\\PIKACHU\\68fcf66a-c7ba-4a5d-9f96-bbe9cf8...</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ranked\\PIKACHU\\68fcf66a-c7ba-4a5d-9f96-bbe9cf8...</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>0</td>\n",
       "      <td>2978</td>\n",
       "      <td>1</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ranked\\PIKACHU\\68fcf66a-c7ba-4a5d-9f96-bbe9cf8...</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>0</td>\n",
       "      <td>5956</td>\n",
       "      <td>2</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ranked\\PIKACHU\\68fcf66a-c7ba-4a5d-9f96-bbe9cf8...</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>0</td>\n",
       "      <td>8934</td>\n",
       "      <td>3</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ranked\\PIKACHU\\f3d5e964-c520-45c5-b589-2eac95d...</td>\n",
       "      <td>PIKACHU</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           player_inputs_np_sub_path   labels  encoded_labels  \\\n",
       "0  ranked\\PIKACHU\\68fcf66a-c7ba-4a5d-9f96-bbe9cf8...  PIKACHU               0   \n",
       "1  ranked\\PIKACHU\\68fcf66a-c7ba-4a5d-9f96-bbe9cf8...  PIKACHU               0   \n",
       "2  ranked\\PIKACHU\\68fcf66a-c7ba-4a5d-9f96-bbe9cf8...  PIKACHU               0   \n",
       "3  ranked\\PIKACHU\\68fcf66a-c7ba-4a5d-9f96-bbe9cf8...  PIKACHU               0   \n",
       "4  ranked\\PIKACHU\\f3d5e964-c520-45c5-b589-2eac95d...  PIKACHU               0   \n",
       "\n",
       "   segment_start_index  segment_index  segment_length  \n",
       "0                    0              0            3600  \n",
       "1                 2978              1            3600  \n",
       "2                 5956              2            3600  \n",
       "3                 8934              3            3600  \n",
       "4                    0              0            3600  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(test_df.shape[0] / (train_df.shape[0] + test_df.shape[0]))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading game segments from compressed numpy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.file_paths = df['player_inputs_np_sub_path'].to_numpy()\n",
    "        self.encoded_labels = df['encoded_labels'].to_numpy()\n",
    "        self.segment_start_index = df['segment_start_index'].to_numpy()\n",
    "        # self.segment_index = df['segment_index'].to_numpy()\n",
    "        self.segment_length = df['segment_length'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads and returns a sample from the dataset at the specified index.\"\"\"\n",
    "        with gzip.open('/workspace/melee_project_data/input_np/' + self.file_paths[idx].replace('\\\\','/'), 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "        \n",
    "        # Start and end of the segment\n",
    "        segment_start = self.segment_start_index[idx]\n",
    "        segment_end = self.segment_start_index[idx] + self.segment_length[idx]\n",
    "        \n",
    "        segment = segment[:,int(segment_start):int(segment_end)]\n",
    "        \n",
    "        # separate into positive and negative values\n",
    "        # if self.transform:\n",
    "        #     transformed = np.zeros((13,60))\n",
    "        #     transformed[0,:] = (np.abs(segment[0]) - .2875) * 1.40350877193 * (segment[0] > 0)\n",
    "        #     transformed[1,:] = (np.abs(segment[0]) - .2875) * 1.40350877193 * (segment[0] < 0)\n",
    "        #     transformed[2,:] = (np.abs(segment[1]) - .2875) * 1.40350877193 * (segment[1] > 0)\n",
    "        #     transformed[3,:] = (np.abs(segment[1]) - .2875) * 1.40350877193 * (segment[1] < 0)\n",
    "        #     transformed[4,:] = (np.abs(segment[2]) - .2875) * 1.40350877193 * (segment[2] > 0)\n",
    "        #     transformed[5,:] = (np.abs(segment[2]) - .2875) * 1.40350877193 * (segment[2] < 0)\n",
    "        #     transformed[6,:] = (np.abs(segment[3]) - .2875) * 1.40350877193 * (segment[3] > 0)\n",
    "        #     transformed[7,:] = (np.abs(segment[3]) - .2875) * 1.40350877193 * (segment[3] < 0)\n",
    "        #     transformed[8:,:] = segment[4:]\n",
    "        #     segment = transformed\n",
    "        \n",
    "        # if self.transform:\n",
    "        #     # Shift inputs: adjusting so that positives are reduced and negatives are increased\n",
    "        #     segment[0:4, :] -= .2875 * (segment[0:4, :] > 0)\n",
    "        #     segment[0:4, :] += .2875 * (segment[0:4, :] < 0)\n",
    "\n",
    "        #     # Scale inputs to be between -.5 and .5\n",
    "        #     segment[0:4, :] *= 1.40350877193 / 2\n",
    "        #     segment[0:4, :] += .5\n",
    "        segment[-5] = (segment[-5] > .5)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = np.zeros((9 + 5+4,60))\n",
    "            \n",
    "            # First 4 rows of transformed are the analog inputs transformed to have range [0,1]\n",
    "            # Shift inputs: adjusting so that positives are reduced and negatives are increased\n",
    "            analog_transformed = np.copy(segment[0:4])\n",
    "            analog_transformed[analog_transformed > 0] -= 0.2875 + 0.0125\n",
    "            analog_transformed[analog_transformed < 0] += 0.2875 - 0.0125\n",
    "            # Scale inputs to be between -.5 and .5\n",
    "            analog_transformed *= .5 / .725\n",
    "            # Add .5 to so final inputs are between 0 and 1\n",
    "            analog_transformed += .5\n",
    "            \n",
    "            transformed[0:4] = analog_transformed\n",
    "            # Next four rows are 1 if the corresponding analog input is 0\n",
    "            transformed[4:8] += (segment[:4] == 0)\n",
    "            \n",
    "            \n",
    "            prepend = np.expand_dims(segment[-5:, 0], axis=1)\n",
    "            transitions= np.abs(np.diff(segment[-5:], axis=1, prepend=prepend))\n",
    "            \n",
    "            transformed[8:13] += transitions\n",
    "            \n",
    "            \n",
    "\n",
    "            # Transform the Trigger so that it is 0 or 1\n",
    "            \n",
    "            \n",
    "            # Remaining rows are button inputs\n",
    "            transformed[-5:] += segment[-5:]\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        segment_tensor = torch.from_numpy(transformed).float()\n",
    "        # label_tensor = torch.tensor(self.encoded_labels[idx], dtype=torch.long)\n",
    "        return segment_tensor#, label_tensor\n",
    "    \n",
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading game segments from compressed numpy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.file_paths = df['player_inputs_np_sub_path'].to_numpy()\n",
    "        self.encoded_labels = df['encoded_labels'].to_numpy()\n",
    "        self.segment_start_index = df['segment_start_index'].to_numpy()\n",
    "        # self.segment_index = df['segment_index'].to_numpy()\n",
    "        self.segment_length = df['segment_length'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads and returns a sample from the dataset at the specified index.\"\"\"\n",
    "        with gzip.open('/workspace/melee_project_data/input_np/' + self.file_paths[idx].replace('\\\\','/'), 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "        \n",
    "        # Start and end of the segment\n",
    "        segment_start = self.segment_start_index[idx]\n",
    "        segment_end = self.segment_start_index[idx] + self.segment_length[idx]\n",
    "        \n",
    "        segment = segment[:,int(segment_start):int(segment_end)]\n",
    "        \n",
    "        # separate into positive and negative values\n",
    "        # if self.transform:\n",
    "        #     transformed = np.zeros((13,60))\n",
    "        #     transformed[0,:] = (np.abs(segment[0]) - .2875) * 1.40350877193 * (segment[0] > 0)\n",
    "        #     transformed[1,:] = (np.abs(segment[0]) - .2875) * 1.40350877193 * (segment[0] < 0)\n",
    "        #     transformed[2,:] = (np.abs(segment[1]) - .2875) * 1.40350877193 * (segment[1] > 0)\n",
    "        #     transformed[3,:] = (np.abs(segment[1]) - .2875) * 1.40350877193 * (segment[1] < 0)\n",
    "        #     transformed[4,:] = (np.abs(segment[2]) - .2875) * 1.40350877193 * (segment[2] > 0)\n",
    "        #     transformed[5,:] = (np.abs(segment[2]) - .2875) * 1.40350877193 * (segment[2] < 0)\n",
    "        #     transformed[6,:] = (np.abs(segment[3]) - .2875) * 1.40350877193 * (segment[3] > 0)\n",
    "        #     transformed[7,:] = (np.abs(segment[3]) - .2875) * 1.40350877193 * (segment[3] < 0)\n",
    "        #     transformed[8:,:] = segment[4:]\n",
    "        #     segment = transformed\n",
    "        \n",
    "        # if self.transform:\n",
    "        #     # Shift inputs: adjusting so that positives are reduced and negatives are increased\n",
    "        #     segment[0:4, :] -= .2875 * (segment[0:4, :] > 0)\n",
    "        #     segment[0:4, :] += .2875 * (segment[0:4, :] < 0)\n",
    "\n",
    "        #     # Scale inputs to be between -.5 and .5\n",
    "        #     segment[0:4, :] *= 1.40350877193 / 2\n",
    "        #     segment[0:4, :] += .5\n",
    "        segment[-5] = (segment[-5] > .5)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = np.zeros((9 + 5+4,3600))\n",
    "            \n",
    "            # First 4 rows of transformed are the analog inputs transformed to have range [0,1]\n",
    "            # Shift inputs: adjusting so that positives are reduced and negatives are increased\n",
    "            analog_transformed = np.copy(segment[0:4])\n",
    "            analog_transformed[analog_transformed > 0] -= 0.2875 + 0.0125\n",
    "            analog_transformed[analog_transformed < 0] += 0.2875 - 0.0125\n",
    "            # Scale inputs to be between -.5 and .5\n",
    "            analog_transformed *= .5 / .725\n",
    "            # Add .5 to so final inputs are between 0 and 1\n",
    "            analog_transformed += .5\n",
    "            \n",
    "            transformed[0:4] = analog_transformed\n",
    "            # Next four rows are 1 if the corresponding analog input is 0\n",
    "            transformed[4:8] += (segment[:4] == 0)\n",
    "            \n",
    "            \n",
    "            prepend = np.expand_dims(segment[-5:, 0], axis=1)\n",
    "            transitions= np.abs(np.diff(segment[-5:], axis=1, prepend=prepend))\n",
    "            \n",
    "            transformed[8:13] += transitions\n",
    "            \n",
    "            \n",
    "\n",
    "            # Transform the Trigger so that it is 0 or 1\n",
    "            \n",
    "            \n",
    "            # Remaining rows are button inputs\n",
    "            transformed[-5:] += segment[-5:]\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        segment_tensor = torch.from_numpy(transformed).float()\n",
    "        # label_tensor = torch.tensor(self.encoded_labels[idx], dtype=torch.long)\n",
    "        return segment_tensor#, label_tensor\n",
    "    \n",
    "def prepare_data_loaders(train_df, test_df, batch_size, num_workers):\n",
    "    # Initialize datasets\n",
    "    train_dataset = TrainingDataset(train_df,True)\n",
    "    # val_dataset = TrainingDataset(file_paths_val, labels_val)\n",
    "    test_dataset = TrainingDataset(test_df,True)\n",
    "\n",
    "    # Initialize data loaders\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True,persistent_workers=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True,persistent_workers=True),\n",
    "        # 'val': DataLoader(val_dataset, batch_size=2**9, num_workers=num_workers, shuffle=False, pin_memory=True,persistent_workers=True)\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "\n",
    "\n",
    "# ''' Get a batch of data to see the size if we want that information. ''' \n",
    "# data_loader_iterator = iter(loaders['train'])\n",
    "# first_batch = next(data_loader_iterator)\n",
    "# print(first_batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_virtual_epochs(model, criterion, optimizer, loaders, device, num_epochs=1):\n",
    "    scaler = GradScaler()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=15, factor=0.1)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    vepoch_total = 0\n",
    "    vepoch_loss_sum = 0\n",
    "    best_vepoch_loss = float('inf')\n",
    "\n",
    "    early_stopping_patience = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        virtual_epoch_start_time = time.time()\n",
    "\n",
    "        # Initialize variables for tracking gradient and parameter stats\n",
    "        grad_max = float('-inf')\n",
    "        grad_min = float('inf')\n",
    "        param_max = float('-inf')\n",
    "        param_min = float('inf')\n",
    "\n",
    "        for batch_number, target_cpu in enumerate(train_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output_gpu = model(target_gpu)\n",
    "                loss = criterion(output_gpu, target_gpu) / (13 * 60 * target_cpu.size(0))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100.0)\n",
    "            # Track max and min of gradients\n",
    "            batch_grad_max = max((p.grad.max().item() for p in model.parameters() if p.grad is not None), default=grad_max)\n",
    "            batch_grad_min = min((p.grad.min().item() for p in model.parameters() if p.grad is not None), default=grad_min)\n",
    "            grad_max = max(grad_max, batch_grad_max)\n",
    "            grad_min = min(grad_min, batch_grad_min)\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            vepoch_total += target_cpu.size(0)\n",
    "            vepoch_loss_sum += loss.item() * target_cpu.size(0)\n",
    "\n",
    "            if time.time() - virtual_epoch_start_time > 60:\n",
    "                vepoch_loss = vepoch_loss_sum / vepoch_total\n",
    "                if best_vepoch_loss > vepoch_loss:\n",
    "                    best_vepoch_loss = vepoch_loss\n",
    "                else:\n",
    "                    early_stopping_patience += 1\n",
    "                # Calculate max and min of model parameters at the end of the virtual epoch\n",
    "                param_max = max(p.data.max().item() for p in model.parameters())\n",
    "                param_min = min(p.data.min().item() for p in model.parameters())\n",
    "\n",
    "                train_loader_tqdm.set_postfix(\n",
    "                    Best=f'{best_vepoch_loss * 100:.10f}',\n",
    "                    Vepoch=f'{vepoch_loss * 100:.10f}',\n",
    "                    patience=early_stopping_patience,\n",
    "                    Grad_Max=grad_max,\n",
    "                    Grad_Min=grad_min,\n",
    "                    Param_Max=param_max,\n",
    "                    Param_Min=param_min\n",
    "                )\n",
    "\n",
    "                virtual_epoch_start_time = time.time()\n",
    "                vepoch_total = 0\n",
    "                vepoch_loss_sum = 0\n",
    "                grad_max = float('-inf')  # Reset for next virtual epoch\n",
    "                grad_min = float('inf')   # Reset for next virtual epoch\n",
    "        \n",
    "    return best_model\n",
    "\n",
    "def train_model_with_virtual_epochs(model, criterion, optimizer, loaders, device, num_epochs=1):\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=15, factor=0.1)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    vepoch_total = 0\n",
    "    vepoch_loss_sum = 0\n",
    "    best_vepoch_loss = float('inf')\n",
    "    early_stopping_patience = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        virtual_epoch_start_time = time.time()\n",
    "\n",
    "        # Initialize variables for tracking gradient and parameter stats\n",
    "        grad_max = float('-inf')\n",
    "        grad_min = float('inf')\n",
    "        param_max = float('-inf')\n",
    "        param_min = float('inf')\n",
    "\n",
    "        for batch_number, target_cpu in enumerate(train_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output_gpu = model(target_gpu)\n",
    "            loss = criterion(output_gpu, target_gpu) / (18 * 3600 * target_cpu.size(0))\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # Track max and min of gradients\n",
    "            batch_grad_max = max((p.grad.max().item() for p in model.parameters() if p.grad is not None), default=grad_max)\n",
    "            batch_grad_min = min((p.grad.min().item() for p in model.parameters() if p.grad is not None), default=grad_min)\n",
    "            grad_max = max(grad_max, batch_grad_max)\n",
    "            grad_min = min(grad_min, batch_grad_min)\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            vepoch_total += target_cpu.size(0)\n",
    "            vepoch_loss_sum += loss.item() * target_cpu.size(0)\n",
    "\n",
    "            if time.time() - virtual_epoch_start_time > 60:\n",
    "                vepoch_loss = vepoch_loss_sum / vepoch_total\n",
    "                if best_vepoch_loss > vepoch_loss:\n",
    "                    best_vepoch_loss = vepoch_loss\n",
    "                else:\n",
    "                    early_stopping_patience += 1\n",
    "\n",
    "                # Calculate max and min of model parameters at the end of the virtual epoch\n",
    "                param_max = max(p.data.max().item() for p in model.parameters())\n",
    "                param_min = min(p.data.min().item() for p in model.parameters())\n",
    "\n",
    "                train_loader_tqdm.set_postfix(\n",
    "                    Best=f'{best_vepoch_loss * 100:.10f}',\n",
    "                    Vepoch=f'{vepoch_loss * 100:.10f}',\n",
    "                    patience=early_stopping_patience,\n",
    "                    Grad_Max=grad_max,\n",
    "                    Grad_Min=grad_min,\n",
    "                    Param_Max=param_max,\n",
    "                    Param_Min=param_min\n",
    "                )\n",
    "                # print('Grad Max:', grad_max, ' Grad Min:', grad_min)\n",
    "                virtual_epoch_start_time = time.time()\n",
    "                vepoch_total = 0\n",
    "                vepoch_loss_sum = 0\n",
    "                grad_max = float('-inf')  # Reset for next virtual epoch\n",
    "                grad_min = float('inf')   # Reset for next virtual epoch\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def train_model(model, criterion, optimizer, loaders, device, num_epochs=1):\n",
    "\n",
    "\n",
    "    \n",
    "    epoch_total = 0\n",
    "    epoch_loss_sum = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "\n",
    "\n",
    "        for _, target_cpu in enumerate(train_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output_gpu = model(target_gpu)\n",
    "            loss = criterion(output_gpu, target_gpu) / (18 * 3600 * target_cpu.size(0))\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # Track max and min of gradients\n",
    "            # batch_grad_max = max((p.grad.max().item() for p in model.parameters() if p.grad is not None), default=grad_max)\n",
    "            # batch_grad_min = min((p.grad.min().item() for p in model.parameters() if p.grad is not None), default=grad_min)\n",
    "            # grad_max = max(grad_max, batch_grad_max)\n",
    "            # grad_min = min(grad_min, batch_grad_min)\n",
    "\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_total += target_cpu.size(0)\n",
    "            epoch_loss_sum += loss.item() * target_cpu.size(0)\n",
    "\n",
    "            epoch_loss = epoch_loss_sum / epoch_total\n",
    "\n",
    "            # Calculate max and min of model parameters at the end of the virtual epoch\n",
    "            # param_max = max(p.data.max().item() for p in model.parameters())\n",
    "            # param_min = min(p.data.min().item() for p in model.parameters())\n",
    "\n",
    "            train_loader_tqdm.set_postfix(\n",
    "                # Best=f'{best_epoch_loss * 100:.10f}',\n",
    "                Loss=f'{epoch_loss * 100:.10f}',\n",
    "                # Grad_Max=grad_max,\n",
    "                # Grad_Min=grad_min,\n",
    "                # Param_Max=param_max,\n",
    "                # Param_Min=param_min\n",
    "            )\n",
    "            # print('Grad Max:', grad_max, ' Grad Min:', grad_min)\n",
    "            epoch_total = 0\n",
    "            epoch_loss_sum = 0\n",
    "            # grad_max = float('-inf')  # Reset for next virtual epoch\n",
    "            # grad_min = float('inf')   # Reset for next virtual epoch\n",
    "\n",
    "    return \n",
    "def train_model(model, criterion, optimizer, loaders, device, num_epochs=1):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss_sum = 0\n",
    "        epoch_total = 0\n",
    "\n",
    "        # Initialize progress bar for the current epoch\n",
    "        train_loader_tqdm = tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "\n",
    "        for _, target_cpu in enumerate(train_loader_tqdm):\n",
    "            # Move data to the appropriate device\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output_gpu = model(target_gpu)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_gpu, target_gpu) / (18 * 3600 * target_cpu.size(0))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to avoid exploding gradients\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Perform a single optimization step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss sum for accurate average calculation\n",
    "            epoch_loss_sum += loss.item() * target_cpu.size(0)\n",
    "            epoch_total += target_cpu.size(0)\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        epoch_loss = epoch_loss_sum / epoch_total\n",
    "\n",
    "        # Update progress bar with the final loss of the epoch\n",
    "        train_loader_tqdm.set_postfix(Loss=f'{epoch_loss * 100:.10f}')\n",
    "    return model\n",
    "\n",
    "def check_for_bad_targets(model, criterion, optimizer, targets_gpu):\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "    bad_targets = []\n",
    "    \n",
    "    try: \n",
    "        for i in range(targets_gpu.size(0)):  # Assuming the first dimension is the batch size\n",
    "            single_target_gpu = targets_gpu[i].unsqueeze(0)  # Maintain batch dimension\n",
    "            \n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward and loss\n",
    "            with autocast():\n",
    "                output_gpu = model(single_target_gpu)\n",
    "                loss = criterion(output_gpu, single_target_gpu) / (13 * 60)\n",
    "            \n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Check for NaNs in gradients\n",
    "            if any(torch.isnan(p.grad).any() for p in model.parameters() if p.grad is not None):\n",
    "                bad_targets.append(single_target_gpu.cpu().numpy())  # Move tensor to CPU and convert to numpy\n",
    "        \n",
    "        if bad_targets:\n",
    "            print(f'There were {len(bad_targets)} bad target(s).')\n",
    "            return bad_targets\n",
    "        else:\n",
    "            print('There were no bad targets.')\n",
    "        return None   \n",
    "     \n",
    "    except:\n",
    "        print('There was a problem evaluating the model on a single target.')\n",
    "        return targets_gpu\n",
    "            \n",
    "\n",
    "def evaluate_model(model, criterion, loaders, loader, device):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loader_tqdm = tqdm(loaders[loader], unit = 'batch')\n",
    "        \n",
    "        for batch_number, target_cpu in enumerate(eval_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            output_gpu = model(target_gpu)\n",
    "            \n",
    "            eval_loss += criterion(output_gpu, target_gpu) / ((32 * 16 * 4) * 13 * 60)\n",
    "            \n",
    "            \n",
    "            total += target_gpu.shape[0] / (32 * 16 * 4)\n",
    "            eval_loader_tqdm.set_postfix(loss=f'{eval_loss / (total):.10f}') \n",
    "            \n",
    "    print(f'Evaluated Loss: {eval_loss / total:.10f}')\n",
    "    \n",
    "def evaluate_model(model, criterion, loaders, loader, device):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loader_tqdm = tqdm(loaders[loader], unit = 'batch')\n",
    "        \n",
    "        for batch_number, target_cpu in enumerate(eval_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            output_gpu = model(target_gpu)\n",
    "            \n",
    "            eval_loss += criterion(output_gpu, target_gpu) \n",
    "            \n",
    "            \n",
    "            \n",
    "            eval_loader_tqdm.set_postfix(loss=f'{eval_loss / (batch_number + 1):.10f}') \n",
    "            \n",
    "    print(f'Evaluated Loss: {eval_loss / (batch_number + 1):.10f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 3600]           1,216\n",
      "       BatchNorm1d-2             [-1, 64, 3600]             128\n",
      "              ReLU-3             [-1, 64, 3600]               0\n",
      "            Conv1d-4             [-1, 64, 3600]          12,352\n",
      "       BatchNorm1d-5             [-1, 64, 3600]             128\n",
      "              ReLU-6             [-1, 64, 3600]               0\n",
      "            Conv1d-7            [-1, 256, 3600]          16,640\n",
      "       BatchNorm1d-8            [-1, 256, 3600]             512\n",
      "            Conv1d-9            [-1, 256, 3600]           4,864\n",
      "      BatchNorm1d-10            [-1, 256, 3600]             512\n",
      "             ReLU-11            [-1, 256, 3600]               0\n",
      "Encoder_Bottleneck-12            [-1, 256, 3600]               0\n",
      "           Conv1d-13             [-1, 64, 3600]          16,448\n",
      "      BatchNorm1d-14             [-1, 64, 3600]             128\n",
      "             ReLU-15             [-1, 64, 3600]               0\n",
      "           Conv1d-16             [-1, 64, 3600]          12,352\n",
      "      BatchNorm1d-17             [-1, 64, 3600]             128\n",
      "             ReLU-18             [-1, 64, 3600]               0\n",
      "           Conv1d-19            [-1, 256, 3600]          16,640\n",
      "      BatchNorm1d-20            [-1, 256, 3600]             512\n",
      "             ReLU-21            [-1, 256, 3600]               0\n",
      "Encoder_Bottleneck-22            [-1, 256, 3600]               0\n",
      "           Conv1d-23            [-1, 128, 3600]          32,896\n",
      "      BatchNorm1d-24            [-1, 128, 3600]             256\n",
      "             ReLU-25            [-1, 128, 3600]               0\n",
      "           Conv1d-26            [-1, 128, 1800]          49,280\n",
      "      BatchNorm1d-27            [-1, 128, 1800]             256\n",
      "             ReLU-28            [-1, 128, 1800]               0\n",
      "           Conv1d-29            [-1, 512, 1800]          66,048\n",
      "      BatchNorm1d-30            [-1, 512, 1800]           1,024\n",
      "           Conv1d-31            [-1, 512, 1800]         131,584\n",
      "      BatchNorm1d-32            [-1, 512, 1800]           1,024\n",
      "             ReLU-33            [-1, 512, 1800]               0\n",
      "Encoder_Bottleneck-34            [-1, 512, 1800]               0\n",
      "           Conv1d-35            [-1, 128, 1800]          65,664\n",
      "      BatchNorm1d-36            [-1, 128, 1800]             256\n",
      "             ReLU-37            [-1, 128, 1800]               0\n",
      "           Conv1d-38            [-1, 128, 1800]          49,280\n",
      "      BatchNorm1d-39            [-1, 128, 1800]             256\n",
      "             ReLU-40            [-1, 128, 1800]               0\n",
      "           Conv1d-41            [-1, 512, 1800]          66,048\n",
      "      BatchNorm1d-42            [-1, 512, 1800]           1,024\n",
      "             ReLU-43            [-1, 512, 1800]               0\n",
      "Encoder_Bottleneck-44            [-1, 512, 1800]               0\n",
      "           Conv1d-45            [-1, 256, 1800]         131,328\n",
      "      BatchNorm1d-46            [-1, 256, 1800]             512\n",
      "             ReLU-47            [-1, 256, 1800]               0\n",
      "           Conv1d-48             [-1, 256, 900]         196,864\n",
      "      BatchNorm1d-49             [-1, 256, 900]             512\n",
      "             ReLU-50             [-1, 256, 900]               0\n",
      "           Conv1d-51            [-1, 1024, 900]         263,168\n",
      "      BatchNorm1d-52            [-1, 1024, 900]           2,048\n",
      "           Conv1d-53            [-1, 1024, 900]         525,312\n",
      "      BatchNorm1d-54            [-1, 1024, 900]           2,048\n",
      "             ReLU-55            [-1, 1024, 900]               0\n",
      "Encoder_Bottleneck-56            [-1, 1024, 900]               0\n",
      "           Conv1d-57             [-1, 256, 900]         262,400\n",
      "      BatchNorm1d-58             [-1, 256, 900]             512\n",
      "             ReLU-59             [-1, 256, 900]               0\n",
      "           Conv1d-60             [-1, 256, 900]         196,864\n",
      "      BatchNorm1d-61             [-1, 256, 900]             512\n",
      "             ReLU-62             [-1, 256, 900]               0\n",
      "           Conv1d-63            [-1, 1024, 900]         263,168\n",
      "      BatchNorm1d-64            [-1, 1024, 900]           2,048\n",
      "             ReLU-65            [-1, 1024, 900]               0\n",
      "Encoder_Bottleneck-66            [-1, 1024, 900]               0\n",
      "           Conv1d-67             [-1, 512, 900]         524,800\n",
      "      BatchNorm1d-68             [-1, 512, 900]           1,024\n",
      "             ReLU-69             [-1, 512, 900]               0\n",
      "           Conv1d-70             [-1, 512, 450]         786,944\n",
      "      BatchNorm1d-71             [-1, 512, 450]           1,024\n",
      "             ReLU-72             [-1, 512, 450]               0\n",
      "           Conv1d-73            [-1, 2048, 450]       1,050,624\n",
      "      BatchNorm1d-74            [-1, 2048, 450]           4,096\n",
      "           Conv1d-75            [-1, 2048, 450]       2,099,200\n",
      "      BatchNorm1d-76            [-1, 2048, 450]           4,096\n",
      "             ReLU-77            [-1, 2048, 450]               0\n",
      "Encoder_Bottleneck-78            [-1, 2048, 450]               0\n",
      "           Conv1d-79             [-1, 512, 450]       1,049,088\n",
      "      BatchNorm1d-80             [-1, 512, 450]           1,024\n",
      "             ReLU-81             [-1, 512, 450]               0\n",
      "           Conv1d-82             [-1, 512, 450]         786,944\n",
      "      BatchNorm1d-83             [-1, 512, 450]           1,024\n",
      "             ReLU-84             [-1, 512, 450]               0\n",
      "           Conv1d-85            [-1, 2048, 450]       1,050,624\n",
      "      BatchNorm1d-86            [-1, 2048, 450]           4,096\n",
      "             ReLU-87            [-1, 2048, 450]               0\n",
      "Encoder_Bottleneck-88            [-1, 2048, 450]               0\n",
      "           Conv1d-89              [-1, 15, 225]          92,175\n",
      "      BatchNorm1d-90              [-1, 15, 225]              30\n",
      "             ReLU-91              [-1, 15, 225]               0\n",
      "           Linear-92                 [-1, 2048]       6,914,048\n",
      "             ReLU-93                 [-1, 2048]               0\n",
      "      BatchNorm1d-94                 [-1, 2048]           4,096\n",
      "          Encoder-95                 [-1, 2048]               0\n",
      "           Linear-96                 [-1, 3375]       6,915,375\n",
      "             ReLU-97                 [-1, 3375]               0\n",
      "      BatchNorm1d-98                 [-1, 3375]           6,750\n",
      "  ConvTranspose1d-99            [-1, 2048, 450]          94,208\n",
      "     BatchNorm1d-100            [-1, 2048, 450]           4,096\n",
      "            ReLU-101            [-1, 2048, 450]               0\n",
      " ConvTranspose1d-102             [-1, 512, 450]       1,049,088\n",
      "     BatchNorm1d-103             [-1, 512, 450]           1,024\n",
      "            ReLU-104             [-1, 512, 450]               0\n",
      " ConvTranspose1d-105             [-1, 512, 450]         786,944\n",
      "     BatchNorm1d-106             [-1, 512, 450]           1,024\n",
      "            ReLU-107             [-1, 512, 450]               0\n",
      " ConvTranspose1d-108            [-1, 2048, 450]       1,050,624\n",
      "     BatchNorm1d-109            [-1, 2048, 450]           4,096\n",
      "            ReLU-110            [-1, 2048, 450]               0\n",
      "Decoder_Bottleneck-111            [-1, 2048, 450]               0\n",
      " ConvTranspose1d-112             [-1, 512, 450]       1,049,088\n",
      "     BatchNorm1d-113             [-1, 512, 450]           1,024\n",
      "            ReLU-114             [-1, 512, 450]               0\n",
      " ConvTranspose1d-115             [-1, 512, 900]         786,944\n",
      "     BatchNorm1d-116             [-1, 512, 900]           1,024\n",
      "            ReLU-117             [-1, 512, 900]               0\n",
      " ConvTranspose1d-118            [-1, 1024, 900]         525,312\n",
      "     BatchNorm1d-119            [-1, 1024, 900]           2,048\n",
      "        Upsample-120            [-1, 2048, 900]               0\n",
      " ConvTranspose1d-121            [-1, 1024, 900]       2,098,176\n",
      "     BatchNorm1d-122            [-1, 1024, 900]           2,048\n",
      "            ReLU-123            [-1, 1024, 900]               0\n",
      "Decoder_Bottleneck-124            [-1, 1024, 900]               0\n",
      " ConvTranspose1d-125             [-1, 256, 900]         262,400\n",
      "     BatchNorm1d-126             [-1, 256, 900]             512\n",
      "            ReLU-127             [-1, 256, 900]               0\n",
      " ConvTranspose1d-128             [-1, 256, 900]         196,864\n",
      "     BatchNorm1d-129             [-1, 256, 900]             512\n",
      "            ReLU-130             [-1, 256, 900]               0\n",
      " ConvTranspose1d-131            [-1, 1024, 900]         263,168\n",
      "     BatchNorm1d-132            [-1, 1024, 900]           2,048\n",
      "            ReLU-133            [-1, 1024, 900]               0\n",
      "Decoder_Bottleneck-134            [-1, 1024, 900]               0\n",
      " ConvTranspose1d-135             [-1, 256, 900]         262,400\n",
      "     BatchNorm1d-136             [-1, 256, 900]             512\n",
      "            ReLU-137             [-1, 256, 900]               0\n",
      " ConvTranspose1d-138            [-1, 256, 1800]         196,864\n",
      "     BatchNorm1d-139            [-1, 256, 1800]             512\n",
      "            ReLU-140            [-1, 256, 1800]               0\n",
      " ConvTranspose1d-141            [-1, 512, 1800]         131,584\n",
      "     BatchNorm1d-142            [-1, 512, 1800]           1,024\n",
      "        Upsample-143           [-1, 1024, 1800]               0\n",
      " ConvTranspose1d-144            [-1, 512, 1800]         524,800\n",
      "     BatchNorm1d-145            [-1, 512, 1800]           1,024\n",
      "            ReLU-146            [-1, 512, 1800]               0\n",
      "Decoder_Bottleneck-147            [-1, 512, 1800]               0\n",
      " ConvTranspose1d-148            [-1, 128, 1800]          65,664\n",
      "     BatchNorm1d-149            [-1, 128, 1800]             256\n",
      "            ReLU-150            [-1, 128, 1800]               0\n",
      " ConvTranspose1d-151            [-1, 128, 1800]          49,280\n",
      "     BatchNorm1d-152            [-1, 128, 1800]             256\n",
      "            ReLU-153            [-1, 128, 1800]               0\n",
      " ConvTranspose1d-154            [-1, 512, 1800]          66,048\n",
      "     BatchNorm1d-155            [-1, 512, 1800]           1,024\n",
      "            ReLU-156            [-1, 512, 1800]               0\n",
      "Decoder_Bottleneck-157            [-1, 512, 1800]               0\n",
      " ConvTranspose1d-158            [-1, 128, 1800]          65,664\n",
      "     BatchNorm1d-159            [-1, 128, 1800]             256\n",
      "            ReLU-160            [-1, 128, 1800]               0\n",
      " ConvTranspose1d-161            [-1, 128, 3600]          49,280\n",
      "     BatchNorm1d-162            [-1, 128, 3600]             256\n",
      "            ReLU-163            [-1, 128, 3600]               0\n",
      " ConvTranspose1d-164            [-1, 256, 3600]          33,024\n",
      "     BatchNorm1d-165            [-1, 256, 3600]             512\n",
      "        Upsample-166            [-1, 512, 3600]               0\n",
      " ConvTranspose1d-167            [-1, 256, 3600]         131,328\n",
      "     BatchNorm1d-168            [-1, 256, 3600]             512\n",
      "            ReLU-169            [-1, 256, 3600]               0\n",
      "Decoder_Bottleneck-170            [-1, 256, 3600]               0\n",
      " ConvTranspose1d-171             [-1, 64, 3600]          16,448\n",
      "     BatchNorm1d-172             [-1, 64, 3600]             128\n",
      "            ReLU-173             [-1, 64, 3600]               0\n",
      " ConvTranspose1d-174             [-1, 64, 3600]          12,352\n",
      "     BatchNorm1d-175             [-1, 64, 3600]             128\n",
      "            ReLU-176             [-1, 64, 3600]               0\n",
      " ConvTranspose1d-177            [-1, 256, 3600]          16,640\n",
      "     BatchNorm1d-178            [-1, 256, 3600]             512\n",
      "            ReLU-179            [-1, 256, 3600]               0\n",
      "Decoder_Bottleneck-180            [-1, 256, 3600]               0\n",
      " ConvTranspose1d-181             [-1, 64, 3600]          16,448\n",
      "     BatchNorm1d-182             [-1, 64, 3600]             128\n",
      "            ReLU-183             [-1, 64, 3600]               0\n",
      " ConvTranspose1d-184             [-1, 64, 3600]          12,352\n",
      "     BatchNorm1d-185             [-1, 64, 3600]             128\n",
      "            ReLU-186             [-1, 64, 3600]               0\n",
      " ConvTranspose1d-187             [-1, 18, 3600]           1,170\n",
      "         Decoder-188             [-1, 18, 3600]               0\n",
      "================================================================\n",
      "Total params: 33,532,620\n",
      "Trainable params: 33,532,620\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 785.19\n",
      "Params size (MB): 127.92\n",
      "Estimated Total Size (MB): 913.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()\n",
    "\n",
    "from Convolutional_Autoencoder_Model import ResNet_Autoencoder\n",
    "\n",
    "\n",
    "channels = 9 + 5 + 4\n",
    "\n",
    "# Build model\n",
    "model = ResNet_Autoencoder(channels)\n",
    "# model.load_state_dict(torch.load('../../melee_project_data/convolutional_autoencoder_1_minute_bottlenecksize_15_2222_weights_1.pt'))\n",
    "model.to('cuda')\n",
    "# # With the size of an input we can get a model summary.\n",
    "summary(model, input_size=(channels, 3600))\n",
    "\n",
    "# Check that the output shape and target shape match\n",
    "# training_example = torch.rand(9, 2 ** 12).to('cuda')\n",
    "# print('Target shape:', training_example.shape)\n",
    "# model.eval()\n",
    "# output = model(training_example)\n",
    "# print('Output shape:', output.shape)\n",
    "\n",
    "## Optionally compile the model\n",
    "# import torch_tensorrt\n",
    "# model = torch.compile(model, mode = 'default')\n",
    "# model = torch.compile(model,mode = 'max-autotune')\n",
    "\n",
    "\n",
    "# model = torch.compile(model, backend=\"torch_tensorrt\")\n",
    "# model = torch.compile(model, backend=\"torch_tensorrt\",mode = 'max-autotune')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   5%|         | 53/1051 [00:02<00:48, 20.59batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 179\u001b[0m\n\u001b[1;32m    174\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# start_time = time.time()\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# train_model_with_virtual_epochs(model, criterion, optimizer, loaders, 'cuda', num_epochs)\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# print(f'Batch Size: {batch_size}, Training time: {time.time() - start_time:.2f}') \u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Again, this sometimes seems to help\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Evaluate the trained model\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# evaluate_model(model, criterion, loaders, 'test', 'cuda')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 238\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, loaders, device, num_epochs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    237\u001b[0m     epoch_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m target_cpu\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 238\u001b[0m     epoch_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m target_cpu\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    240\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss_sum \u001b[38;5;241m/\u001b[39m epoch_total\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch_loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "# Pepare data loaders\n",
    "batch_size =  8\n",
    "num_workers = 16\n",
    "loaders = prepare_data_loaders(train_df, test_df, batch_size, num_workers)\n",
    "\n",
    "    \n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "#         self.bin_threshold = 2\n",
    "\n",
    "#     def forward(self, pred, target):\n",
    "#         # Splitting predictions and targets into sticks and buttons\n",
    "#         pred_sticks = torch.tanh(pred[:4, :])  # First 4 rows for stick positions\n",
    "#         pred_buttons = torch.sigmoid(pred[4:, :])  # Remaining rows for button states\n",
    "#         target_sticks = target[:4, :]\n",
    "#         target_buttons = target[4:, :]\n",
    "        \n",
    "#         # Calculate angles for j-stick (rows 0, 1) and c-stick (rows 2, 3)\n",
    "#         target_angle_jstick = torch.atan2(target_sticks[1], target_sticks[0])\n",
    "#         target_angle_cstick = torch.atan2(target_sticks[3], target_sticks[2])\n",
    "#         epsilon = 1e-6\n",
    "#         predicted_angle_jstick = torch.atan2(pred_sticks[1]*target_buttons[1] + epsilon, pred_sticks[0]*target_buttons[0] + epsilon)\n",
    "#         predicted_angle_cstick = torch.atan2(pred_sticks[3]*target_buttons[3] + epsilon, pred_sticks[2]*target_buttons[2] + epsilon)\n",
    "\n",
    "#         # Calculate radii for j-stick and c-stick/\n",
    "#         target_radius_jstick = torch.sqrt(target_sticks[0]**2 + target_sticks[1]**2)\n",
    "#         target_radius_cstick = torch.sqrt(target_sticks[2]**2 + target_sticks[3]**2)\n",
    "#         pred_radius_jstick = torch.sqrt(pred_sticks[0]**2 + pred_sticks[1]**2)\n",
    "#         pred_radius_cstick = torch.sqrt(pred_sticks[2]**2 + pred_sticks[3]**2)\n",
    "        \n",
    "#         # Calculate smallest absolute angle difference\n",
    "#         abs_smallest_angle_difference_jstick = torch.abs((target_angle_jstick - predicted_angle_jstick + np.pi) % (2 * np.pi) - np.pi)\n",
    "#         abs_smallest_angle_difference_cstick = torch.abs((target_angle_cstick - predicted_angle_cstick + np.pi) % (2 * np.pi) - np.pi)\n",
    "        \n",
    "#         # Scale the angle difference\n",
    "#         abs_smallest_angle_difference_jstick *= (0.5 / ((1 + 2 * self.bin_threshold) * 0.055))\n",
    "#         abs_smallest_angle_difference_cstick *= (0.5 / ((1 + 2 * self.bin_threshold) * 0.055))\n",
    "        \n",
    "#         # Calculate absolute radius difference\n",
    "#         abs_radius_difference_jstick = torch.abs(target_radius_jstick - pred_radius_jstick) * (0.5 / (1 + 2 * self.bin_threshold) * (1 / 114))\n",
    "#         abs_radius_difference_cstick = torch.abs(target_radius_cstick - pred_radius_cstick) * (0.5 / (1 + 2 * self.bin_threshold) * (1 / 114))\n",
    "        \n",
    "#         # Calculate log loss for buttons\n",
    "#         log_loss_buttons = -(target_buttons * torch.log(pred_buttons.clamp(min=1e-6, max=1-1e-6)) + (1 - target_buttons) * torch.log(1 - pred_buttons.clamp(min=1e-6, max=1-1e-6)))\n",
    "\n",
    "#         loss_jstick = -torch.log(1 - abs_smallest_angle_difference_jstick.clamp(min=1e-6, max=1-1e-6)) - torch.log(1 - abs_radius_difference_jstick.clamp(min=1e-6, max=1-1e-6))\n",
    "#         loss_cstick = -torch.log(1 - abs_smallest_angle_difference_cstick.clamp(min=1e-6, max=1-1e-6)) - torch.log(1 - abs_radius_difference_cstick.clamp(min=1e-6, max=1-1e-6))\n",
    "\n",
    "#         return torch.sum(log_loss_buttons) + torch.sum(loss_jstick) +  torch.sum(loss_cstick)\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bin_threshold = 2\n",
    "        # Target sticks are roughly .00862 apart\n",
    "        # self.bin_thresholds bins to the left and right means\n",
    "        self.barely_close_enough = (.5 + self.bin_threshold) * .00862\n",
    "        self.epsilon = .0000001\n",
    "    \n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # Sigmoid has been applied, all values are in [0,1]\n",
    "        abs_stick_diff = torch.abs(target[0:4] - pred[0:4])\n",
    "        # Scale so that self.barely_close_enough becomes .5, i.e. multiply by 23.2018561485\n",
    "        abs_stick_diff *= .5 / self.barely_close_enough\n",
    "        log_abs_stick_diff = -torch.log(torch.clamp(1-abs_stick_diff, min=self.epsilon, max=1-self.epsilon))\n",
    "        \n",
    "        BCE_buttons = -  target[4:] * torch.log(torch.clamp(pred[4:], min=self.epsilon, max=1-self.epsilon)) - (1 - target[4:]) * torch.log(torch.clamp(1 - pred[4:], min=self.epsilon, max=1-self.epsilon))\n",
    "        return torch.sum(log_abs_stick_diff) + torch.sum(BCE_buttons)\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bin_threshold = 2\n",
    "        # Self.bin_threshold bins to the left and right should account for the barely close enough threshold\n",
    "        self.barely_close_enough = (.5 + self.bin_threshold) * 0.00862\n",
    "        self.epsilon = 1e-7\n",
    "        # Inverses of the weights for each feature, reshaped for proper broadcasting\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Sigmoid outputs for joystick values are between [0,1] and are first four in the data\n",
    "        abs_stick_diff = torch.abs(target[:,0:4,:] - pred[:,0:4,:])\n",
    "        # Rescale differences so that self.barely_close_enough corresponds to 0.5\n",
    "        scale_factor = 0.5 / self.barely_close_enough\n",
    "        abs_stick_diff *= scale_factor\n",
    "        log_abs_stick_diff = torch.log(1 + abs_stick_diff)\n",
    "        \n",
    "        abs_button_difference = torch.abs(target[:,4:,:] - target[:,4:,:])\n",
    "        # abs_button_difference[target[4:] == 1.] = (1 - pred[4:]) * self.weights\n",
    "        log_abs_button_difference = torch.log(1 + abs_button_difference)\n",
    "        \n",
    "        return torch.sum(log_abs_stick_diff) + torch.sum(log_abs_button_difference)\n",
    "    \n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bin_threshold = 10\n",
    "        # Self.bin_threshold bins to the left and right should account for the barely close enough threshold\n",
    "        self.barely_close_enough = (.5 + self.bin_threshold) * 0.00862\n",
    "        self.epsilon = 1e-7\n",
    "        # Inverses of the weights for each feature, reshaped for proper broadcasting\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Sigmoid outputs for joystick values are between [0,1] and are first four in the data\n",
    "        abs_stick_diff = torch.abs(target[:,0:4,0:60] - pred[:,0:4,0:60])\n",
    "        # Rescale differences so that self.barely_close_enough corresponds to 0.5\n",
    "        scale_factor = 0.5 / self.barely_close_enough\n",
    "        abs_stick_diff *= scale_factor\n",
    "        \n",
    "        log_abs_stick_diff = torch.exp(abs_stick_diff)\n",
    "        \n",
    "        # abs_button_difference = torch.abs(target[:,4:,0:60] - target[:,4:,0:60])\n",
    "        # abs_button_difference[target[4:] == 1.] = (1 - pred[4:]) * self.weights\n",
    "        BCE_buttons = -  target[:,4:,0:60] * torch.log(pred[:,4:,0:60]) - (1 - target[:,4:,0:60]) * torch.log(1 - pred[:,4:,0:60])\n",
    "        \n",
    "        return torch.sum(log_abs_stick_diff) + torch.sum(BCE_buttons)\n",
    "    \n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.BCE = nn.BCEWithLogitsLoss(reduction='sum')  # Consider using weighted BCE if needed\n",
    "        self.MSE = nn.MSELoss(reduction='sum')\n",
    "        self.bin_threshold = 2\n",
    "        # Self.bin_threshold bins to the left and right should account for the barely close enough threshold\n",
    "        self.barely_close_enough = (.5 + self.bin_threshold) * 0.00862#\n",
    "        self.scale_factor = - math.log(.5) / self.barely_close_enough ** 2\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # Calculating losses\n",
    "        mse_loss = self.MSE(torch.sigmoid(pred[:,0:4,:]), target[:,0:4,:]) \n",
    "        bce_loss = self.BCE(pred[:,4:,:], target[:,4:,:])\n",
    "        \n",
    "        # Total loss\n",
    "        return mse_loss  + bce_loss / 100\n",
    "    \n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "#         self.bin_threshold = 2\n",
    "#         # Define barely close enough threshold\n",
    "#         self.barely_close_enough = (.5 + self.bin_threshold) * 0.00862\n",
    "#         self.epsilon = 1e-7\n",
    "#         # Inverses of the weights for each feature\n",
    "#         self.weights = (torch.tensor([1, 1, 1, 1, 0.172214** -1, 0.008446** -1, 0.060667** -1, 0.045861** -1, 0.098047** -1]).reshape((1, -1, 1)) ).to('cuda')\n",
    "\n",
    "#     def forward(self, pred, target):\n",
    "#         # Assuming pred and target are in the shape [batch_size, num_features]\n",
    "#         # Process joystick differences\n",
    "#         abs_stick_diff = torch.abs(target[:, 0:4] - pred[:, 0:4])\n",
    "#         scale_factor = 0.5 / self.barely_close_enough\n",
    "#         abs_stick_diff *= scale_factor\n",
    "#         log_abs_stick_diff = torch.log(1 + abs_stick_diff)\n",
    "\n",
    "#         # Process button differences\n",
    "#         button_diff = torch.abs(target[:, 4:] - pred[:, 4:])\n",
    "#         log_abs_button_diff = torch.log(1 + button_diff)\n",
    "#         # weights = (target[:,4:] > .5) * self.weights\n",
    "#         return torch.sum(log_abs_stick_diff) + torch.sum(log_abs_button_diff)\n",
    "\n",
    "criterion = CustomLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 10\n",
    "\n",
    "# This seems to sometimes help\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Train the model\n",
    "# start_time = time.time()\n",
    "# train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)\n",
    "# train_model_with_virtual_epochs(model, criterion, optimizer, loaders, 'cuda', num_epochs)\n",
    "train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)\n",
    "# print(f'Batch Size: {batch_size}, Training time: {time.time() - start_time:.2f}') \n",
    "\n",
    "# Again, this sometimes seems to help\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()w\n",
    "\n",
    "# Evaluate the trained model\n",
    "# evaluate_model(model, criterion, loaders, 'test', 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../melee_project_data/pikachu_convolutional_autoencoder_1_minute_bottlenecksize_15_2222_weights_1.pt')\n",
    "# torch.save(model, '../../melee_project_data/convolutional_autoencoder_1_minute_bottlenecksize_10_3463_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "num_epochs = 10\n",
    "\n",
    "# # This seems to sometimes help\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# # Train the model\n",
    "# # start_time = time.time()\n",
    "# train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)\n",
    "train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../melee_project_data/pikachu_convolutional_autoencoder_1_minute_bottlenecksize_15_2222_weights_2.pt')\n",
    "# torch.save(model, '../../melee_project_data/convolutional_autoencoder_1_minute_bottlenecksize_15_2222_model_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.000001)\n",
    "num_epochs = 10\n",
    "\n",
    "# # This seems to sometimes help\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# # Train the model\n",
    "# # start_time = time.time()\n",
    "# train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)\n",
    "train_model(model, criterion, optimizer, loaders, 'cuda', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../melee_project_data/pikachu_convolutional_autoencoder_1_minute_bottlenecksize_15_2222_weights_3.pt')\n",
    "# torch.save(model, '../../melee_project_data/convolutional_autoencoder_1_minute_bottlenecksize_15_2222_model_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loaders, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loader_tqdm = tqdm(loaders[loader], unit='batch')\n",
    "        \n",
    "        for _, target_cpu in enumerate(eval_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            output_gpu = model(target_gpu)\n",
    "            # output_gpu = torch.sigmoid(output_gpu)\n",
    "            \n",
    "            predictions.append(torch.sigmoid(output_gpu).cpu().numpy())\n",
    "            targets.append(target_cpu.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pred, target = predict(model, loaders, 'test','cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_button_analysis(pred, target):\n",
    "    buttons = ['TRIGGER_LOGICAL', 'Z', 'A', 'B', 'X_or_Y']\n",
    "    \n",
    "    # Create dictionary to hold the data\n",
    "    data = {}\n",
    "    \n",
    "    # Process for first frame and last frame for both target and pred\n",
    "    for index, button in enumerate(buttons):\n",
    "        data[f'{button}_first_frame_target'] = (target[:, -5+index, 0] > .5).astype(int)\n",
    "        data[f'{button}_first_frame_pred'] = (pred[:, -5+index, 0] > .5).astype(int)\n",
    "        data[f'{button}_last_frame_target'] = (target[:, -5+index, -1] > .5).astype(int)\n",
    "        data[f'{button}_last_frame_pred'] = (pred[:, -5+index, -1] > .5).astype(int)\n",
    "        \n",
    "        # Ensure that the dimensions match for prepend operation\n",
    "        prepend_target = np.expand_dims(target[:, -5+index, 0], axis=1)\n",
    "        prepend_pred = np.expand_dims(pred[:, -5+index, 0], axis=1)\n",
    "\n",
    "        transitions_target = np.diff(target[:, -5+index, :], axis=1, prepend=prepend_target)\n",
    "        transitions_pred = np.diff(pred[:, -5+index, :], axis=1, prepend=prepend_pred)\n",
    "        \n",
    "        count_0_to_1_target = np.sum(transitions_target > .5, axis=1)\n",
    "        count_0_to_1_target += target[:,-5+index,0] > .5\n",
    "        count_0_to_1_pred = np.sum(transitions_pred > .5, axis=1)\n",
    "        count_0_to_1_pred += pred[:,-5+index,0] > .5\n",
    "        \n",
    "        data[f'{button}_num_presses_target'] = count_0_to_1_target\n",
    "        data[f'{button}_num_presses_pred'] = count_0_to_1_pred\n",
    "        \n",
    "    # Create DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'pred' and 'target' are defined and appropriate for this function\n",
    "df = predicted_button_analysis(pred, target)\n",
    "print(df.describe())\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buttons = ['TRIGGER_LOGICAL', 'Z', 'A', 'B', 'X_or_Y']\n",
    "# buttons = ['X_or_Y']\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(np.arange(7,dtype=np.int16),columns=['Target Pressed'])\n",
    "# print(summary_df)\n",
    "\n",
    "\n",
    "for index, button in enumerate(buttons):\n",
    "    print('-----', button, '-----')\n",
    "    example_data = []\n",
    "    accuracy_data = []\n",
    "    under_predicted_data = []\n",
    "    for i in range(7):\n",
    "        print()\n",
    "        print(f'Pressed {i} times')\n",
    "        df_button_is_pressed = df[df[f'{button}_num_presses_target'] == i]\n",
    "        columns = [\n",
    "                    f'{button}_first_frame_target',\n",
    "                    f'{button}_first_frame_pred',\n",
    "                    f'{button}_last_frame_target',\n",
    "                    f'{button}_last_frame_pred',\n",
    "                    f'{button}_num_presses_target',\n",
    "                    f'{button}_num_presses_pred']    \n",
    "        df_button_is_pressed = df_button_is_pressed[columns]\n",
    "        \n",
    "        # print(f'Target was pressed {i} time(s): < 0 means under predicting')\n",
    "        df_button_is_pressed['Off by'] = df_button_is_pressed[f'{button}_num_presses_pred'] - df_button_is_pressed[f'{button}_num_presses_target']\n",
    "        counts = df_button_is_pressed['Off by'].value_counts().sort_index().to_frame(name='Count')\n",
    "\n",
    "        # Calculating accuracy\n",
    "        total_presses = counts['Count'].sum()\n",
    "        correct_predictions = counts.loc[0, 'Count'] if 0 in counts.index else 0\n",
    "        accuracy = correct_predictions / total_presses if total_presses > 0 else 0\n",
    "        print(counts)\n",
    "        print('Accuracy:', \"{:.4f}\".format(accuracy))\n",
    "        accuracy_data += [accuracy]\n",
    "        \n",
    "                # Calculating under-prediction rate among non-zero predictions\n",
    "        under_predicted = counts[counts.index < 0]['Count'].sum() if any(counts.index < 0) else 0\n",
    "        non_zero_predictions = total_presses - (counts.loc[0, 'Count'] if 0 in counts.index else 0)\n",
    "        under_prediction_rate = under_predicted / non_zero_predictions if non_zero_predictions > 0 else 0\n",
    "        print(f'Under-prediction rate: {under_prediction_rate:.4f}')\n",
    "        under_predicted_data += [under_prediction_rate]\n",
    "        \n",
    "        example_data += [df_button_is_pressed.shape[0] / df.shape[0]]\n",
    "        \n",
    "    print()\n",
    "        \n",
    "    summary_df[f'{button} Examples'] = example_data\n",
    "    summary_df[f'{button} Correct'] = accuracy_data\n",
    "    summary_df[f'{button} Under'] = under_predicted_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Table shows:')\n",
    "print('- The percentage of test examples that were actually pressed n times.')\n",
    "print('- Accuracy of the prediction given the button was pressed n times.')\n",
    "print('- Under prediction rate of an incorrect prediction given the button was pressed n times.')           \n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df[['Target Pressed', 'TRIGGER_LOGICAL Correct', 'Z Correct', 'A Correct', 'B Correct', 'X_or_Y Correct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the target button was pressed once and the prediction was pressed once. How big was the difference in length of the button press. TO DO: further restrict to the prediction being pressed for the same number of frames and seeing if it was pressed at the exact same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buttons = ['TRIGGER_LOGICAL', 'Z', 'A', 'B', 'X_or_Y']\n",
    "results = []\n",
    "\n",
    "for index, button in enumerate(buttons):\n",
    "    button_is_pressed_once = (df[f'{button}_num_presses_target'] == 1) & (df[f'{button}_num_presses_pred'] == 1)\n",
    "    \n",
    "    \n",
    "    # Filtering rows where the button was pressed exactly once in both target and prediction\n",
    "    target_button_is_pressed_once = target[button_is_pressed_once, -5+index, :]\n",
    "    pred_button_is_pressed_once = pred[button_is_pressed_once, -5+index, :]\n",
    "    \n",
    "    # Calculating indices for the first and last button press in the target and prediction for each example\n",
    "    index_of_first_1_target = np.argmax(target_button_is_pressed_once > 0.5, axis=1)\n",
    "    index_of_first_1_pred = np.argmax(pred_button_is_pressed_once > 0.5, axis=1)\n",
    "    index_of_last_1_target = (target_button_is_pressed_once.shape[1] - np.argmax(target_button_is_pressed_once[:, ::-1] > 0.5, axis=1))\n",
    "    index_of_last_1_pred = (pred_button_is_pressed_once.shape[1] - np.argmax(pred_button_is_pressed_once[:, ::-1] > 0.5, axis=1))\n",
    "\n",
    "    length_of_target_press = index_of_last_1_target - index_of_first_1_target\n",
    "    length_of_pred_press = index_of_last_1_pred - index_of_first_1_pred\n",
    "    \n",
    "    unique, count = np.unique(length_of_pred_press - length_of_target_press, return_counts=True)\n",
    "    print(button)\n",
    "    print(np.array([unique,count]).T)\n",
    "\n",
    "    plt.bar(unique,np.log(count+1))\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the differece in the angle of the joystick and c-stick.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the target back to the the standard stick input. \n",
    "# 9 channels\n",
    "target_no_transform = target[:, :4, :] - .5\n",
    "target_no_transform[0:4, :] /= .5 / .725\n",
    "target_no_transform[target_no_transform[:, 0:4, :] < 0] -= .2875 + 0.0125\n",
    "target_no_transform[target_no_transform[:, 0:4, :] < 0] += .2875 - 0.0125\n",
    "# target_no_transform *= target[:,4:8,:]\n",
    " \n",
    "predicted_no_transform = pred[:, :4, :] - .5\n",
    "predicted_no_transform[0:4, :] /= .5 / .725\n",
    "predicted_no_transform[predicted_no_transform[:, 0:4, :] < 0] -= .2875 + 0.0125\n",
    "predicted_no_transform[predicted_no_transform[:, 0:4, :] < 0] += .2875 - 0.0125\n",
    "\n",
    "target_angle_JSTICK = np.arctan2(target_no_transform[:,0,:], target_no_transform[:,1,:]) * 180 / np.pi\n",
    "predicted_angle_JSTICK = np.arctan2(predicted_no_transform[:,0,:], predicted_no_transform[:,1,:]) * 180 / np.pi\n",
    "\n",
    "target_radius_JSTICK = np.sqrt(target_no_transform[:,0,:] ** 2 + target_no_transform[:,1,:] ** 2)\n",
    "pred_radius_JSTICK = np.sqrt(predicted_no_transform[:,0,:] ** 2 + predicted_no_transform[:,1,:] ** 2)\n",
    "\n",
    "print(target_no_transform[:,0,:])\n",
    "# # print(target_no_transform[:,1,:])\n",
    "print()\n",
    "\n",
    "print(predicted_no_transform[:,0,:])\n",
    "# # print(predicted_no_transform[:,1,:])\n",
    "# print()\n",
    "\n",
    "# print(target_angle_JSTICK)\n",
    "# print(predicted_angle_JSTICK)\n",
    "# print()\n",
    "\n",
    "# print(target_radius_JSTICK)\n",
    "# print(pred_radius_JSTICK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasform the stick predictions back to the original inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sticks_no_transform = target[:, :4] - .5\n",
    "target_sticks_no_transform /= .5 / .725\n",
    "target_sticks_no_transform[target_sticks_no_transform < 0] -= .2875 + 0.0125\n",
    "target_sticks_no_transform[target_sticks_no_transform < 0] += .2875 - 0.0125\n",
    " \n",
    "predicted_sticks_no_transform = pred[:, :4] - .5\n",
    "predicted_sticks_no_transform /= .5 / .725\n",
    "predicted_sticks_no_transform[predicted_sticks_no_transform < 0] -= .2875 + 0.0125\n",
    "predicted_sticks_no_transform[predicted_sticks_no_transform < 0] += .2875 - 0.0125\n",
    "\n",
    "# Multiply by 0 where the model predicted the stick input to be 0\n",
    "# predicted_sticks_no_transform *= (pred[:,4:8] < .5)\n",
    "predicted_sticks_no_transform *= (target[:,4:8] < .5)\n",
    "# predicted_sticks_no_transform = np.where(predicted_sticks_no_transform == 0.0, 0.0, predicted_sticks_no_transform)\n",
    "\n",
    "target_angle_JSTICK = np.arctan2(target_sticks_no_transform[:,[1,3]], target_sticks_no_transform[:,[0,2]]) * 180 / np.pi\n",
    "predicted_angle_JSTICK = np.arctan2(predicted_sticks_no_transform[:,[1,3]], predicted_sticks_no_transform[:,[0,2]]) * 180 / np.pi\n",
    "\n",
    "angle_difference_no_transform = (predicted_angle_JSTICK - target_angle_JSTICK) \n",
    "smallest_angle_difference_no_transform = np.abs((angle_difference_no_transform + 180) % 360 - 180)\n",
    "\n",
    "target_radius_squared_no_transform = target_sticks_no_transform[:,[0,2]] ** 2 + target_sticks_no_transform[:,[1,3]] ** 2\n",
    "pred_radius_squared_no_transform = predicted_sticks_no_transform[:,[0,2]] ** 2 + predicted_sticks_no_transform[:,[1,3]] ** 2\n",
    "\n",
    "radius_difference_no_trasform = np.abs(target_radius_squared_no_transform - pred_radius_squared_no_transform) ** (1/2)\n",
    "\n",
    "print('JSTICK average angle difference', np.average(smallest_angle_difference_no_transform[:,0]))\n",
    "print('CSTICK average angle difference', np.average(smallest_angle_difference_no_transform[:,1]))\n",
    "\n",
    "print('JSTICK average radius difference', np.average(radius_difference_no_trasform[:,0]))\n",
    "print('CSTICK average radius difference', np.average(radius_difference_no_trasform[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sticks_no_transform = np.round(target[:,0:4] / 0.008620689655172415 ).astype(np.int32) * 0.008620689655172415 - .5\n",
    "predicted_sticks_no_transform = np.round(pred[:,0:4] / 0.008620689655172415).astype(np.int32) * 0.008620689655172415 - .5\n",
    "\n",
    "# target_sticks_no_transform = target[:, :4] - .5\n",
    "target_sticks_no_transform /= .5 / .725\n",
    "target_sticks_no_transform[target_sticks_no_transform < 0] -= .2875 + 0.0125\n",
    "target_sticks_no_transform[target_sticks_no_transform < 0] += .2875 - 0.0125\n",
    " \n",
    "# predicted_sticks_no_transform = pred[:, :4] - .5\n",
    "predicted_sticks_no_transform /= .5 / .725\n",
    "predicted_sticks_no_transform[predicted_sticks_no_transform < 0] -= .2875 + 0.0125\n",
    "predicted_sticks_no_transform[predicted_sticks_no_transform < 0] += .2875 - 0.0125\n",
    "\n",
    "# Multiply by 0 where the model predicted the stick input to be 0\n",
    "# predicted_sticks_no_transform *= (pred[:,4:8] < .5)\n",
    "# predicted_sticks_no_transform *= (target[:,4:8,:] < .5)\n",
    "# predicted_sticks_no_transform = np.where(predicted_sticks_no_transform == 0.0, 0.0, predicted_sticks_no_transform)\n",
    "\n",
    "target_angle_JSTICK = np.arctan2(target_sticks_no_transform[:,[1,3]], target_sticks_no_transform[:,[0,2]]) * 180 / np.pi\n",
    "predicted_angle_JSTICK = np.arctan2(predicted_sticks_no_transform[:,[1,3]], predicted_sticks_no_transform[:,[0,2]]) * 180 / np.pi\n",
    "\n",
    "angle_difference_no_transform = (predicted_angle_JSTICK - target_angle_JSTICK) \n",
    "smallest_angle_difference_no_transform = np.abs((angle_difference_no_transform + 180) % 360 - 180)\n",
    "\n",
    "target_radius_squared_no_transform = target_sticks_no_transform[:,[0,2]] ** 2 + target_sticks_no_transform[:,[1,3]] ** 2\n",
    "pred_radius_squared_no_transform = predicted_sticks_no_transform[:,[0,2]] ** 2 + predicted_sticks_no_transform[:,[1,3]] ** 2\n",
    "\n",
    "radius_difference_no_trasform = np.abs(target_radius_squared_no_transform - pred_radius_squared_no_transform) ** (1/2)\n",
    "\n",
    "print('JSTICK average angle difference', np.average(smallest_angle_difference_no_transform[:,0]))\n",
    "print('CSTICK average angle difference', np.average(smallest_angle_difference_no_transform[:,1]))\n",
    "\n",
    "print('JSTICK average radius difference', np.average(radius_difference_no_trasform[:,0]))\n",
    "print('CSTICK average radius difference', np.average(radius_difference_no_trasform[:,1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_binary_0 = (target[:,:4] == 0)\n",
    "pred_stick_0 = predicted_sticks_no_transform == 0.\n",
    "\n",
    "\n",
    "# print('Binary and continuous both predicted 0 JSTICK_X:', np.sum((target[:,4,:] > .5)) / (pred_binary_0.shape[0] * 60) * 100)\n",
    "# print('Binary and continuous both predicted 0 JSTICK_Y:', np.sum((target[:,5,:] > .5)) / (pred_binary_0.shape[0] * 60) * 100)\n",
    "# print('Binary and continuous both predicted 0 CSTICK_X:', np.sum((target[:,6,:] > .5)) / (pred_binary_0.shape[0] * 60) * 100)\n",
    "# print('Binary and continuous both predicted 0 CSTICK_Y:', np.sum((target[:,7,:] > .5)) / (pred_binary_0.shape[0] * 60) * 100)\n",
    "\n",
    "# print()\n",
    "\n",
    "print('Binary and continuous both predicted 0 JSTICK_X:', np.sum(pred_binary_0[:,0] == pred_stick_0[:,0]) / (pred_binary_0.shape[0] * 60) * 100)\n",
    "print('Binary and continuous both predicted 0 JSTICK_Y:', np.sum(pred_binary_0[:,1] == pred_stick_0[:,1]) / (pred_binary_0.shape[0] * 60) * 100)\n",
    "print('Binary and continuous both predicted 0 CSTICK_X:', np.sum(pred_binary_0[:,2] == pred_stick_0[:,2]) / (pred_binary_0.shape[0] * 60) * 100)\n",
    "print('Binary and continuous both predicted 0 CSTICK_Y:', np.sum(pred_binary_0[:,3] == pred_stick_0[:,3]) / (pred_binary_0.shape[0] * 60) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "buttons = ['TRIGGER_LOGICAL', 'Z', 'A', 'B', 'X_or_Y']\n",
    "\n",
    "first_and_last_df = pd.DataFrame(buttons, columns=['Button'])\n",
    "for index, button in enumerate(buttons):\n",
    "    first_frame_0 = (df[f'{button}_first_frame_target'] == 0)\n",
    "    last_frame_0 = (df[f'{button}_last_frame_target'] == 0)\n",
    "    \n",
    "    first_and_last_df.loc[index, 'Ratio target[0]==0'] = first_frame_0.mean()\n",
    "    first_and_last_df.loc[index, 'Acc target[0]==0'] = 1 - df.loc[first_frame_0, f'{button}_first_frame_pred'].mean()\n",
    "    first_and_last_df.loc[index, 'Acc target[0]==1'] = df.loc[~first_frame_0, f'{button}_first_frame_pred'].mean()\n",
    "    \n",
    "    first_and_last_df.loc[index, 'Ratio target[-1]==0'] = last_frame_0.mean()\n",
    "    first_and_last_df.loc[index, 'Acc target[-1]==0'] = 1-df.loc[last_frame_0, f'{button}_last_frame_pred'].mean()\n",
    "    first_and_last_df.loc[index, 'Acc target[-1]==1'] = df.loc[~last_frame_0, f'{button}_last_frame_pred'].mean()\n",
    "    \n",
    "print('Table shows:')\n",
    "print('- Percentage of test examples where the button was not pressed on the first or last frame')\n",
    "print('- Given the button was pressed or not on the first or last frame, what was the accuracy of the prediction.')\n",
    "\n",
    "first_and_last_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how accurately the model predicted model predicted each analog input being zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_zero_stick = (pred[:,4:8] == 1) and (target[:,4:8] == 1)\n",
    "# print('Accuracy of JSTICK_X = 0:', np.sum(correct_zero_stick[:,0]) / np.sum(target[:,4]) * 100)\n",
    "# print('Accuracy of JSTICK_Y = 0:', np.sum(correct_zero_stick[:,1]) / np.sum(target[:,5]) * 100)\n",
    "# print('Accuracy of CSTICK_X = 0:', np.sum(correct_zero_stick[:,2]) / np.sum(target[:,6]) * 100)\n",
    "# print('Accuracy of JSTICK_Y = 0:', np.sum(correct_zero_stick[:,3]) / np.sum(target[:,7]) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(pred[:,0:4]))\n",
    "print(np.max(target[:,0:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make try to figure out how close the predicted stick vales were to the actual values. The possible stick values are discreet and we round the prediction to the nearest possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_stick_targets = np.round(target[:,0:4] / 0.008620689655172415 ).astype(np.int32)\n",
    "integer_stick_pred = np.round(pred[:,0:4] / 0.008620689655172415).astype(np.int32)\n",
    "\n",
    "print(np.unique(integer_stick_targets))\n",
    "print(np.unique(integer_stick_pred))\n",
    "\n",
    "\n",
    "unique, counts = np.unique(integer_stick_pred[:,0] - integer_stick_targets[:,0], return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "print('Good enough accuracy of JSICK_X', np.sum(counts[58-2:58+2]) / np.sum(counts) * 100)\n",
    "print('Accuracy of JSTICK_X', counts[58] / np.sum(counts)* 100)\n",
    "plt.title('Difference Between JSTICK_X Prediction and Target')\n",
    "plt.show()\n",
    "\n",
    "unique, counts = np.unique(integer_stick_pred[:,1] - integer_stick_targets[:,1], return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "print('Good enough accuracy of JSTICK_Y', np.sum(counts[58-2:58+2]) / np.sum(counts)* 100)\n",
    "print('Accuracy of JSTICK_Y', counts[58] / np.sum(counts)* 100)\n",
    "plt.title('Difference Between JSTICK_Y Prediction and Target')\n",
    "plt.show()\n",
    "\n",
    "unique, counts = np.unique(integer_stick_pred[:,2] - integer_stick_targets[:,2], return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "print('Good enough accuracy of CSTICK_X', np.sum(counts[58-2:58+2]) / np.sum(counts)* 100)\n",
    "print('Accuracy of CSTICK_X', counts[58] / np.sum(counts)* 100)\n",
    "plt.title('Difference Between CSTICK_X Prediction and Target')\n",
    "plt.show()\n",
    "\n",
    "unique, counts = np.unique(integer_stick_pred[:,3] - integer_stick_targets[:,3], return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "print('Good enough accuracy of CSTICK_Y', np.sum(counts[58-2:58+2]) / np.sum(counts)* 100)\n",
    "print('Accuracy of CSTICK_Y', counts[58] / np.sum(counts)* 100)\n",
    "plt.title('Difference Between CSTICK_Y Prediction and Target')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of predicting 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall accuracy of Sticks and buttons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_stick_targets = np.round(target[:,0:4] / 0.008620689655172415 ).astype(np.int32)\n",
    "integer_stick_pred = np.round(pred[:,0:4] / 0.008620689655172415).astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "n = 10\n",
    "\n",
    "buttons = ['JSTICK_X', 'JSTICK_Y', 'CSTICK_X', 'CSTICK_Y']\n",
    "# buttons = ['X_or_Y']\n",
    "\n",
    "\n",
    "stick_accuracy_df = pd.DataFrame(np.arange(n,dtype=np.int16),columns=['How Close'])\n",
    "# print(summary_df)\n",
    "\n",
    "\n",
    "for j in range(4):\n",
    "    unique, counts = np.unique(integer_stick_pred[:,0] - integer_stick_targets[:,0], return_counts=True)\n",
    "    data = []\n",
    "    num = np.sum(counts)\n",
    "    for i in range(n):\n",
    "        mask = np.abs(unique) <= i\n",
    "        data += [np.sum(counts[mask]) / num * 100]\n",
    "    stick_accuracy_df[buttons[j]] = data\n",
    "        \n",
    "print(stick_accuracy_df)     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example data setup (make sure you have your actual data in these variables)\n",
    "# target = np.random.randint(0, 2, (100, 9))  # Example target array\n",
    "# pred = np.random.random((100, 9))  # Example predictions array\n",
    "buttons = [ 'TRIGGER_LOGICAL', 'Z', 'A', 'B', 'X_or_Y']\n",
    "\n",
    "# Initializing the DataFrame\n",
    "button_accuracy_df = pd.DataFrame(columns=['Button', 'Accuracy', 'Acc of 0', 'Acc of 1'])\n",
    "\n",
    "target_buttons = target[:, 4:]\n",
    "pred_buttons = pred[:, 4:] > 0.5\n",
    "total = target_buttons.shape[0] * 3600\n",
    "\n",
    "# Computing accuracies and filling the DataFrame\n",
    "rows = []  # List to hold row data\n",
    "\n",
    "for i, button in enumerate(buttons):\n",
    "    correct_predictions = np.sum(target_buttons[:, i] == pred_buttons[:, i])\n",
    "    correct_zeros = np.sum((target_buttons[:, i] == 0) & (pred_buttons[:, i] == 0))\n",
    "    correct_ones = np.sum((target_buttons[:, i] == 1) & (pred_buttons[:, i] == 1))\n",
    "\n",
    "    accuracy = correct_predictions / total * 100\n",
    "    acc_of_0 = correct_zeros / np.sum(target_buttons[:, i] == 0) * 100 if np.sum(target_buttons[:, i] == 0) > 0 else 0\n",
    "    acc_of_1 = correct_ones / np.sum(target_buttons[:, i] == 1) * 100 if np.sum(target_buttons[:, i] == 1) > 0 else 0\n",
    "\n",
    "    rows.append({\n",
    "        'Button': button,\n",
    "        'Accuracy': accuracy,\n",
    "        'Acc of 0': acc_of_0,\n",
    "        'Acc of 1': acc_of_1\n",
    "    })\n",
    "\n",
    "# Use concat to add all new rows to the DataFrame at once\n",
    "button_accuracy_df = pd.concat([button_accuracy_df, pd.DataFrame(rows)], ignore_index=True)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(button_accuracy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = integer_stick_targets[:,0:4] == 58\n",
    "\n",
    "# unique, counts = np.unique(integer_stick_pred[mask[:,0]] - integer_stick_targets[mask[:,0]], return_counts=True)\n",
    "# plt.bar(unique, counts)\n",
    "# print('Good enough accuracy of JSICK_X at 0', np.sum(counts[58-2:58+2]) / np.sum(counts))\n",
    "# print('Accuracy of JSTICK_X at 0', counts[58] / np.sum(counts))\n",
    "# plt.title('Difference Between JSTICK_X Prediction and Target at 0')\n",
    "# plt.show()\n",
    "\n",
    "# unique, counts = np.unique(integer_stick_pred[mask[:,1]] - integer_stick_targets[mask[:,1]], return_counts=True)\n",
    "# plt.bar(unique, counts)\n",
    "# print('Good enough accuracy of JSICK_X at 0', np.sum(counts[58-2:58+2]) / np.sum(counts))\n",
    "# print('Accuracy of JSTICK_X at 0', counts[58] / np.sum(counts))\n",
    "# plt.title('Difference Between JSTICK_Y Prediction and Target at 0')\n",
    "# plt.show()\n",
    "\n",
    "# unique, counts = np.unique(integer_stick_pred[mask[:,2]] - integer_stick_targets[mask[:,2]], return_counts=True)\n",
    "# plt.bar(unique, counts)\n",
    "# print('Good enough accuracy of JSICK_X at 0', np.sum(counts[58-2:58+2]) / np.sum(counts))\n",
    "# print('Accuracy of JSTICK_X at 0', counts[58] / np.sum(counts))\n",
    "# plt.title('Difference Between CSTICK_X Prediction and Target at 0')\n",
    "# plt.show()\n",
    "\n",
    "# unique, counts = np.unique(integer_stick_pred[mask[:,3]] - integer_stick_targets[mask[:,3]], return_counts=True)\n",
    "# plt.bar(unique, counts)\n",
    "# print('Good enough accuracy of JSICK_X at 0', np.sum(counts[58-2:58+2]) / np.sum(counts))\n",
    "# print('Accuracy of JSTICK_X at 0', counts[58] / np.sum(counts))\n",
    "# plt.title('Difference Between CSTICK_Y Prediction and Target at 0')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for i in range(4):\n",
    "    # bce = log_loss(target[:,i,:].flatten(),pred[:,i,:].flatten())\n",
    "    mse = mean_squared_error(target[:,i,:].flatten(),pred[:,i,:].flatten())\n",
    "    print('input',i)\n",
    "    # print('BCE', bce)\n",
    "    print('MSE', mse)\n",
    "    print()\n",
    "    \n",
    "for i in range(4,13):\n",
    "    bce = log_loss(target[:,i,:].flatten(),pred[:,i,:].flatten())\n",
    "    mse = mean_squared_error(target[:,i,:].flatten(),pred[:,i,:].flatten())\n",
    "    print('input',i)\n",
    "    print('BCE', bce)\n",
    "    print('MSE', mse)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_angle = np.arctan2((target[:,[0,2]] - .5) * 2, (target[:,[1,3]] - .5) * 2) #* 180 / np.pi\n",
    "predicted_angle = np.arctan2((pred[:,[0,2]] - .5) * 2, (pred[:,[1,3]] - .5) * 2) #* 180 / np.pi\n",
    "# predicted_angle = np.arctan2(pred[:,[0,2]], pred[:,[1,3]]) * 180 / np.pi\n",
    "\n",
    "angle_difference = (predicted_angle - target_angle) # between -2pi and 2pi\n",
    "smallest_angle_difference = (angle_difference + np.pi) % (2 * np.pi) - np.pi\n",
    "# smallest_angle_difference = (angle_difference + 180) % 360 - 180=\n",
    "\n",
    "# n,i = 4,0\n",
    "# print(target_angle[n,i])\n",
    "# print()\n",
    "# print(predicted_angle[n,i])\n",
    "# print()\n",
    "# print(smallest_angle_difference[n,i])\n",
    "\n",
    "print('JSTICK average angle', np.average(np.abs(smallest_angle_difference[:,0])))\n",
    "print('CSTICK average angle', np.average(np.abs(smallest_angle_difference[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_angle = np.arctan2((target[:,[0,2]] - .5) * 2, (target[:,[1,3]] - .5) * 2) * 180 / np.pi\n",
    "predicted_angle = np.arctan2((pred[:,[0,2]] - .5) * 2, (pred[:,[1,3]] - .5) * 2) * 180 / np.pi\n",
    "# predicted_angle = np.arctan2(pred[:,[0,2]], pred[:,[1,3]]) * 180 / np.pi\n",
    "\n",
    "angle_difference = (predicted_angle - target_angle) # between -2pi and 2pi\n",
    "# smallest_angle_difference = (angle_difference + np.pi) % (2 * np.pi) - np.pi\n",
    "smallest_angle_difference = (angle_difference + 180) % 360 - 180\n",
    "smallest_angle_difference *= (np.pi / 180)\n",
    "\n",
    "# n,i = 4,0\n",
    "# print(target_angle[n,i])\n",
    "# print()\n",
    "# print(predicted_angle[n,i])\n",
    "# print()\n",
    "# print(smallest_angle_difference[n,i])\n",
    "\n",
    "print('JSTICK average angle', np.average(np.abs(smallest_angle_difference[:,0])))\n",
    "print('CSTICK average angle', np.average(np.abs(smallest_angle_difference[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_radius_squared = target[:,[0,2]] ** 2 + target[:,[1,3]] ** 2\n",
    "pred_radius_squared = pred[:,[0,2]] ** 2 + pred[:,[1,3]] ** 2\n",
    "\n",
    "radius_difference = target_radius_squared - pred_radius_squared\n",
    "\n",
    "print('JSTICK average radius', np.average(np.abs(radius_difference[:,0])))\n",
    "print('CSTICK average radius', np.average(np.abs(radius_difference[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = np.unique((target[:,0] - .5) * 2)\n",
    "print(unique_values.shape)\n",
    "print(np.diff(unique_values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print((2 * np.pi) * .017545)\n",
    "print((2 * np.pi) * .017545 * math.sqrt(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = np.unique(np.arctan2((target[:,[0,2]] - .5) * 2, (target[:,[1,3]] - .5) * 2))\n",
    "print(unique_values.shape)\n",
    "print(np.diff(unique_values))\n",
    "print(np.average(np.diff(unique_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = np.unique(target[:,4:])\n",
    "print(unique_values)\n",
    "# print(np.diff(unique_values))\n",
    "# print(np.average(np.diff(unique_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # class CustomLoss(nn.Module):==\n",
    "#     def __init__(self):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "#         self.BCE = nn.BCEWithLogitsLoss(reduction='sum')  # Consider using weighted BCE if needed\n",
    "#         self.MSE = nn.MSELoss(reduction='sum')\n",
    "#         self.bin_threshold = 2\n",
    "#         # Self.bin_threshold bins to the left and right should account for the barely close enough threshold\n",
    "#         self.barely_close_enough = (.5 + self.bin_threshold) * 0.00862#\n",
    "#         self.scale_factor = - math.log(.5) / self.barely_close_enough ** 2\n",
    "        \n",
    "#     def forward(self, pred, target):\n",
    "#         # Calculating losses\n",
    "#         mse_loss = self.MSE(torch.sigmoid(pred[:,0:4,0:60]), target[:,0:4,0:60]) \n",
    "#         bce_loss = self.BCE(pred[:,4:,0:60], target[:,4:,0:60])\n",
    "        \n",
    "#         # Total loss\n",
    "#         return mse_loss * 100 + bce_loss\n",
    "import math\n",
    "(- math.log(.5) / (4.5* 0.00862) ** 2) ** (1/2)\n",
    "\n",
    "# ((- math.log(.5) * 100) ** (1/2)) / 0.00862 - .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segment = np.array([[0,0,0,0,1], [0,1,1,0,0],[1,0,0,0,1],[0,0,0,1,1],[1,1,1,0,0]])\n",
    "\n",
    "print(segment.shape)\n",
    "prepend = np.expand_dims(segment[:, 0], axis=1)\n",
    "print(prepend)\n",
    "transitions= np.abs(np.diff(segment[:], axis=1, prepend=prepend))\n",
    "\n",
    "print(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

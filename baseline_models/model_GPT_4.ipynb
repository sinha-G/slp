{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model.ipynb </h1>\n",
    "\n",
    "This file contains the model definition and training for the Sheik classification problem. We begin by loading required packages and checking the device being used by PyTorch. Use the GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "import numpy as np \n",
    "from prettytable import PrettyTable\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Definition</h1>\n",
    "GPT4: For your model, using F.sigmoid at the end is not recommended for binary classification tasks due to numerical stability issues. Instead, it's better to output raw logits from the last layer and use a loss function that includes the sigmoid operation, like nn.BCEWithLogitsLoss. For binary classification, nn.BCEWithLogitsLoss is suitable as it combines a sigmoid layer and the BCE loss in a single class, which is more numerically stable than using a plain nn.Sigmoid followed by nn.BCELoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(9, 18, 3)\n",
    "        self.conv2 = nn.Conv1d(18, 36, 5)\n",
    "        self.pool1 = nn.Conv1d(36, 18, 9, 4)\n",
    "        self.fc1 = nn.LazyLinear(128)  # Adjusted based on output shape from conv and pool layers\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x = F.sigmoid(self.fc3(x))\n",
    "        x = self.fc3(x)  # Output raw logits\n",
    "        \n",
    "        return x\n",
    "         \n",
    "    \n",
    "net = Net()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Read Data </h1>\n",
    "\n",
    "We reading the data saved in `data_processing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X  = np.load('../data/training_inputs_cart_numpy_binary.npy') # Stick input as cartesian coordinates.\n",
    "# X  = np.load('../data/training_inputs_polar_numpy_binary.npy') # Stick inputs as polar coordinates.\n",
    "\n",
    "# Load labels\n",
    "y  = np.load('../data/labes_is_sheik_numpy_binary.npy')\n",
    "\n",
    "# Print shape to make sure we have what we want.\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Splitting </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training + validation and holdout sets\n",
    "X_train_val, X_holdout, y_train_val, y_holdout = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Split training + validation set into separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)  # 0.25 * 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Loader </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays into tensors and create dataset objects\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "holdout_dataset = TensorDataset(torch.tensor(X_holdout, dtype=torch.float32), torch.tensor(y_holdout, dtype=torch.float32))\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64  # Can be tuned\n",
    "num_workers = 1 # Can be tuned\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Training </h1>\n",
    "\n",
    "Below are the training setup and training loop for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    # Apply sigmoid and threshold at 0.5\n",
    "    preds = torch.sigmoid(outputs) >= 0.5\n",
    "    correct = (preds.squeeze().long() == labels.long()).float().sum()\n",
    "    return correct / labels.shape[0]\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Training loop with progress bar, timing, and accuracy\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_accuracy = train_epoch(net, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = validate_epoch(net, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    \n",
    "    tqdm.write(f'Epoch {epoch+1}/{epochs} - Duration: {epoch_duration:.2f}s - Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f} - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Holdout Test </h1>\n",
    "\n",
    "We test the model on the holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_holdout(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    with torch.no_grad():  # No gradients needed\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Evaluate model on holdout set after training is complete\n",
    "holdout_loss, holdout_accuracy = evaluate_holdout(net, holdout_loader, criterion)\n",
    "print(f'Holdout Loss: {holdout_loss:.4f}, Accuracy: {holdout_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Parameter Count </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    table = PrettyTable(['Modules', 'Parameters'])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f'Total Trainable Params: {total_params}')\n",
    "    return total_params\n",
    "\n",
    "count_parameters(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

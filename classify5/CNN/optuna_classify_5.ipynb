{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "import gc\n",
    "\n",
    "import pymysql\n",
    "from datetime import datetime\n",
    "# import mysql.connector\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730754, 9, 1024)\n",
      "(730754, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X  = np.load('C:/Users/jaspa/Grant ML/slp/data/classify_5_data_2.npy')\n",
    "\n",
    "# Load labels\n",
    "y  = np.load('C:/Users/jaspa/Grant ML/slp/data/classify_5_labels_2.npy')\n",
    "\n",
    "# Print shape to make sure we have what we want.\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Convert one-hot encoded labels to integer labels for stratification </h1>\n",
    "For stratification with one-hot encoded labels, you need to convert these back to single integer labels. You can use np.argmax to achieve this because train_test_split's stratify parameter doesn't directly support one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels to integer labels\n",
    "y_int_labels = np.argmax(y, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Splitting </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training + validation and holdout sets\n",
    "X_train_val, X_holdout, y_train_val_int, y_holdout_int = train_test_split(X, y_int_labels, test_size=0.2, stratify=y_int_labels, random_state=42)\n",
    "\n",
    "# Convert integer labels back to one-hot encoding for training\n",
    "y_train_val = y[y_train_val_int]\n",
    "y_holdout = y[y_holdout_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Loader </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training + validation set into separate training and validation sets\n",
    "X_train, X_val, y_train_int, y_val_int = train_test_split(X_train_val, y_train_val_int, test_size=0.25, stratify=y_train_val_int, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Convert integer labels back to one-hot encoding for training\n",
    "y_train = y[y_train_int]\n",
    "y_val = y[y_val_int]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "holdout_dataset = TensorDataset(torch.tensor(X_holdout, dtype=torch.float32), torch.tensor(y_holdout, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# The data with 5 characters is roughly 16 times bigger than the is_sheik dataset\n",
    "num_batches = 32 * 16 # Adjust\n",
    "batch_size = X.shape[0] // num_batches  # Adjust as needed\n",
    "batch_size = 2 ** 12\n",
    "batch_size = 2 ** 9\n",
    "print(batch_size)\n",
    "# print(X.shape[0] / 2 ** 12)\n",
    "num_workers = 1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Funtion to calculate accuracy </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)  # Get the index of the max logit\n",
    "    correct = torch.sum(preds == labels).float()  # Count how many predictions match the labels\n",
    "    return correct / labels.shape[0]\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)  # Convert logits to class predictions\n",
    "    if labels.dim() > 1:  # Assuming labels are one-hot encoded\n",
    "        labels = torch.argmax(labels, dim=1)  # Convert to class indices\n",
    "\n",
    "    correct = torch.sum(preds == labels).float()  # Count correct predictions\n",
    "    return correct / labels.size(0)  # Compute accuracy\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable(['Modules', 'Parameters'])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f'Total Trainable Params: {total_params}')\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define the Customizable Network </h1>\n",
    "We define the neural network architecture. We'll use Optuna to suggest hyperparameters for convolutional layers, optional ReLU activation, max pooling layers, and linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, trial):\n",
    "        super(CustomNet, self).__init__()\n",
    "\n",
    "        # Fixed dropout rate (not tuned by Optuna)\n",
    "        dropout_rate = 0.5\n",
    "\n",
    "        # Convolutional layers setup\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.activations = []\n",
    "        self.poolings = []\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "\n",
    "        num_layers = trial.suggest_int(f\"num_conv_layers\", 3, 5)\n",
    "        in_channels = 9  # Fixed input channel size\n",
    "\n",
    "        \n",
    "        \n",
    "        ######################################################################################################\n",
    "        # In length is 2 ** 10\n",
    "        # Padding is set up so that the out length is always reduced by 1 / 2 ** out_length_reduction_exponent\n",
    "        # The length of a kernel is: kernel + (dilation - 1) * (kernel_size - 1)\n",
    "        # The max lenght of a kernel is 25 which is kernel_size = 7 and dilation = 4\n",
    "        # The in lenght can never be less than 25\n",
    "        # Since the in lenght is always a power of 2, the in lenght can be no less than 2 ** 5 = 32,\n",
    "        # we need to make sure not to reduce the in lenght too much, we keep track of\n",
    "        # how much we can still reduce the length by using length_reduction_power_left which is set to 5.\n",
    "        ######################################################################################################\n",
    "        length_reduction_exporent_remaining = 5\n",
    "        in_length_exponent = 10\n",
    "        for i in range(num_layers):  # Convolutional layers\n",
    "            ###########################\n",
    "            # In length is a power of 2\n",
    "            ###########################\n",
    "            if i == 0: \n",
    "                out_channels = trial.suggest_int(f\"conv_{i}_out_channels\", 9, 9 * 48, step = 9)\n",
    "                groups = 9\n",
    "            elif i == -1:\n",
    "                out_channels = trial.suggest_int(f\"conv_{i}_out_channels\", 1, 256)\n",
    "                groups = 1\n",
    "            else:\n",
    "                out_channels = trial.suggest_int(f\"conv_{i}_out_channels\", 1, 512)\n",
    "                groups = 1\n",
    "            # kernel_size = trial.suggest_int(f\"conv_{i}_kernel_size\", 3, 7, step=2)\n",
    "            k = trial.suggest_int(f\"conv_{i}_kernel_size_power\", 1, 5)  # can safely change 5 to be anything\n",
    "            kernel_size = 2 * k + 1\n",
    "            dilation = trial.suggest_int(f\"conv_{i}_dilation\", 1, 4)\n",
    "            out_length_reduction_exponent = trial.suggest_int(f\"conv_{i}_out_length_reduction_exponent\", 0, min(2,length_reduction_exporent_remaining))\n",
    "            # conv_stride_length_exponent = trial.suggest_int(f\"conv_{i}_stride_length_exponent\", 0, out_length_reduction_exponent)\n",
    "            conv_stride_length_exponent = out_length_reduction_exponent\n",
    "            # Keep track of how much reducing we still can do\n",
    "            length_reduction_exporent_remaining -= out_length_reduction_exponent\n",
    "            in_length_exponent -= out_length_reduction_exponent\n",
    "            # Set stride\n",
    "            stride = 2 ** conv_stride_length_exponent\n",
    "            # Padding is chosen so that out length is a power of 2\n",
    "            # there is a floor in the formula. If we want to use more than 2 for out_length_reduction_exponent, we neen do caluclate the cases\n",
    "            if (conv_stride_length_exponent == 2) and (((dilation * k) % 2) == 1):\n",
    "                padding = dilation * k - 1\n",
    "            else:\n",
    "                padding = dilation * k\n",
    "                \n",
    "            self.conv_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size,stride, padding, dilation, groups))\n",
    "            in_channels = out_channels  # Update in_channels for the next layer\n",
    "\n",
    "            if conv_stride_length_exponent < out_length_reduction_exponent:\n",
    "                pooling_type = trial.suggest_int(f\"layer_{i}_pooling_type\", 0, 1)    # 1: max, 0: avg\n",
    "                pool_kernal_size_exponent = out_length_reduction_exponent - conv_stride_length_exponent\n",
    "                if pooling_type == 1:\n",
    "                    self.poolings.append(nn.MaxPool1d(2 ** pool_kernal_size_exponent))\n",
    "                else:\n",
    "                    self.poolings.append(nn.AvgPool1d(2 ** pool_kernal_size_exponent))\n",
    "            else:\n",
    "                self.poolings.append(None)    #   No pooling in current layer\n",
    "\n",
    "            \n",
    "            # Optional Batch Normalization\n",
    "            use_bn = trial.suggest_categorical(f\"conv_{i}_bn\", [True, False])\n",
    "            if use_bn:\n",
    "                self.bns.append(nn.BatchNorm1d(in_channels))\n",
    "            else:\n",
    "                self.bns.append(None)\n",
    "\n",
    "            # Optional ReLU activation\n",
    "            use_activation = trial.suggest_categorical(f\"conv_{i}_activation\", [True, False])\n",
    "            self.activations.append(use_activation)\n",
    "\n",
    "        \n",
    "        # Max pooling layer\n",
    "        # The kernel can be a power of two, up to the in lenght\n",
    "        # In length of the output will be 2 ** out_length_exponent\n",
    "        # and lenght can be 1, 2, 4, 8, 16, 32\n",
    "        \n",
    "        kernel_exponent = trial.suggest_int(f\"maxpool_kernel_exponent\",length_reduction_exporent_remaining , in_length_exponent)\n",
    "        kernel_size = 2 ** kernel_exponent\n",
    "        in_length_exponent -= kernel_exponent\n",
    "        \n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "        \n",
    "        \n",
    "        # The length right now should be 2 ** in_length_exponent, so we can be exact in our first lineal layer\n",
    "        self.fc1 = nn.Linear(out_channels * 2 ** in_length_exponent, trial.suggest_int(\"fc1_out_features\", 32, 256))\n",
    "        # self.fc1 = nn.LazyLinear(trial.suggest_int(\"fc1_out_features\", 64, 256))\n",
    "        self.fc1_dropout = nn.Dropout(dropout_rate)  # Dropout after fc1\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, trial.suggest_int(\"fc2_out_features\", 32, 128))\n",
    "        self.fc2_dropout = nn.Dropout(dropout_rate)  # Dropout after fc2\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, 5)  # Output layer with 5 unit for classification with 5 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with optional ReLU and fixed dropout\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            x = conv_layer(x)\n",
    "            if self.bns[i]:\n",
    "                x = self.bns[i](x)\n",
    "            if self.poolings[i]:\n",
    "                x = self.poolings[i](x)\n",
    "            if self.activations[i]:\n",
    "                x = F.relu(x)            \n",
    "\n",
    "        # Optional max pooling after conv layers\n",
    "        # if self.use_pool1:\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_dropout(x)\n",
    "        x = self.fc3(x)  # Output without activation for BCEWithLogitsLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Define the Objective Function </h1>\n",
    "We define the objective function for Optuna, which involves training and validating the model with the suggested hyperparameters to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model with hyperparameters suggested by Optuna\n",
    "    model = CustomNet(trial).to(device)\n",
    "\n",
    "    # print(f\"Trial {trial.number}:\")\n",
    "    # print(model)\n",
    "\n",
    "    # Load and prepare data (assuming X and y are already loaded)\n",
    "    # Splitting, converting to TensorDataset, and DataLoader setup would go here\n",
    "\n",
    "    # Define the optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    def train_epoch(model, dataloader, optimizer, criterion):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "            \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "        return epoch_loss, epoch_accuracy\n",
    "\n",
    "    def validate_epoch(model, dataloader, criterion):\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                # loss = criterion(torch.sigmoid(outputs.squeeze()), labels)\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "                \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "        return epoch_loss, epoch_accuracy\n",
    "\n",
    "    def evaluate_holdout(model, dataloader, criterion):\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        with torch.no_grad():  # No gradients needed\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
    "                \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_accuracy = running_accuracy / len(dataloader.dataset)\n",
    "        return epoch_loss, epoch_accuracy\n",
    "                \n",
    "    # Training loop with early stopping and tqdm progress bar\n",
    "    patience = 3\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    epochs_overfit = 0\n",
    "    epochs = 250\n",
    "    min_delta = 0.001\n",
    "    min_overfit = .275\n",
    "    min_overfit = .025\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm(total=epochs, desc=\"Epochs\", position=0, leave=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_accuracy = validate_epoch(model, val_loader, criterion)\n",
    "        \n",
    "        # Early Stopping check and progress bar update\n",
    "        if (val_loss + min_delta) < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if abs(train_loss - val_loss) < min_overfit:\n",
    "            epochs_overfit = 0\n",
    "        else:\n",
    "            epochs_overfit += 1\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix_str(f\"Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "        pbar.update(1)  # Move the progress bar by one epoch\n",
    "\n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve >= patience or epochs_overfit >= patience:\n",
    "            # pbar.write(f'Early stopping triggered at epoch {epoch + 1}')\n",
    "            pbar.close()  # Close the progress bar\n",
    "            break\n",
    "\n",
    "    # Evaluate model on holdout set after training is complete (if necessary)\n",
    "    holdout_loss, holdout_accuracy = evaluate_holdout(model, holdout_loader, criterion)\n",
    "    print(f'Holdout Loss: {holdout_loss:.4f}, Accuracy: {holdout_accuracy:.4f}')\n",
    "    count_parameters(CustomNet(trial))\n",
    "    pbar.close()  # Ensure the progress bar is closed\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Run the Optimization </h1>\n",
    "We create an Optuna study and then iterate the optimizer separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-25 11:40:49,588] A new study created in RDB with name: 2024-02-25 11:40:49 Classical CNN\n"
     ]
    }
   ],
   "source": [
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "current_datetime_string = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S \")\n",
    "\n",
    "study = optuna.create_study(study_name = current_datetime_string + \"Classical CNN\",\n",
    "                            direction=\"minimize\",\n",
    "                            storage = \"mysql+pymysql://root:MomentusPigs@localhost:3306/optuna_trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd83d86a11d48498f7c968531eadc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 19/250 [16:15<3:17:40, 51.35s/it, Training Loss: 0.8528, Accuracy: 0.5981, Validation Loss: 0.8904, Accuracy: 0.5974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Loss: 0.8896, Accuracy: 0.5970\n",
      "+----------------------+------------+\n",
      "|       Modules        | Parameters |\n",
      "+----------------------+------------+\n",
      "| conv_layers.0.weight |    837     |\n",
      "|  conv_layers.0.bias  |    279     |\n",
      "| conv_layers.1.weight |   482391   |\n",
      "|  conv_layers.1.bias  |    247     |\n",
      "| conv_layers.2.weight |    4446    |\n",
      "|  conv_layers.2.bias  |     2      |\n",
      "| conv_layers.3.weight |    5908    |\n",
      "|  conv_layers.3.bias  |    422     |\n",
      "|     bns.0.weight     |    279     |\n",
      "|      bns.0.bias      |    279     |\n",
      "|     bns.1.weight     |    247     |\n",
      "|      bns.1.bias      |    247     |\n",
      "|     bns.2.weight     |     2      |\n",
      "|      bns.2.bias      |     2      |\n",
      "|      fc1.weight      |   69208    |\n",
      "|       fc1.bias       |     41     |\n",
      "|      fc2.weight      |    4428    |\n",
      "|       fc2.bias       |    108     |\n",
      "|      fc3.weight      |    540     |\n",
      "|       fc3.bias       |     5      |\n",
      "+----------------------+------------+\n",
      "Total Trainable Params: 569918\n",
      "[I 2024-02-25 11:57:15,719] Trial 0 finished with value: 0.8732714363578756 and parameters: {'num_conv_layers': 4, 'conv_0_out_channels': 279, 'conv_0_kernel_size_power': 1, 'conv_0_dilation': 3, 'conv_0_out_length_reduction_exponent': 1, 'conv_0_bn': True, 'conv_0_activation': False, 'conv_1_out_channels': 247, 'conv_1_kernel_size_power': 3, 'conv_1_dilation': 4, 'conv_1_out_length_reduction_exponent': 0, 'conv_1_bn': True, 'conv_1_activation': False, 'conv_2_out_channels': 2, 'conv_2_kernel_size_power': 4, 'conv_2_dilation': 3, 'conv_2_out_length_reduction_exponent': 0, 'conv_2_bn': True, 'conv_2_activation': True, 'conv_3_out_channels': 422, 'conv_3_kernel_size_power': 3, 'conv_3_dilation': 2, 'conv_3_out_length_reduction_exponent': 0, 'conv_3_bn': False, 'conv_3_activation': True, 'maxpool_kernel_exponent': 7, 'fc1_out_features': 41, 'fc2_out_features': 108}. Best is trial 0 with value: 0.8732714363578756.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 6/250 [02:46<1:53:09, 27.83s/it, Training Loss: 0.6749, Accuracy: 0.7049, Validation Loss: 0.8104, Accuracy: 0.6191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Loss: 0.8141, Accuracy: 0.6164\n",
      "+----------------------+------------+\n",
      "|       Modules        | Parameters |\n",
      "+----------------------+------------+\n",
      "| conv_layers.0.weight |    297     |\n",
      "|  conv_layers.0.bias  |     27     |\n",
      "| conv_layers.1.weight |   82863    |\n",
      "|  conv_layers.1.bias  |    279     |\n",
      "| conv_layers.2.weight |  1054620   |\n",
      "|  conv_layers.2.bias  |    420     |\n",
      "|     bns.0.weight     |     27     |\n",
      "|      bns.0.bias      |     27     |\n",
      "|      fc1.weight      |   84840    |\n",
      "|       fc1.bias       |    101     |\n",
      "|      fc2.weight      |    7575    |\n",
      "|       fc2.bias       |     75     |\n",
      "|      fc3.weight      |    375     |\n",
      "|       fc3.bias       |     5      |\n",
      "+----------------------+------------+\n",
      "Total Trainable Params: 1231531\n",
      "[I 2024-02-25 12:00:08,667] Trial 1 finished with value: 0.8033129432000065 and parameters: {'num_conv_layers': 3, 'conv_0_out_channels': 27, 'conv_0_kernel_size_power': 5, 'conv_0_dilation': 2, 'conv_0_out_length_reduction_exponent': 1, 'conv_0_bn': True, 'conv_0_activation': True, 'conv_1_out_channels': 279, 'conv_1_kernel_size_power': 5, 'conv_1_dilation': 4, 'conv_1_out_length_reduction_exponent': 1, 'conv_1_bn': False, 'conv_1_activation': True, 'conv_2_out_channels': 420, 'conv_2_kernel_size_power': 4, 'conv_2_dilation': 4, 'conv_2_out_length_reduction_exponent': 2, 'conv_2_bn': False, 'conv_2_activation': True, 'maxpool_kernel_exponent': 5, 'fc1_out_features': 101, 'fc2_out_features': 75}. Best is trial 1 with value: 0.8033129432000065.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 3/250 [07:57<10:54:52, 159.08s/it, Training Loss: 0.8186, Accuracy: 0.6259, Validation Loss: 3.4921, Accuracy: 0.4030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Loss: 3.4921, Accuracy: 0.4030\n",
      "+----------------------+------------+\n",
      "|       Modules        | Parameters |\n",
      "+----------------------+------------+\n",
      "| conv_layers.0.weight |    2574    |\n",
      "|  conv_layers.0.bias  |    234     |\n",
      "| conv_layers.1.weight |   640926   |\n",
      "|  conv_layers.1.bias  |    249     |\n",
      "| conv_layers.2.weight |   345114   |\n",
      "|  conv_layers.2.bias  |    198     |\n",
      "| conv_layers.3.weight |   502524   |\n",
      "|  conv_layers.3.bias  |    282     |\n",
      "| conv_layers.4.weight |   102366   |\n",
      "|  conv_layers.4.bias  |     33     |\n",
      "|     bns.2.weight     |    198     |\n",
      "|      bns.2.bias      |    198     |\n",
      "|     bns.3.weight     |    282     |\n",
      "|      bns.3.bias      |    282     |\n",
      "|      fc1.weight      |   20064    |\n",
      "|       fc1.bias       |     38     |\n",
      "|      fc2.weight      |    2166    |\n",
      "|       fc2.bias       |     57     |\n",
      "|      fc3.weight      |    285     |\n",
      "|       fc3.bias       |     5      |\n",
      "+----------------------+------------+\n",
      "Total Trainable Params: 1618075\n",
      "[I 2024-02-25 12:08:24,308] Trial 2 finished with value: 1.2080582648564844 and parameters: {'num_conv_layers': 5, 'conv_0_out_channels': 234, 'conv_0_kernel_size_power': 5, 'conv_0_dilation': 3, 'conv_0_out_length_reduction_exponent': 0, 'conv_0_bn': False, 'conv_0_activation': True, 'conv_1_out_channels': 249, 'conv_1_kernel_size_power': 5, 'conv_1_dilation': 1, 'conv_1_out_length_reduction_exponent': 0, 'conv_1_bn': False, 'conv_1_activation': True, 'conv_2_out_channels': 198, 'conv_2_kernel_size_power': 3, 'conv_2_dilation': 3, 'conv_2_out_length_reduction_exponent': 0, 'conv_2_bn': True, 'conv_2_activation': True, 'conv_3_out_channels': 282, 'conv_3_kernel_size_power': 4, 'conv_3_dilation': 1, 'conv_3_out_length_reduction_exponent': 0, 'conv_3_bn': True, 'conv_3_activation': False, 'conv_4_out_channels': 33, 'conv_4_kernel_size_power': 5, 'conv_4_dilation': 3, 'conv_4_out_length_reduction_exponent': 0, 'conv_4_bn': False, 'conv_4_activation': True, 'maxpool_kernel_exponent': 6, 'fc1_out_features': 38, 'fc2_out_features': 57}. Best is trial 1 with value: 0.8033129432000065.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 3/250 [02:55<4:00:32, 58.43s/it, Training Loss: 0.8845, Accuracy: 0.5694, Validation Loss: 1.0705, Accuracy: 0.4030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Loss: 1.0705, Accuracy: 0.4030\n",
      "+----------------------+------------+\n",
      "|       Modules        | Parameters |\n",
      "+----------------------+------------+\n",
      "| conv_layers.0.weight |    4653    |\n",
      "|  conv_layers.0.bias  |    423     |\n",
      "| conv_layers.1.weight |   772398   |\n",
      "|  conv_layers.1.bias  |    166     |\n",
      "| conv_layers.2.weight |   417158   |\n",
      "|  conv_layers.2.bias  |    359     |\n",
      "| conv_layers.3.weight |   954940   |\n",
      "|  conv_layers.3.bias  |    380     |\n",
      "| conv_layers.4.weight |   375060   |\n",
      "|  conv_layers.4.bias  |    329     |\n",
      "|     bns.2.weight     |    359     |\n",
      "|      bns.2.bias      |    359     |\n",
      "|     bns.3.weight     |    380     |\n",
      "|      bns.3.bias      |    380     |\n",
      "|     bns.4.weight     |    329     |\n",
      "|      bns.4.bias      |    329     |\n",
      "|      fc1.weight      |  1084384   |\n",
      "|       fc1.bias       |    103     |\n",
      "|      fc2.weight      |    4635    |\n",
      "|       fc2.bias       |     45     |\n",
      "|      fc3.weight      |    225     |\n",
      "|       fc3.bias       |     5      |\n",
      "+----------------------+------------+\n",
      "Total Trainable Params: 3617399\n",
      "[I 2024-02-25 12:11:28,895] Trial 3 finished with value: 1.0704511022872152 and parameters: {'num_conv_layers': 5, 'conv_0_out_channels': 423, 'conv_0_kernel_size_power': 5, 'conv_0_dilation': 2, 'conv_0_out_length_reduction_exponent': 1, 'conv_0_bn': False, 'conv_0_activation': True, 'conv_1_out_channels': 166, 'conv_1_kernel_size_power': 5, 'conv_1_dilation': 4, 'conv_1_out_length_reduction_exponent': 0, 'conv_1_bn': False, 'conv_1_activation': False, 'conv_2_out_channels': 359, 'conv_2_kernel_size_power': 3, 'conv_2_dilation': 1, 'conv_2_out_length_reduction_exponent': 2, 'conv_2_bn': True, 'conv_2_activation': False, 'conv_3_out_channels': 380, 'conv_3_kernel_size_power': 3, 'conv_3_dilation': 2, 'conv_3_out_length_reduction_exponent': 2, 'conv_3_bn': True, 'conv_3_activation': False, 'conv_4_out_channels': 329, 'conv_4_kernel_size_power': 1, 'conv_4_dilation': 3, 'conv_4_out_length_reduction_exponent': 0, 'conv_4_bn': True, 'conv_4_activation': True, 'maxpool_kernel_exponent': 0, 'fc1_out_features': 103, 'fc2_out_features': 45}. Best is trial 1 with value: 0.8033129432000065.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▍         | 12/250 [07:26<2:27:31, 37.19s/it, Training Loss: 0.6756, Accuracy: 0.7095, Validation Loss: 0.7160, Accuracy: 0.6783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Loss: 0.7137, Accuracy: 0.6805\n",
      "+----------------------+------------+\n",
      "|       Modules        | Parameters |\n",
      "+----------------------+------------+\n",
      "| conv_layers.0.weight |    1980    |\n",
      "|  conv_layers.0.bias  |    396     |\n",
      "| conv_layers.1.weight |   209088   |\n",
      "|  conv_layers.1.bias  |    176     |\n",
      "| conv_layers.2.weight |   75680    |\n",
      "|  conv_layers.2.bias  |     86     |\n",
      "|     bns.0.weight     |    396     |\n",
      "|      bns.0.bias      |    396     |\n",
      "|     bns.1.weight     |    176     |\n",
      "|      bns.1.bias      |    176     |\n",
      "|      fc1.weight      |   302720   |\n",
      "|       fc1.bias       |    110     |\n",
      "|      fc2.weight      |   12210    |\n",
      "|       fc2.bias       |    111     |\n",
      "|      fc3.weight      |    555     |\n",
      "|       fc3.bias       |     5      |\n",
      "+----------------------+------------+\n",
      "Total Trainable Params: 604261\n",
      "[I 2024-02-25 12:19:01,838] Trial 4 finished with value: 0.7159914605097182 and parameters: {'num_conv_layers': 3, 'conv_0_out_channels': 396, 'conv_0_kernel_size_power': 2, 'conv_0_dilation': 1, 'conv_0_out_length_reduction_exponent': 1, 'conv_0_bn': True, 'conv_0_activation': False, 'conv_1_out_channels': 176, 'conv_1_kernel_size_power': 1, 'conv_1_dilation': 1, 'conv_1_out_length_reduction_exponent': 0, 'conv_1_bn': True, 'conv_1_activation': True, 'conv_2_out_channels': 86, 'conv_2_kernel_size_power': 2, 'conv_2_dilation': 4, 'conv_2_out_length_reduction_exponent': 2, 'conv_2_bn': False, 'conv_2_activation': True, 'maxpool_kernel_exponent': 2, 'fc1_out_features': 110, 'fc2_out_features': 111}. Best is trial 4 with value: 0.7159914605097182.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 6/250 [07:08<4:50:16, 71.38s/it, Training Loss: 0.7248, Accuracy: 0.6795, Validation Loss: 0.7632, Accuracy: 0.6542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Loss: 0.7632, Accuracy: 0.6539\n",
      "+----------------------+------------+\n",
      "|       Modules        | Parameters |\n",
      "+----------------------+------------+\n",
      "| conv_layers.0.weight |    1386    |\n",
      "|  conv_layers.0.bias  |    198     |\n",
      "| conv_layers.1.weight |   601128   |\n",
      "|  conv_layers.1.bias  |    276     |\n",
      "| conv_layers.2.weight |   543444   |\n",
      "|  conv_layers.2.bias  |    179     |\n",
      "| conv_layers.3.weight |   622741   |\n",
      "|  conv_layers.3.bias  |    497     |\n",
      "| conv_layers.4.weight |  1842379   |\n",
      "|  conv_layers.4.bias  |    337     |\n",
      "|     bns.0.weight     |    198     |\n",
      "|      bns.0.bias      |    198     |\n",
      "|     bns.1.weight     |    276     |\n",
      "|      bns.1.bias      |    276     |\n",
      "|     bns.2.weight     |    179     |\n",
      "|      bns.2.bias      |    179     |\n",
      "|      fc1.weight      |  1089184   |\n",
      "|       fc1.bias       |    202     |\n",
      "|      fc2.weight      |    9898    |\n",
      "|       fc2.bias       |     49     |\n",
      "|      fc3.weight      |    245     |\n",
      "|       fc3.bias       |     5      |\n",
      "+----------------------+------------+\n",
      "Total Trainable Params: 4713454\n",
      "[I 2024-02-25 12:26:19,706] Trial 5 finished with value: 0.7631761295022311 and parameters: {'num_conv_layers': 5, 'conv_0_out_channels': 198, 'conv_0_kernel_size_power': 3, 'conv_0_dilation': 3, 'conv_0_out_length_reduction_exponent': 0, 'conv_0_bn': True, 'conv_0_activation': True, 'conv_1_out_channels': 276, 'conv_1_kernel_size_power': 5, 'conv_1_dilation': 4, 'conv_1_out_length_reduction_exponent': 1, 'conv_1_bn': True, 'conv_1_activation': False, 'conv_2_out_channels': 179, 'conv_2_kernel_size_power': 5, 'conv_2_dilation': 3, 'conv_2_out_length_reduction_exponent': 2, 'conv_2_bn': True, 'conv_2_activation': True, 'conv_3_out_channels': 497, 'conv_3_kernel_size_power': 3, 'conv_3_dilation': 3, 'conv_3_out_length_reduction_exponent': 1, 'conv_3_bn': False, 'conv_3_activation': False, 'conv_4_out_channels': 337, 'conv_4_kernel_size_power': 5, 'conv_4_dilation': 3, 'conv_4_out_length_reduction_exponent': 0, 'conv_4_bn': False, 'conv_4_activation': True, 'maxpool_kernel_exponent': 2, 'fc1_out_features': 202, 'fc2_out_features': 49}. Best is trial 4 with value: 0.7159914605097182.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   2%|▏         | 4/250 [01:39<1:42:27, 24.99s/it, Training Loss: 0.8433, Accuracy: 0.6006, Validation Loss: 1.0572, Accuracy: 0.4030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Loss: 1.0572, Accuracy: 0.4030\n",
      "+----------------------+------------+\n",
      "|       Modules        | Parameters |\n",
      "+----------------------+------------+\n",
      "| conv_layers.0.weight |    297     |\n",
      "|  conv_layers.0.bias  |     99     |\n",
      "| conv_layers.1.weight |   184338   |\n",
      "|  conv_layers.1.bias  |    266     |\n",
      "| conv_layers.2.weight |   42294    |\n",
      "|  conv_layers.2.bias  |     53     |\n",
      "| conv_layers.3.weight |   104940   |\n",
      "|  conv_layers.3.bias  |    220     |\n",
      "|     bns.3.weight     |    220     |\n",
      "|      bns.3.bias      |    220     |\n",
      "|      fc1.weight      |   170720   |\n",
      "|       fc1.bias       |    194     |\n",
      "|      fc2.weight      |    6984    |\n",
      "|       fc2.bias       |     36     |\n",
      "|      fc3.weight      |    180     |\n",
      "|       fc3.bias       |     5      |\n",
      "+----------------------+------------+\n",
      "Total Trainable Params: 511066\n",
      "[I 2024-02-25 12:28:05,738] Trial 6 finished with value: 1.0563038037791965 and parameters: {'num_conv_layers': 4, 'conv_0_out_channels': 99, 'conv_0_kernel_size_power': 1, 'conv_0_dilation': 4, 'conv_0_out_length_reduction_exponent': 0, 'conv_0_bn': False, 'conv_0_activation': False, 'conv_1_out_channels': 266, 'conv_1_kernel_size_power': 3, 'conv_1_dilation': 3, 'conv_1_out_length_reduction_exponent': 2, 'conv_1_bn': False, 'conv_1_activation': True, 'conv_2_out_channels': 53, 'conv_2_kernel_size_power': 1, 'conv_2_dilation': 3, 'conv_2_out_length_reduction_exponent': 2, 'conv_2_bn': False, 'conv_2_activation': False, 'conv_3_out_channels': 220, 'conv_3_kernel_size_power': 4, 'conv_3_dilation': 1, 'conv_3_out_length_reduction_exponent': 0, 'conv_3_bn': True, 'conv_3_activation': False, 'maxpool_kernel_exponent': 4, 'fc1_out_features': 194, 'fc2_out_features': 36}. Best is trial 4 with value: 0.7159914605097182.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   9%|▉         | 23/250 [1:03:19<10:15:58, 162.81s/it, Training Loss: 0.7848, Accuracy: 0.6476, Validation Loss: 0.8103, Accuracy: 0.6755]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2024-02-25 13:31:25,648] Trial 7 failed with parameters: {'num_conv_layers': 3, 'conv_0_out_channels': 414, 'conv_0_kernel_size_power': 1, 'conv_0_dilation': 4, 'conv_0_out_length_reduction_exponent': 0, 'conv_0_bn': True, 'conv_0_activation': True, 'conv_1_out_channels': 436, 'conv_1_kernel_size_power': 5, 'conv_1_dilation': 3, 'conv_1_out_length_reduction_exponent': 0, 'conv_1_bn': False, 'conv_1_activation': False, 'conv_2_out_channels': 90, 'conv_2_kernel_size_power': 3, 'conv_2_dilation': 1, 'conv_2_out_length_reduction_exponent': 0, 'conv_2_bn': True, 'conv_2_activation': False, 'maxpool_kernel_exponent': 5, 'fc1_out_features': 41, 'fc2_out_features': 120} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jaspa\\AppData\\Local\\Temp\\ipykernel_34988\\1649193968.py\", line 86, in objective\n",
      "    train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, criterion)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jaspa\\AppData\\Local\\Temp\\ipykernel_34988\\1649193968.py\", line 30, in train_epoch\n",
      "    running_loss += loss.item() * inputs.size(0)\n",
      "                    ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-02-25 13:31:25,656] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# # Print the overall best hyperparameters\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(\"Best trial overall:\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# trial = study.best_trial\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# for key, value in trial.params.items():\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#     print(f\"    {key}: {value}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\pytorch\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[10], line 86\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     83\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mepochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 86\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, criterion)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# Early Stopping check and progress bar update\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mobjective.<locals>.train_epoch\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 30\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m     running_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m calculate_accuracy(outputs, labels) \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     33\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study.optimize(objective, n_trials=1000, show_progress_bar=True, timeout=3600*6)\n",
    "\n",
    "# # Print the overall best hyperparameters\n",
    "# print(\"Best trial overall:\")\n",
    "# trial = study.best_trial\n",
    "# print(f\"  Value: {trial.value}\")\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "\n",
    "We will extract data from multiple `.slp` (Slippi) replay files and store the extracted information in various formats. This notebook uses a **modified version** of `py-slippi` to handle replays with missing metadata (particularly in the *ranked* dataset). \n",
    "\n",
    "## Datasets\n",
    "\n",
    "We have three main datasets of `.slp` files:\n",
    "\n",
    "1. **Ranked (116,248 anonymized replays)**  \n",
    "   - Obtained from the pinned messages in the Slippi Discord's `#artificial-intelligence` channel.  \n",
    "   - We refer to this as `ranked`.\n",
    "\n",
    "2. **Public (95,102 `.slp` files from the Slippi Public Dataset v3)**  \n",
    "   - Also available in the pinned messages in the `#artificial-intelligence` Slippi Discord channel.  \n",
    "   - The dataset unzips to about 200 GB of `.slp` files.  \n",
    "   - We refer to this as `public`.\n",
    "\n",
    "3. **Mang0's Patreon Replays**  \n",
    "   - Retrieved from Mang0’s Patreon.  \n",
    "   - We refer to this dataset as `mango`.\n",
    "\n",
    "Because the *Ranked* dataset may not contain certain metadata fields, we rely on a modified `py-slippi` to handle these incomplete metadata entries. \n",
    "\n",
    "### Extraction Strategy\n",
    "\n",
    "We will extract the following items from each `.slp` replay:\n",
    "\n",
    "- **Replay-level metadata (non-frame data)**  \n",
    "  A single row in a Pandas DataFrame represents each replay. Within that DataFrame, we store columns describing replay metadata (e.g., characters, player ports, stage, etc.), as well as the **file paths** to the corresponding frame data files.\n",
    "\n",
    "- **Per-frame data**  \n",
    "  All frame-level information for each player is saved in a separate Parquet file for easy query and analysis later on.\n",
    "\n",
    "- **Per-player input data**  \n",
    "  Each player’s input actions (buttons, analog sticks, etc.) are extracted into a DataFrame and saved as a Parquet file.  \n",
    "\n",
    "- **Subset of player inputs for model training**  \n",
    "  A condensed view of inputs (potentially removing the first 123 “pre-match” frames) stored in compressed `.npz.gz` files for direct ingestion into training pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import polars as pl\n",
    "import tqdm\n",
    "import slippi as slp\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "import gzip\n",
    "import pickle\n",
    "import feather\n",
    "import uuid\n",
    "import time\n",
    "import tables\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Functions for Handling Paths and Files </h2>\n",
    "Here, we define helper functions to handle directory creation, path splitting, and cleanup operations:\n",
    "\n",
    "1. **`create_directories(path)`**: Creates a directory if it doesn’t exist.  \n",
    "2. **`split_paths(paths)`**: Finds and returns the common prefix among a list of file paths, along with the unique remainders.  \n",
    "3. **`add_to_triples(file_paths, common_path, source, list_of_paths)`**: Appends tuples \\(`(common_path, file, source)`\\) to our master list of paths.  \n",
    "4. **`delete_folder(path)`**: Removes a specified folder and all contents within it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create necessary directories\n",
    "def create_directories(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "# Split a list of paths into the common path and the rest of the path\n",
    "def split_paths(paths):\n",
    "    # Find the common prefix\n",
    "    common_prefix = os.path.commonprefix(paths)\n",
    "    \n",
    "    # Ensure the common prefix ends with a directory separator\n",
    "    common_path = os.path.dirname(common_prefix) + os.sep\n",
    "\n",
    "    # Split each path into the common part and the unique part\n",
    "    split_paths = [ path[len(common_path):] for path in paths]\n",
    "\n",
    "    return common_path, split_paths\n",
    "\n",
    "\n",
    "# Function that puts all the information about where the file we send to process_slp_file into lists\n",
    "def add_to_triples(file_paths, common_path, source, list_of_paths):\n",
    "    for file in file_paths:\n",
    "        list_of_paths.append((common_path, file, source))\n",
    "    return list_of_paths\n",
    "\n",
    "def delete_folder(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"The folder '{path}' has been deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting the folder '{path}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Handling Paths and Files\n",
    "Here, we define helper functions to handle directory creation, path splitting, and cleanup operations:\n",
    "\n",
    "1. **`create_directories(path)`**: Creates a directory if it doesn’t exist.  \n",
    "2. **`split_paths(paths)`**: Finds and returns the common prefix among a list of file paths, along with the unique remainders.  \n",
    "3. **`add_to_triples(file_paths, common_path, source, list_of_paths)`**: Appends tuples \\(`(common_path, file, source)`\\) to our master list of paths.  \n",
    "4. **`delete_folder(path)`**: Removes a specified folder and all contents within it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_path_list(num_files):\n",
    "    common_ranked_path = 'D:\\\\ranked\\\\ranked-anonymized-1-116248\\\\ranked-anonymized'\n",
    "    common_public_path = 'C:\\\\Users\\\\jaspa\\\\Grant ML\\\\Slippi_Public_Dataset_v3'\n",
    "    mango_path = 'D:\\\\Mango'\n",
    "\n",
    "    # Get all the files in the  directories\n",
    "    ranked_files = [file for file in os.listdir(common_ranked_path) if file.endswith('.slp')] \n",
    "    public_files = [file for file in os.listdir(common_public_path) if file.endswith('.slp')] \n",
    "    mango_files = [] # Has sub folders so must treat differently\n",
    "\n",
    "    # Walk through the directory tree of mango_path\n",
    "    for mango_path, dirnames, filenames in os.walk(mango_path):\n",
    "        for filename in filenames:\n",
    "            # Check if the file ends with .slp\n",
    "            if filename.endswith('.slp'):\n",
    "                # Construct the full file path and add it to the list\n",
    "                file_path =os.path.join(mango_path,filename)\n",
    "                mango_files.append(file_path)\n",
    "\n",
    "    common_mango_path, sub_mango_paths = split_paths(mango_files)\n",
    "\n",
    "    list_of_paths = []\n",
    "    # Add files to triples list\n",
    "    list_of_paths = add_to_triples(ranked_files[:num_files], common_ranked_path, 'ranked', list_of_paths)\n",
    "    # list_of_paths = add_to_triples(public_files[:num_files], common_public_path,'public', list_of_paths)\n",
    "    # list_of_paths = add_to_triples(sub_mango_paths[:num_files], common_mango_path, 'mango', list_of_paths)\n",
    "\n",
    "    # print(ranked_files[:2])\n",
    "    # print(public_files[:2])\n",
    "    # print(mango_files[:2])\n",
    "    return list_of_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Extract Data from SLP Files\n",
    "\n",
    "We parse several components from the `slippi.Game` object—namely `game.start`, `game.metadata`, `game.frames`, and `game.end`.\n",
    "\n",
    "1. **`game.start`**: Contains information about the starting state of the match (teams on/off, chosen characters, costumes, initial stocks, etc.).  \n",
    "2. **`game.metadata`**: Contains console-related data, netplay codes, and match date/time (may be missing for the *ranked* dataset).  \n",
    "3. **`game.frames`**: Stores per-frame events (positions, inputs, states, damage, etc.).  \n",
    "4. **`game.end`**: Info about how the game ended, who won (if determinable), or if it was inconclusive.\n",
    "\n",
    "### Extract Simple Data\n",
    "\n",
    "The functions below fetch replay-level metadata:\n",
    "\n",
    "- **`get_event_start_data(game, occupied_ports)`**: Captures the match’s basic starting info, character selections, and additional flags like `is_teams` and `is_pal`.  \n",
    "- **`get_event_end_data(game, occupied_ports)`**: Attempts to identify the match’s result (winner, end method, etc.).  \n",
    "- **`get_metadata(game, occupied_ports)`**: Extracts console data and player netplay details (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the overall metadata from the start of the game\n",
    "def get_event_start_data(game,occupied_ports):\n",
    "    ################\n",
    "    # Start Metadata\n",
    "    ################\n",
    "    start_metadata_dict = {\n",
    "        'is_teams': game.start.is_teams, #bool # We only take two player games\n",
    "    }\n",
    "    \n",
    "    for i, port in enumerate(occupied_ports):\n",
    "        start_metadata_dict.update({\n",
    "            f'player_{i+1}_port': port, #int\n",
    "            f'player_{i+1}_character_name': game.start.players[port].character.name, #string\n",
    "            # # f'player_{i+1}_character_value': game.start.players[port_1].character.value, #int\n",
    "            f'player_{i+1}_type_name': game.start.players[port].type.name, #string\n",
    "            # # f'player_{i+1}_type_value': game.start.players[port].type.value, #int\n",
    "            f'player_{i+1}_stocks': game.start.players[port].stocks, #int\n",
    "            f'player_{i+1}_costume': game.start.players[port].costume, #int\n",
    "        })\n",
    "        try:\n",
    "            start_metadata_dict.update({f'player_{i+1}_team_value': game.start.players[port].team.value if game.start.players[port].team else None}) #int\n",
    "        except: pass\n",
    "        start_metadata_dict.update({\n",
    "            f'player_{i+1}_ucf_shield_drop_name': game.start.players[port].ucf.shield_drop.name, #string\n",
    "            # f'player_{i+1}_ucf_shield_drop_value': game.start.players[port_1].ucf.shield_drop.value, #int\n",
    "            f'player_{i+1}_tag': game.start.players[port].tag, #string\n",
    "            f'player_{i+1}_display_name': game.start.players[port].display_name #string\n",
    "        })\n",
    "                # frame.ports[port].leader.post.state_age if frame.ports[port].leader.post.state_age else None,\n",
    "    start_metadata_dict.update({\n",
    "        # 'players': game.start.players, #tuple that contains players by port.\n",
    "        'random_seed': game.start.random_seed, #int #\n",
    "        'slippi': str(game.start.slippi.version), #slippi TUPLE NEEDS PROCESSING\n",
    "        'stage_name': game.start.stage.name, #string\n",
    "        # 'stage_value': game.start.stage.value, #int\n",
    "        'is_pal': game.start.is_pal, #bool\n",
    "        'is_frozen_ps': game.start.is_frozen_ps, #bool\n",
    "    })\n",
    "    return start_metadata_dict\n",
    "\n",
    "# Extract all the data from the slippi.event.end module\n",
    "# Determine winner if it is a conclusive two player game\n",
    "def get_event_end_data(game, occupied_ports):\n",
    "    end_data_dict = {\n",
    "        'end_method_name' : game.end.method.name, #string\n",
    "        # 'end_method_value' : game.end.method.value, #int\n",
    "        'lras_initiator' : game.end.lras_initiator #int\n",
    "    }\n",
    "    \n",
    "    end_method = game.end.method.name\n",
    "    if game.start.is_teams == False and len(occupied_ports) == 2:\n",
    "        if end_method == 'CONCLUSIVE' or end_method == 'TIME ' or end_method == 'GAME':\n",
    "            last_frame = game.frames[-1]\n",
    "            player_1_port = occupied_ports[0]\n",
    "            player_2_port = occupied_ports[1]\n",
    "            if last_frame.ports[player_1_port].leader.post.stocks > last_frame.ports[player_2_port].leader.post.stocks:\n",
    "                end_data_dict.update({\n",
    "                    'conclusive': True,\n",
    "                    'winning_player': int(1),\n",
    "                    'player_1_win': True,\n",
    "                    'player_2_win': False\n",
    "                })\n",
    "            elif last_frame.ports[player_1_port].leader.post.stocks < last_frame.ports[player_2_port].leader.post.stocks:\n",
    "                end_data_dict.update({\n",
    "                    'conclusive': True,\n",
    "                    'winning_player': int(2),\n",
    "                    'player_1_win': False,\n",
    "                    'player_2_win': True\n",
    "                })\n",
    "            elif last_frame.ports[player_1_port].leader.post.stocks == last_frame.ports[player_2_port].leader.post.stocks:\n",
    "                if last_frame.ports[player_1_port].leader.post.damage < last_frame.ports[player_2_port].leader.post.damage:\n",
    "                    end_data_dict.update({\n",
    "                        'conclusive': True,\n",
    "                        'winning_player': int(1),\n",
    "                        'player_1_win': True,\n",
    "                        'player_2_win': False\n",
    "                    })\n",
    "                elif last_frame.ports[player_1_port].leader.post.damage > last_frame.ports[player_2_port].leader.post.damage:\n",
    "                    end_data_dict.update({\n",
    "                        'conclusive': True,\n",
    "                        'winning_player': int(2),\n",
    "                        'player_1_win': False,\n",
    "                        'player_2_win': True\n",
    "                    })\n",
    "            else:\n",
    "                end_data_dict.update({\n",
    "                    'conclusive': False,\n",
    "                    'winning_player': int(0),\n",
    "                    # 'player_1_did_win': False,\n",
    "                    # 'player_2_did_win': False\n",
    "                    })\n",
    "        else:\n",
    "            end_data_dict.update({\n",
    "                'conclusive': False,\n",
    "                'winning_player': int(0),\n",
    "                # 'player_1_did_win': False,\n",
    "                # 'player_2_did_win': False\n",
    "                })\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "    return end_data_dict\n",
    "\n",
    "\n",
    "# Some games won't have metadata\n",
    "def get_metadata(game, occupied_ports):\n",
    "    metadata_dict = {\n",
    "       'date': game.metadata.date, #datetime\n",
    "       'duration': game.metadata.duration, #int\n",
    "       'platform': game.metadata.platform.name #string\n",
    "        }\n",
    "    \n",
    "    for i, port in enumerate(occupied_ports):\n",
    "        try:\n",
    "            metadata_dict.update({\n",
    "                # f'player_{i+1}_character': game.metadata.players[port].characters if game.metadata.players[port] else None, #string\n",
    "                f'player_{i+1}_netplay_code': game.metadata.players[port].netplay.code # String\n",
    "                })\n",
    "        except: pass\n",
    "        try:\n",
    "            metadata_dict.update({\n",
    "                f'player_{i+1}_netplay_name': game.metadata.players[port].netplay.name if game.metadata.players[port] else None, #string\n",
    "                })\n",
    "        except: pass\n",
    "    \n",
    "    try:   \n",
    "        metadata_dict.update({\n",
    "            'console_name': game.metadata.console_name\n",
    "            })\n",
    "    except: pass\n",
    "        \n",
    "    return metadata_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Data Functions\n",
    "We store each player’s frame data separately. Additionally, some columns are one-hot encoded for button presses.\n",
    "\n",
    "### Get Frames\n",
    "\n",
    "- **`get_frames_df(frames, port)`**  \n",
    "  For the specified `port`, returns two DataFrames:\n",
    "  1. A **full** DataFrame containing all pre- and post-frame data (positions, damage, stocks, action states, etc.).\n",
    "  2. An **input-specific** DataFrame capturing joystick positions, triggers, and button presses.\n",
    "\n",
    "- **`inputs_to_np(input_data_df)`**  \n",
    "  Produces a NumPy array suitable for model training. Typically, we skip the first 123 frames that occur before “GO!” is displayed. This function also highlights where further filtering or windowing for training might happen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract frame data\n",
    "def get_frames_df(frames, port):\n",
    "    \n",
    "    '''\n",
    "    This function extracts all the data we need from the slippi.event.Frame for one of the players.\n",
    "    It returns a DataFrame with all the frame data and another DataFrame with only the input data.\n",
    "    \n",
    "    This is a list of all the attributes that are available in the slippi.event.Frame class.\n",
    "    We write the code to extract all the data and comment out the ones we do not need.\n",
    "    # Pre\n",
    "    # self.state = state #: :py:class:`slippi.id.ActionState` | int: Character's action state\n",
    "    # self.position = position #: :py:class:`Position`: Character's position\n",
    "    # self.direction = direction #: :py:class:`Direction`: Direction the character is facing\n",
    "    # self.joystick = joystick #: :py:class:`Position`: Processed analog joystick position\n",
    "    # self.cstick = cstick #: :py:class:`Position`: Processed analog c-stick position\n",
    "    # self.triggers = triggers #: :py:class:`Triggers`: Trigger state\n",
    "    # self.buttons = buttons #: :py:class:`Buttons`: Button state\n",
    "    # self.random_seed = random_seed #: int: Random seed at this point\n",
    "    # self.raw_analog_x = raw_analog_x #: int | None: `added(1.2.0)` Raw x analog controller input (for UCF)\n",
    "    # self.damage = damage #: float | None: `added(1.4.0)` Current damage percent\n",
    "    \n",
    "    # Post\n",
    "    # character: sid.InGameCharacter #: In-game character (can only change for Zelda/Sheik). Check on first frame to determine if Zelda started as Sheik\n",
    "    # state: Union[sid.ActionState, int] #: Character's action state\n",
    "    # position: Position #: Character's position\n",
    "    # direction: Direction #: Direction the character is facing\n",
    "    # damage: float #: Current damage percent\n",
    "    # shield: float #: Current size of shield\n",
    "    # stocks: int #: Number of stocks remaining\n",
    "    # last_attack_landed: Union[Attack, int] #: Last attack that this character landed\n",
    "    # last_hit_by: Optional[int] #: Port of character that last hit this character\n",
    "    # combo_count: int #: Combo count as defined by the game\n",
    "    # state_age: Optional[float] #: `added(0.2.0)` Number of frames action state has been active. Can have a fractional component for certain actions\n",
    "    # flags: Optional[StateFlags] #: `added(2.0.0)` State flags\n",
    "    # hit_stun: Optional[float] #: `added(2.0.0)` Number of hitstun frames remaining\n",
    "    # airborne: Optional[bool] #: `added(2.0.0)` True if character is airborne\n",
    "    # ground: Optional[int] #: `added(2.0.0)` ID of ground character is standing on, if any\n",
    "    # jumps: Optional[int] #: `added(2.0.0)` Jumps remaining\n",
    "    # l_cancel: Optional[LCancel] #: `added(2.0.0)` L-cancel status, if any\n",
    "    '''\n",
    "    column_names = ['frame_index',\n",
    "                    # \n",
    "                    'pre_state', 'pre_position_x','pre_position_y','pre_direction',\n",
    "                    'pre_joystick_x','pre_joystick_y', 'pre_cstick_x', 'pre_cstick_y',\n",
    "                    'pre_triggers_logical','pre_triggers_physical_l','pre_triggers_physical_r',\n",
    "                    'pre_buttons_logical','pre_buttons_physical',\n",
    "                    'pre_random_seed','pre_raw_analog_x', 'pre_damage',\n",
    "                    # \n",
    "                    'post_character',\n",
    "                    'post_state','post_position_x','post_position_y','post_direction',\n",
    "                    'post_damage','post_sheild','post_stocks',\n",
    "                    'post_last_attack_landed','post_last_hit_by','post_combo_count',\n",
    "                    'post_state_age','post_flags','post_hit_stun',\n",
    "                    'post_airbourn', 'post_ground','post_jumps','post_l_cancel'\n",
    "                    ]\n",
    "    \n",
    "\n",
    "    frame_data = []\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame_data.append([\n",
    "            frame.index, # To remind us that it starts at -123\n",
    "            # Pre\n",
    "            # frame.ports[port].leader.pre.state.value,\n",
    "            frame.ports[port].leader.pre.state,\n",
    "            # 0,\n",
    "            frame.ports[port].leader.pre.position.x,\n",
    "            frame.ports[port].leader.pre.position.y,\n",
    "            frame.ports[port].leader.pre.direction.value,\n",
    "            # 0,\n",
    "            #\n",
    "            frame.ports[port].leader.pre.joystick.x,\n",
    "            frame.ports[port].leader.pre.joystick.y,\n",
    "            frame.ports[port].leader.pre.cstick.x,\n",
    "            frame.ports[port].leader.pre.cstick.y,\n",
    "            #\n",
    "            frame.ports[port].leader.pre.triggers.logical,\n",
    "            frame.ports[port].leader.pre.triggers.physical.l,\n",
    "            frame.ports[port].leader.pre.triggers.physical.r,\n",
    "            #\n",
    "            frame.ports[port].leader.pre.buttons.logical.value,\n",
    "            # 0,\n",
    "            frame.ports[port].leader.pre.buttons.physical.value,\n",
    "            # 0,\n",
    "            # \n",
    "            frame.ports[port].leader.pre.random_seed,\n",
    "            frame.ports[port].leader.pre.raw_analog_x,\n",
    "            frame.ports[port].leader.pre.damage,\n",
    "            # Post\n",
    "            frame.ports[port].leader.post.character.value,\n",
    "            #\n",
    "            # frame.ports[port].leader.post.state.value,\n",
    "            frame.ports[port].leader.post.state,\n",
    "            frame.ports[port].leader.post.position.x,\n",
    "            frame.ports[port].leader.post.position.y,\n",
    "            frame.ports[port].leader.post.direction.value,\n",
    "            #\n",
    "            frame.ports[port].leader.post.damage,\n",
    "            frame.ports[port].leader.post.shield,\n",
    "            frame.ports[port].leader.post.stocks,\n",
    "            #\n",
    "            frame.ports[port].leader.post.last_attack_landed.value if frame.ports[port].leader.post.last_attack_landed else None,\n",
    "            # 0,\n",
    "            frame.ports[port].leader.post.last_hit_by if frame.ports[port].leader.post.last_hit_by else None,\n",
    "            frame.ports[port].leader.post.combo_count,\n",
    "            #\n",
    "            frame.ports[port].leader.post.state_age if frame.ports[port].leader.post.state_age else None,\n",
    "            frame.ports[port].leader.post.flags.value if frame.ports[port].leader.post.flags else None,\n",
    "            # 0,\n",
    "            frame.ports[port].leader.post.hit_stun if frame.ports[port].leader.post.hit_stun else None,\n",
    "            #\n",
    "            frame.ports[port].leader.post.airborne if frame.ports[port].leader.post.airborne else None,\n",
    "            frame.ports[port].leader.post.ground if frame.ports[port].leader.post.ground else None,\n",
    "            frame.ports[port].leader.post.jumps if frame.ports[port].leader.post.jumps else None,\n",
    "            frame.ports[port].leader.post.l_cancel.value if frame.ports[port].leader.post.l_cancel else None\n",
    "            # 0\n",
    "            ])\n",
    "\n",
    "    # pre_list = ['frame_index',\n",
    "    #             'pre_state', 'pre_position_x','pre_position_y','pre_direction',\n",
    "    #             'pre_joystick_x','pre_joystick_y', 'pre_cstick_x', 'pre_cstick_y',\n",
    "    #             'pre_triggers_logical','pre_triggers_physical_l','pre_triggers_physical_r',\n",
    "    #             'pre_buttons_logical','pre_buttons_physical',\n",
    "    #             'pre_random_seed','pre_raw_analog_x', 'pre_damage']\n",
    "    # post_list = ['post_character',\n",
    "    #                 'post_state','post_position_x','post_position_y','post_direction',\n",
    "    #                 'post_damage','post_sheild','post_stocks',\n",
    "    #                 'post_last_attack_landed','post_last_hit_by','post_combo_count',\n",
    "    #                 'post_state_age','post_flags','post_hit_stun',\n",
    "    #                 'post_airbourn', 'post_ground','post_jumps','post_l_cancel']    \n",
    "    input_list = ['frame_index',\n",
    "                  'pre_joystick_x','pre_joystick_y', 'pre_cstick_x', 'pre_cstick_y',\n",
    "                  'pre_triggers_logical','pre_triggers_physical_l','pre_triggers_physical_r']\n",
    "    \n",
    "    all_data_df = pd.DataFrame(frame_data, columns=column_names)\n",
    "    # pre_data_df = all_data_df#[pre_list]\n",
    "    # post_data_df = all_data_df#[post_list]\n",
    "    input_data_df = all_data_df[input_list]\n",
    "    \n",
    "    button_states = all_data_df['pre_buttons_logical'].values\n",
    "\n",
    "    # For each button, apply the bitmask and add a column to the DataFrame\n",
    "    for button in slp.event.Buttons.Logical:\n",
    "        if button == slp.event.Buttons.Logical.NONE:  # Skip the NONE value to avoid an unnecessary column\n",
    "            continue\n",
    "        # Use broadcasting to apply the bitmask and generate the one-hot enco\n",
    "        # ed array\n",
    "        input_data_df[button.name] = (button_states & button.value) != 0\n",
    "    \n",
    "                  \n",
    "    return all_data_df, input_data_df \n",
    "\n",
    "def inputs_to_np(input_data_df):\n",
    "    '''\n",
    "    This function takes the input data DataFrame, extracts the columns we will use for training and converts it to a numpy array.\n",
    "    '''\n",
    "    inputs_array = np.empty((input_data_df.shape[0]-123, 9),dtype=np.float32)\n",
    "    inputs_X_Y_array =  np.empty((input_data_df.shape[0]-123, 2),dtype=np.float32)\n",
    "    inputs = input_data_df[['pre_joystick_x', 'pre_joystick_y', 'pre_cstick_x', 'pre_cstick_y','pre_triggers_logical','Z', 'A', 'B']]\n",
    "    inputs_X_Y = input_data_df[['X','Y']]\n",
    "    inputs_array[:,:8] = inputs.iloc[123:]\n",
    "    inputs_X_Y_array = inputs_X_Y.iloc[123:]\n",
    "    inputs_array[:,8] = np.max(inputs_X_Y_array,axis = 1)\n",
    "    return np.transpose(inputs_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Process Fuction </h2>\n",
    "A function to process a .slp file path. Save the frame data in character subfolder of public, ranked, and mango depending on which dataset the game comes from. We want each frame data file to be saved with a unique name that is relatively short (I think there is a package to generate unique codes and then we would put underscore followed by the port.)  We will save all the data, even if we don't think will use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Function\n",
    "The **`process_slp_file`** function takes in a single `.slp` file path, reads the replay data, and saves:\n",
    "\n",
    "- A *full* **Parquet** file of the frame data for each *player*, stored under:\n",
    "  ```\n",
    "  <common_frame_data_path>/<dataset>/<character>/*.parquet\n",
    "  ```\n",
    "- A *full* **Parquet** file of the input data for each *player* under:\n",
    "  ```\n",
    "  <common_inputs_df_path>/<dataset>/<character>/*.parquet\n",
    "  ```\n",
    "- A compressed **NumPy** file (`.npy.gz`) for each player’s reduced input data (if the game lasts beyond the initial 123 frames), under:\n",
    "  ```\n",
    "  <common_inputs_np_path>/<dataset>/<character>/*.npy.gz\n",
    "  ```\n",
    "\n",
    "It then populates a dictionary of metadata (`game_data_dict`) for each replay (like number of players, paths to the saved files, etc.) and appends that to a *global* list representing all replays processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Each iteration of this creates a row of the dataframe and appends it to the dataframe.\n",
    "def process_slp_file(path, common_frame_data_path,common_inputs_df_path, common_inputs_np_path, no_teams_1_players, no_teams_2_players, no_teams_3_players,\n",
    "                     no_teams_4_players, teams_1_players, teams_2_players, teams_3_players, teams_4_players, all_data_list, load_time_list,error_list):#, dataset_path, save_path, no_teams_2_players,no_teams_3_players,no_teams_4_players,teams_3_players,teams_4_players):\n",
    "    try:\n",
    "        slp_file_path = os.path.join(path[0], path[1])\n",
    "        # slp_file_name = slp_file.removesuffix('.slp')\n",
    "        \n",
    "        # game = slp.Game(file_path)\n",
    "        start_time = time.time()\n",
    "        game = slp.Game(slp_file_path)\n",
    "        load_time_list.append(time.time()- start_time)\n",
    "        # Get the frame data and save it\n",
    "        frames = game.frames\n",
    "        # Get occupied ports\n",
    "        occupied_ports = [i for i, port in enumerate(game.start.players) if port is not None]\n",
    "        game_data_dict = {'source':path[2], 'source_path_prefix': path[0], 'source_path_suffix': path[1], 'length': len(frames), 'num_players': len(occupied_ports)}\n",
    "        # Get the game's start data\n",
    "        game_data_dict.update(get_event_start_data(game,occupied_ports))\n",
    "        # Get the game's end data\n",
    "        if game.end is not None:\n",
    "            game_data_dict.update(get_event_end_data(game,occupied_ports))\n",
    "        # # Determine player placements\n",
    "        # game_data_dict.update(player_placement(game,occupied_ports))\n",
    "        \n",
    "        # # Get the game's metadata, hadle the case that there is no metadata\n",
    "        if game.metadata is not None:\n",
    "            game_data_dict.update(get_metadata(game, occupied_ports))\n",
    "        \n",
    "        game_data_dict.update({'all_data_df_common_path': common_frame_data_path, 'inputs_df_common_path': common_inputs_df_path, 'inputs_np_common_path': common_inputs_np_path})\n",
    "        for i, port in enumerate(occupied_ports):\n",
    "            all_data_df, input_data_df = get_frames_df(frames, port)\n",
    "            character = game.start.players[port].character.name\n",
    "            \n",
    "            subpath = os.path.join(path[2],character)\n",
    "            create_directories(os.path.join(common_frame_data_path,subpath))\n",
    "            all_data_df_sub_path = os.path.join(subpath, str(uuid.uuid4()) + '.parquet')\n",
    "            all_data_df_save_path = os.path.join(common_frame_data_path, all_data_df_sub_path)\n",
    "            all_data_df.to_parquet(all_data_df_save_path, engine = 'pyarrow', compression='gzip')\n",
    "            game_data_dict.update({f'player_{i+1}_all_data_df_sub_path': all_data_df_sub_path,f'player_{i+1}_all_data_df_save_path': all_data_df_save_path})\n",
    "            \n",
    "            create_directories(os.path.join(common_inputs_df_path,subpath))\n",
    "            input_df_sub_path = os.path.join(subpath, str(uuid.uuid4()) + '.parquet')\n",
    "            input_df_save_path = os.path.join(common_inputs_df_path, input_df_sub_path)\n",
    "            input_data_df.to_parquet(input_df_save_path, engine = 'pyarrow', compression='gzip')\n",
    "            game_data_dict.update({f'player_{i+1}_inputs_df_sub_path': input_df_sub_path,f'player_{i+1}_inputs_df_save_path': input_df_save_path})\n",
    "            \n",
    "            if len(frames) > 123:\n",
    "                input_data_np = inputs_to_np(input_data_df)\n",
    "                create_directories(os.path.join(common_inputs_np_path,subpath))\n",
    "                input_data_np_sub_path = os.path.join(subpath, f\"{uuid.uuid4()}.npy.gz\")\n",
    "                input_data_np_save_path = os.path.join(common_inputs_np_path, input_data_np_sub_path) \n",
    "                with gzip.open(input_data_np_save_path, 'wb') as f:\n",
    "                    np.save(f, input_data_np)\n",
    "                game_data_dict.update({f'player_{i+1}_inputs_np_sub_path': input_data_np_sub_path,f'player_{i+1}_inputs_np_save_path': input_data_np_save_path})\n",
    "                \n",
    "            \n",
    "        \n",
    "        if game.start.is_teams:\n",
    "            if len(occupied_ports) == 1: \n",
    "                teams_1_players.append(game_data_dict)\n",
    "            elif len(occupied_ports) == 2:\n",
    "                teams_2_players.append(game_data_dict)\n",
    "            elif len(occupied_ports) == 3:\n",
    "                teams_3_players.append(game_data_dict)\n",
    "            elif len(occupied_ports) == 4:\n",
    "                teams_4_players.append(game_data_dict)\n",
    "        else:\n",
    "            if len(occupied_ports) == 1: \n",
    "                no_teams_1_players.append(game_data_dict)\n",
    "            elif len(occupied_ports) == 2:\n",
    "                no_teams_2_players.append(game_data_dict)\n",
    "            elif len(occupied_ports) == 3:\n",
    "                no_teams_3_players.append(game_data_dict)\n",
    "            elif len(occupied_ports) == 4:\n",
    "                no_teams_4_players.append(game_data_dict)\n",
    "        \n",
    "        all_data_list.append(game_data_dict)\n",
    "        \n",
    "        return \n",
    "             \n",
    "    except Exception as e:\n",
    "        error_list.append(f\"Error processing {path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data\n",
    "Finally, we gather the list of `.slp` file paths and process them in **batches**. Note that the user might comment/uncomment the relevant lines in `make_path_list` to target `ranked`, `public`, or `mango` datasets separately.\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "The output directories follow this structure:\n",
    "\n",
    "```\n",
    "melee_project_data/\n",
    "    data/\n",
    "        # Parquet dataframes containing high-level replay metadata\n",
    "    frame_data/\n",
    "        mango/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "        public/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "        ranked/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "    input_df/\n",
    "        mango/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "        public/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "        ranked/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "    input_np/\n",
    "        mango/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "        public/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "        ranked/\n",
    "            BOWSER/\n",
    "            CAPTAIN_FALCON/\n",
    "            ...\n",
    "```\n",
    "\n",
    "### Batching\n",
    "We run multiple passes over the dataset (often in three slices) by changing how `tqdm` slices our `path_list`:\n",
    "\n",
    "```python\n",
    "tqdm.tqdm(path_list[:batch_size])\n",
    "tqdm.tqdm(path_list[batch_size:batch_size*2])\n",
    "tqdm.tqdm(path_list[batch_size*2:batch_size*3])\n",
    "```\n",
    "\n",
    "*(The code below shows how we set up our parallel processing and directory creation logic. The user can switch to the appropriate slice of data when needed.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of paths to process\n",
    "path_list = make_path_list(-1)\n",
    "\n",
    "# Create directory to save frame data dataframse\n",
    "common_frame_data_path = 'C:\\\\Users\\\\jaspa\\\\Grant ML\\\\frame_data'\n",
    "# Delete the directories if they exist\n",
    "# delete_folder(common_frame_data_path)\n",
    "# Create the directories\n",
    "create_directories(os.path.join(common_frame_data_path, 'ranked'))\n",
    "create_directories(os.path.join(common_frame_data_path, 'public'))\n",
    "create_directories(os.path.join(common_frame_data_path, 'mango'))\n",
    "\n",
    "# Create directory to save input data dataframse\n",
    "common_inputs_df_path = 'C:\\\\Users\\\\jaspa\\\\Grant ML\\\\input_df'\n",
    "# Delete the directories if they exist\n",
    "# delete_folder(common_inputs_df_path)\n",
    "# Create the directories\n",
    "create_directories(os.path.join(common_inputs_df_path, 'ranked'))\n",
    "create_directories(os.path.join(common_inputs_df_path, 'public'))\n",
    "create_directories(os.path.join(common_inputs_df_path, 'mango'))\n",
    "\n",
    "# Create directory to save input data numpy\n",
    "common_inputs_np_path = 'C:\\\\Users\\\\jaspa\\\\Grant ML\\\\input_np'\n",
    "# Delete the directories if they exist\n",
    "# delete_folder(common_inputs_np_path)\n",
    "# Create the directories\n",
    "create_directories(os.path.join(common_inputs_np_path, 'ranked'))\n",
    "create_directories(os.path.join(common_inputs_np_path, 'public'))\n",
    "create_directories(os.path.join(common_inputs_np_path, 'mango'))\n",
    "\n",
    "\n",
    "\n",
    "# # Print the first few triples to verify\n",
    "# for slp_path_double in slp_paths_doubles:  # Print more or less based on your need\n",
    "#     print(slp_path_double)\n",
    "\n",
    "# Create lists to send to process_slp_file\n",
    "manager = Manager()\n",
    "no_teams_1_players = manager.list()\n",
    "no_teams_2_players = manager.list()\n",
    "no_teams_3_players = manager.list()\n",
    "no_teams_4_players = manager.list()\n",
    "teams_1_players = manager.list()\n",
    "teams_2_players = manager.list()\n",
    "teams_3_players = manager.list()\n",
    "teams_4_players = manager.list()\n",
    "all_data_list = manager.list()\n",
    "load_time_list = manager.list()\n",
    "error_list = manager.list()\n",
    "\n",
    "# The average time to process a game varries between the data sets\n",
    "# Shuffle the order we process them in so that the estimated time to complete in the\n",
    "# progress bar is more accurate\n",
    "# random.shuffle(path_list)\n",
    "batch_size = len(path_list) // 3\n",
    "\n",
    "# Use joblib to parallelize processing of SLP files\n",
    "Parallel(n_jobs=20, verbose=1)(delayed(process_slp_file)(path,common_frame_data_path,common_inputs_df_path,common_inputs_np_path, no_teams_1_players, no_teams_2_players, no_teams_3_players, no_teams_4_players, teams_1_players, teams_2_players, teams_3_players, teams_4_players,all_data_list,load_time_list,error_list) for path in tqdm.tqdm(path_list[batch_size*2: ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(no_teams_1_players)\n",
    "print(no_teams_2_players)\n",
    "# print(no_teams_3_players)\n",
    "# print(no_teams_4_players)\n",
    "# print(teams_1_players)\n",
    "# print(teams_2_players)\n",
    "# print(teams_3_players)\n",
    "# print(teams_4_players)\n",
    "print(all_data_list)\n",
    "\n",
    "print(len(all_data_list))\n",
    "print(sum(load_time_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Steps: Saving the Consolidated Metadata\n",
    "All the per-replay metadata is combined into DataFrames (like `no_teams_2_players_df`), and finally, a single `all_game_data_df` is compiled and saved in Parquet format. We keep separate files or DataFrames for each scenario (e.g., `no_teams_1_players`, `teams_4_players`, etc.) if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_teams_1_players_df = pd.DataFrame(list(no_teams_1_players))\n",
    "no_teams_2_players_df = pd.DataFrame(list(no_teams_2_players))\n",
    "no_teams_3_players_df = pd.DataFrame(list(no_teams_3_players))\n",
    "no_teams_4_players_df = pd.DataFrame(list(no_teams_4_players))\n",
    "teams_1_players_df = pd.DataFrame(list(teams_1_players))\n",
    "teams_2_players_df = pd.DataFrame(list(teams_2_players))\n",
    "teams_3_players_df = pd.DataFrame(list(teams_3_players))\n",
    "teams_4_players_df = pd.DataFrame(list(teams_4_players))\n",
    "all_game_data_df = pd.DataFrame(list(all_data_list))\n",
    "print(all_game_data_df.shape)\n",
    "all_game_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataframe we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'C:\\\\Users\\\\jaspa\\\\Grant ML\\\\slp\\\\data'\n",
    "file_path = os.path.join(save_path, 'ranked_all_game_data_df_3' + '.parquet')\n",
    "all_game_data_df.to_parquet(file_path, engine = 'pyarrow', compression='gzip')\n",
    "\n",
    "# file_path = os.path.join(save_path, 'no_teams_1_players_df' + '.feather')\n",
    "# no_teams_1_players_df.to_feather(file_path)\n",
    "file_path = os.path.join(save_path, 'ranked_no_teams_2_players_df_3' + '.parquet')\n",
    "no_teams_2_players_df.to_parquet(file_path, engine = 'pyarrow', compression='gzip')\n",
    "# file_path = os.path.join(save_path, 'no_teams_3_players_df' + '.feather')\n",
    "# no_teams_3_players_df.to_feather(file_path)\n",
    "# file_path = os.path.join(save_path, 'no_teams_4_players_df' + '.feather')\n",
    "# no_teams_4_players_df.to_feather(file_path)\n",
    "# file_path = os.path.join(save_path, 'teams_1_players_df' + '.feather')\n",
    "# teams_1_players_df.to_feather(file_path)\n",
    "# file_path = os.path.join(save_path, 'teams_2_players_df' + '.feather')\n",
    "# teams_2_players_df.to_feather(file_path)\n",
    "# file_path = os.path.join(save_path, 'teams_3_players_df' + '.feather')\n",
    "# teams_3_players_df.to_feather(file_path)\n",
    "# file_path = os.path.join(save_path, 'teams_4_players_df' + '.feather')\n",
    "# teams_4_players_df.to_feather(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Logs\n",
    "If an error occurs while processing a file (e.g., corrupted `.slp` or unexpected data), it’s appended to `error_list`, which can be reviewed after the run to identify failed replays.\n",
    "\n",
    "\n",
    "> **Note**:  \n",
    "> Always confirm the final file sizes and the correctness of your extracted data. When dealing with large datasets, partial or batched runs and verifying the logs can save time and ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_game_data_df['source'].value_counts())\n",
    "\n",
    "print(len(error_list))\n",
    "# for error in error_list:\n",
    "#     print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_game_data_df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_slp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

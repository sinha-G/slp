{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will assist in creating and testing a general purpose function that will be in slp_package.slp_functions that will create a training dataset given some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "\n",
    "sys.path.append('../..')\n",
    "from slp.slp_package.slp_functions import create_merged_game_data_df, prepare_data_for_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_merged_game_data_df(['public','ranked','mango'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['player_1_character_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_general_filters(df, filters):\n",
    "#     \"\"\"\n",
    "#     Applies filters to the dataframe based on the provided dictionary of filters.\n",
    "\n",
    "#     :param df: The pandas DataFrame to filter.\n",
    "#     :param filters: Dictionary of column names and their desired values.\n",
    "#     :return: The filtered DataFrame.\n",
    "#     \"\"\"\n",
    "#     for feature, values in filters.items():\n",
    "#         if isinstance(values, list):\n",
    "#             df = df[df[feature].isin(values)]\n",
    "#         else:\n",
    "#             df = df[df[feature] == values]\n",
    "#     return df\n",
    "\n",
    "# def identify_and_label_players(df, player_features, opposing_player_features):\n",
    "#     \"\"\"\n",
    "#     Identifies which player (player_1 or player_2) matches the specified features and renames columns accordingly,\n",
    "#     also considering the opposing player features.\n",
    "\n",
    "#     :param df: The merged DataFrame.\n",
    "#     :param player_features: Dictionary of features for the player we are training on.\n",
    "#     :param opposing_player_features: Dictionary of features for the opposing player.\n",
    "#     :return: DataFrame with columns renamed for player and opposing player features, including input paths.\n",
    "#     \"\"\"\n",
    "#     # Reset the index of the DataFrame to ensure alignment\n",
    "#     df = df.reset_index(drop=True)\n",
    "\n",
    "#     # Initialize masks for player 1 and player 2\n",
    "#     player_1_mask = pd.Series([True] * len(df))\n",
    "#     player_2_mask = pd.Series([True] * len(df))\n",
    "\n",
    "#     # Update masks for player features\n",
    "#     for feature, values in player_features.items():\n",
    "#         player_1_mask &= df[f'player_1_{feature}'].isin(values) if isinstance(values, list) else df[f'player_1_{feature}'] == values\n",
    "#         player_2_mask &= df[f'player_2_{feature}'].isin(values) if isinstance(values, list) else df[f'player_2_{feature}'] == values\n",
    "\n",
    "#     # Update masks for opposing player features\n",
    "#     for feature, values in opposing_player_features.items():\n",
    "#         player_1_mask &= df[f'player_2_{feature}'].isin(values) if isinstance(values, list) else df[f'player_2_{feature}'] == values\n",
    "#         player_2_mask &= df[f'player_1_{feature}'].isin(values) if isinstance(values, list) else df[f'player_1_{feature}'] == values\n",
    "\n",
    "#     # Apply the masks to filter the DataFrame\n",
    "#     player_1_df = df[player_1_mask]\n",
    "#     player_2_df = df[player_2_mask]\n",
    "\n",
    "#     # Rename columns for player_1 and player_2 in their respective DataFrames\n",
    "#     player_1_df = player_1_df.rename(columns=lambda col: col.replace('player_1_', 'player_') if 'player_1_' in col else col.replace('player_2_', 'opposing_player_'))\n",
    "#     player_2_df = player_2_df.rename(columns=lambda col: col.replace('player_2_', 'player_') if 'player_2_' in col else col.replace('player_1_', 'opposing_player_'))\n",
    "\n",
    "#     # Concatenate the two DataFrames\n",
    "#     processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n",
    "\n",
    "#     return processed_df\n",
    "\n",
    "\n",
    "\n",
    "# def extract_label(df, label_info):\n",
    "#     \"\"\"\n",
    "#     Extracts the label column from the dataframe based on label_info and renames it to 'label'.\n",
    "\n",
    "#     :param df: The DataFrame to extract the label from.\n",
    "#     :param label_info: Dictionary specifying the source and feature name for the label.\n",
    "#     :return: DataFrame with the label column extracted and renamed to 'label'.\n",
    "#     \"\"\"\n",
    "#     label_source = label_info['source'][0]  # Assuming label_source is passed as a list\n",
    "#     label_feature = label_info['feature'][0]  # Assuming label_feature is passed as a list\n",
    "\n",
    "#     # Construct the full column name based on the source\n",
    "#     if label_source == 'player':\n",
    "#         label_column = f'player_{label_feature}'\n",
    "#     elif label_source == 'opposing_player':\n",
    "#         label_column = f'opposing_player_{label_feature}'\n",
    "#     else:\n",
    "#         label_column = label_feature\n",
    "\n",
    "#     # Check if the column exists after renaming\n",
    "#     if label_column not in df.columns:\n",
    "#         raise KeyError(f\"{label_column} not found in the DataFrame columns\")\n",
    "    \n",
    "#     df['label'] = df[label_column]\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# def prepare_data_for_training(source_data, general_features, player_features, opposing_player_features, label_info):\n",
    "#     \"\"\"\n",
    "#     Prepares data for training based on specified features and filters.\n",
    "\n",
    "#     :param source_data: List of sources to include in the data merge.\n",
    "#     :param general_features: Dictionary of general game features and their desired values.\n",
    "#     :param player_features: Dictionary of features for the player we are training on.\n",
    "#     :param opposing_player_features: Dictionary of features for the opposing player.\n",
    "#     :param label_info: Dictionary specifying the source and feature name for the label.\n",
    "#     :return: A pandas DataFrame with the prepared training data, containing only specified features and the label.\n",
    "#     \"\"\"\n",
    "#     # Merge data from specified sources\n",
    "#     merged_df = create_merged_game_data_df(source_data)\n",
    "\n",
    "#     # Apply filters to general game data\n",
    "#     merged_df = apply_general_filters(merged_df, general_features)\n",
    "\n",
    "#     # Identify and label player and opposing player features\n",
    "#     merged_df = identify_and_label_players(merged_df, player_features, opposing_player_features)\n",
    "\n",
    "#     # Extract and set the label for training\n",
    "#     merged_df = extract_label(merged_df, label_info)\n",
    "\n",
    "#     # Define the order of columns to be selected\n",
    "#     general_feature_columns = list(general_features.keys())\n",
    "#     player_feature_columns = [f'player_{feature}' for feature in player_features.keys()]\n",
    "#     opposing_player_feature_columns = [f'opposing_player_{feature}' for feature in opposing_player_features.keys()]\n",
    "#     input_path_column = ['player_inputs_np_save_path']\n",
    "#     label_column = ['label']\n",
    "\n",
    "#     # Combine all columns in the desired order\n",
    "#     final_columns = general_feature_columns + player_feature_columns + opposing_player_feature_columns + input_path_column + label_column\n",
    "\n",
    "#     # Select only the specified columns from the DataFrame\n",
    "#     final_df = merged_df[final_columns]\n",
    "\n",
    "#     return final_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "source_data = ['mango','ranked', 'public']\n",
    "general_features = {\n",
    "    # 'stage_name': ['FOUNTAIN_OF_DREAMS'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK', 'PEACH', 'JIGGLYPUFF']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}\n",
    "    \n",
    "processed_df = prepare_data_for_training(source_data, general_features, player_features, opposing_player_features, label_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_df.shape)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(processed_df['labels'].value_counts(), columns=['labels', 'count'])\n",
    "\n",
    "# Get the value counts of the 'labels' column\n",
    "label_counts = processed_df['labels'].value_counts()\n",
    "\n",
    "# Create a DataFrame from the value counts\n",
    "label_counts_df = pd.DataFrame(label_counts).reset_index()\n",
    "label_counts_df.columns = ['labels', 'count']\n",
    "\n",
    "label_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def segment_overlap_info(df, segment_length_power):\n",
    "#     unique_labels = df['labels'].unique()\n",
    "#     # Get the value counts of the 'labels' column\n",
    "#     label_counts = df['labels'].value_counts()\n",
    "\n",
    "#     # Create a DataFrame from the value counts\n",
    "#     info_df = pd.DataFrame(label_counts).reset_index()\n",
    "#     info_df.columns = ['labels', 'value_count']\n",
    "    \n",
    "#     segment_length = 2 ** segment_length_power\n",
    "    \n",
    "#     for i in range(segment_length_power):\n",
    "#         segment_shift = 2 ** (segment_length_power - i)\n",
    "#         info_df[f'shift_by_{segment_shift}'] = 0\n",
    "#         for label in unique_labels:\n",
    "#             game_lengths = df.loc[df['labels'] == label, 'length']\n",
    "#             num_segments = sum(game_lengths - segment_length) // segment_shift\n",
    "#             info_df.loc[df['labels'] == label, f'shift_by_{segment_shift}'] = num_segments\n",
    "#             # info_df[f'shift_by_{segment_shift}'] = num_segments\n",
    "    \n",
    "#     return info_df\n",
    "\n",
    "# segment_overlap_info(processed_df, 10)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_overlap(data_df, segments_per_label, segment_length_power):\n",
    "    segment_length = 2 ** segment_length_power\n",
    "    data_df.loc[:, 'segment_shift'] = 0\n",
    "\n",
    "    for label in data_df['labels'].unique():\n",
    "        label_indices = data_df['labels'] == label\n",
    "        game_lengths = data_df.loc[label_indices, 'length']\n",
    "        for i in range(segment_length_power):\n",
    "            segment_shift = 2 ** (segment_length_power - i)\n",
    "            num_segments = sum((game_lengths - segment_length) // segment_shift)\n",
    "            if num_segments > segments_per_label:\n",
    "                break\n",
    "        data_df.loc[label_indices, 'segment_shift'] = segment_shift\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "\n",
    "def split_games(data_df, unique_labels, test_ratio, val_ratio, val):\n",
    "    \"\"\"\n",
    "    Splits the data into training, testing, and optionally validation sets based on the number of segments per label.\n",
    "\n",
    "    :param data_df: DataFrame to split.\n",
    "    :param unique_labels: Unique labels in the dataset.\n",
    "    :param test_ratio: Proportion of the data to allocate to the test set.\n",
    "    :param val_ratio: Proportion of the data to allocate to the validation set.\n",
    "    :param val: Boolean indicating whether to create a validation set.\n",
    "    :return: Tuple of DataFrames (test_df, val_df, train_df).\n",
    "    \"\"\"\n",
    "    test_dfs, val_dfs, train_dfs = [], [], []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_df = data_df.loc[data_df['labels'] == label].sample(frac=1).reset_index(drop=True)\n",
    "        num_segments_cumsum = label_df['num_segments'].cumsum()\n",
    "\n",
    "        test_limit = int(num_segments_cumsum.iloc[-1] * test_ratio)\n",
    "        val_limit = test_limit + int(num_segments_cumsum.iloc[-1] * val_ratio)\n",
    "\n",
    "        test_idx = num_segments_cumsum[num_segments_cumsum <= test_limit].last_valid_index() or 0\n",
    "        val_idx = num_segments_cumsum[num_segments_cumsum <= val_limit].last_valid_index() or test_idx\n",
    "\n",
    "        test_dfs.append(label_df.iloc[:test_idx + 1])\n",
    "        if val:\n",
    "            val_dfs.append(label_df.iloc[test_idx + 1:val_idx + 1])\n",
    "        train_dfs.append(label_df.iloc[val_idx + 1:])\n",
    "\n",
    "    test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "    val_df = pd.concat(val_dfs, ignore_index=True) if val else pd.DataFrame()\n",
    "    train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "    \n",
    "    return_columns = ['player_inputs_np_save_path', 'labels', 'length', 'segment_shift', 'num_segments']\n",
    "    # return\n",
    "    # return test_df, val_df, train_df\n",
    "    return test_df[return_columns], val_df[return_columns], train_df[return_columns]\n",
    "\n",
    "\n",
    "\n",
    "def expand_df_vectorized(df):\n",
    "    \"\"\"\n",
    "    Expand the dataframe so that each segment is one row\n",
    "    \"\"\"\n",
    "    # Calculate the repeat count for each row based on 'num_segments'\n",
    "    repeats = df['num_segments'].values\n",
    "    \n",
    "    # Repeat each index according to its corresponding 'num_segments' value\n",
    "    index_repeated = np.repeat(df.index, repeats)\n",
    "    \n",
    "    # Create a new DataFrame by repeating rows\n",
    "    df_repeated = df.loc[index_repeated].reset_index(drop=True)\n",
    "    \n",
    "    # Create a 'segment_index' column that counts up for each group of repeated rows\n",
    "    segment_indices = np.concatenate([np.arange(n,dtype = np.int16) for n in repeats])\n",
    "    \n",
    "    # Assign 'segment_index' to the repeated DataFrame\n",
    "    df_repeated['segment_index'] = segment_indices\n",
    "    \n",
    "    # Optionally, drop the 'num_segments' column if it's no longer needed\n",
    "    df_repeated = df_repeated.drop(columns=['num_segments'])\n",
    "    \n",
    "    return df_repeated\n",
    "\n",
    "def split_games_df(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True):\n",
    "    \"\"\"\n",
    "    data_df should be the output of prepare_data_for_training()\n",
    "    This function splits data_df into two or three three dataframes depending on if we want a validation set or not.\n",
    "    \"\"\"\n",
    "    unique_labels = data_df['labels'].unique()\n",
    "    data_df['length'] -= 123    \n",
    "\n",
    "    segment_length = 2 ** segment_length_power\n",
    "    # Filter out games that are shorter than the segment length\n",
    "    data_df = data_df[data_df['length'] > segment_length]\n",
    "    \n",
    "    # Creating a new DataFrame with these unique labels\n",
    "    \n",
    "    data_df = get_segment_overlap(data_df,  segments_per_label, segment_length_power)\n",
    "\n",
    "    # Calculate 'num_segments' based on the merged information\n",
    "    data_df['num_segments'] = np.ceil((data_df['length'] - segment_length) / data_df['segment_shift']).astype(int)\n",
    "    \n",
    "    test_df, val_df, train_df = split_games(data_df, unique_labels,  test_ratio, val_ratio, val)\n",
    "\n",
    "    return test_df, val_df, train_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_rows_per_label(df, proportion, segments_per_label, encoder):\n",
    "    rows_per_label = int(segments_per_label * proportion)\n",
    "    sampled_df = pd.DataFrame()\n",
    "    \n",
    "    for label in df['labels']:\n",
    "        label_df = df[df['labels'] == label]\n",
    "        sampled_rows = label_df.sample(n=min(rows_per_label, len(label_df)), random_state=1)\n",
    "        sampled_df = pd.concat([sampled_df, sampled_rows], ignore_index=True)\n",
    "    \n",
    "    # Add the 'labels' column using the fitted encoder\n",
    "    # sampled_df['labels'] = encoder.transform(sampled_df['labels'])\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "def sample_rows_per_label(df, proportion, segments_per_label, encoder):\n",
    "    \"\"\"\n",
    "    Sample a proportion of rows from each label category up to a maximum number per label.\n",
    "\n",
    "    :param df: DataFrame containing the data to be sampled.\n",
    "    :param proportion: The proportion of rows to sample from each label category.\n",
    "    :param segments_per_label: Maximum number of segments to sample per label.\n",
    "    :param encoder: An encoder object to transform the label column.\n",
    "    :return: A DataFrame with the sampled rows and transformed labels.\n",
    "    \"\"\"\n",
    "    rows_per_label = int(segments_per_label * proportion)\n",
    "    sampled_dfs = []\n",
    "\n",
    "    # Loop through unique labels to avoid redundant sampling\n",
    "    for label in df['labels'].unique():\n",
    "        label_df = df[df['labels'] == label]\n",
    "        sample_size = min(rows_per_label, len(label_df))\n",
    "        sampled_rows = label_df.sample(n=sample_size, random_state=1)\n",
    "        sampled_dfs.append(sampled_rows)\n",
    "\n",
    "    # Concatenate all sampled rows outside of the loop\n",
    "    sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "    # Transform the 'labels' column using the fitted encoder\n",
    "    sampled_df['labels'] = encoder.transform(sampled_df['labels'])\n",
    "\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "def create_training_segments_df(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True):\n",
    "    \"\"\"\n",
    "    data_df should be the output of prepare_data_for_training()\n",
    "    This function creates three dataframes with a path and segment index that the data loader can use.\n",
    "    \"\"\"\n",
    "    data_df = data_df.copy()\n",
    "    test_df, val_df, train_df = split_games_df(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True)\n",
    "    \n",
    "    expanded_test_df = expand_df_vectorized(test_df)\n",
    "    expanded_val_df = expand_df_vectorized(val_df) if val else  pd.DataFrame()\n",
    "    expanded_train_df = expand_df_vectorized(train_df)\n",
    "    \n",
    "    unique_labels = data_df['labels'].unique()\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(unique_labels)\n",
    "\n",
    "    train_ratio = 1 - test_ratio - val_ratio * val\n",
    "    sampled_test_df = sample_rows_per_label(expanded_test_df, test_ratio, segments_per_label, encoder)\n",
    "    sampled_val_df = sample_rows_per_label(expanded_val_df, val_ratio, segments_per_label, encoder) if val else pd.DataFrame\n",
    "    sampled_train_df = sample_rows_per_label(expanded_train_df, train_ratio, segments_per_label, encoder)\n",
    "\n",
    "\n",
    "    # Shuffle the dataframes so that the labels are mixed up. If we implement a progress bar that keeps track of the\n",
    "    # running loss, we will get a more stable estimate throught the validation process if the labels are shuffled.\n",
    "    sampled_test_df = sampled_test_df.sample(frac=1).reset_index(drop=True)\n",
    "    sampled_val_df = sampled_val_df.sample(frac=1).reset_index(drop=True)\n",
    "    # sampled_train_df = sampled_train_df.sample(frac=1).reset_index(drop=True) \n",
    "    \n",
    "    # return test_df, val_df, train_df\n",
    "    # return expanded_test_df, expanded_val_df, expanded_train_df\n",
    "    \n",
    "    return sampled_test_df, sampled_val_df, sampled_train_df\n",
    "    \n",
    "def create_training_segments_np(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True):\n",
    "    data_df = data_df.copy()\n",
    "    test_df, val_df, train_df = split_games_df(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True)\n",
    "    \n",
    "    expanded_test_df = expand_df_vectorized(test_df)\n",
    "    expanded_val_df = expand_df_vectorized(val_df) if val else  pd.DataFrame()\n",
    "    expanded_train_df = expand_df_vectorized(train_df)\n",
    "    \n",
    "    unique_labels = data_df['labels'].unique()\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(unique_labels)\n",
    "\n",
    "    train_ratio = 1 - test_ratio - val_ratio * val\n",
    "    \n",
    "    sampled_test_df = sample_rows_per_label(expanded_test_df, test_ratio, segments_per_label, encoder)\n",
    "    sampled_val_df = sample_rows_per_label(expanded_val_df, val_ratio, segments_per_label, encoder) if val else pd.DataFrame\n",
    "    sampled_train_df = sample_rows_per_label(expanded_train_df, train_ratio, segments_per_label, encoder)\n",
    "    \n",
    "    def load_segment(path, segment_index, segment_shift, segment_length):\n",
    "        \"\"\"\n",
    "        Load a specific segment from a file.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the file.\n",
    "            segment_index (int): Index of the segment within the file.\n",
    "            segment_shift (int): Number of frames to shift for each segment.\n",
    "            segment_length (int): Length of the segment in frames.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The loaded segment.\n",
    "        \"\"\"\n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            game = np.load(f)\n",
    "        start_frame = segment_shift * segment_index\n",
    "        end_frame = start_frame + segment_length\n",
    "        segment = game[:, start_frame:end_frame]\n",
    "        return segment\n",
    "    \n",
    "    # manager = Manager()\n",
    "    # shared_list = manager.list()\n",
    "    \n",
    "    def process_dataframe_parallel(df, segment_length_power):\n",
    "        segment_length = 2 ** segment_length_power\n",
    "        tasks = [(row['player_inputs_np_save_path'], index, row['segment_shift'], segment_length) \n",
    "                for index, row in df.iterrows()]\n",
    "\n",
    "        # Using Parallel and delayed to load segments in parallel\n",
    "        results = Parallel(n_jobs=20, verbose=1)(\n",
    "            delayed(load_segment)(*task) for task in tqdm.tqdm(tasks, desc='Loading segments')\n",
    "        )\n",
    "\n",
    "        return np.array(results)\n",
    "\n",
    "    process_dataframe_parallel(sampled_test_df, segment_length_power)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "    \n",
    "# # create_segment_df(processed_df, 1000000, 10)   \n",
    "# A_df, B_df, C_df = split_games_df(processed_df, 1000000, 10)\n",
    "# print(A_df.describe())\n",
    "# print(A_df.shape)\n",
    "# print(B_df.shape)\n",
    "# print(C_df.shape)\n",
    "# print(C_df['labels'].unique())\n",
    "# C_df.head()\n",
    "\n",
    "# create_segment_df(processed_df, 1000000, 10)   \n",
    "# A_df, B_df, C_df = create_training_segments_df(processed_df, 1000000, 10)\n",
    "# print(A_df.describe())\n",
    "# print(A_df.shape)\n",
    "# print(B_df.shape)\n",
    "# print(C_df.shape)\n",
    "# print(C_df['labels'].unique())\n",
    "# A_df.head()\n",
    "\n",
    "create_training_segments_np(processed_df, 1000000, 10)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(B_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

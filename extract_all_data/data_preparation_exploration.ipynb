{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will assist in creating and testing a general purpose function that will be in slp_package.slp_functions that will create a training dataset given some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "\n",
    "sys.path.append('..')\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "from slp_package.slp_functions import create_merged_game_data_df, prepare_data_for_training\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    8805\n",
       "0.0    8083\n",
       "3.0    1723\n",
       "2.0    1299\n",
       "Name: lras_initiator, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_merged_game_data_df(['public','ranked','mango'])\n",
    "df.columns\n",
    "df['lras_initiator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/extract_all_data/../slp_package/slp_functions.py:107: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "source_data = ['ranked', 'public', 'mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK', 'PEACH', 'JIGGLYPUFF']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['general'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['length']\n",
    "}\n",
    "    \n",
    "df = prepare_data_for_training(source_data, general_features, player_features, opposing_player_features, label_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    409260.000000\n",
       "mean          2.759175\n",
       "std           0.801034\n",
       "min           0.011944\n",
       "25%           2.221111\n",
       "50%           2.690556\n",
       "75%           3.215833\n",
       "max           8.000278\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['length'] / 60 ** 2).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008620689655172415\n",
      "[0.00862071 0.00862062 0.00862074 0.00862071 0.00862065 0.00862071\n",
      " 0.00862065 0.00862071 0.00862074 0.00862065 0.00862071 0.00862065\n",
      " 0.00862071 0.00862074 0.00862062 0.00862074 0.00862065 0.00862074\n",
      " 0.00862068 0.00862068 0.00862068 0.00862068 0.00862071 0.00862071\n",
      " 0.00862068 0.00862068 0.00862068 0.00862071 0.00862068 0.00862068\n",
      " 0.00862068 0.00862068 0.00862074 0.00862068 0.00862068 0.00862068\n",
      " 0.00862068 0.00862074 0.00862068 0.00862068 0.00862068 0.00862068\n",
      " 0.00862071 0.00862071 0.00862068 0.00862068 0.00862068 0.00862068\n",
      " 0.00862071 0.00862068 0.00862068 0.00862071 0.00862068 0.00862071\n",
      " 0.00862068 0.00862068 0.00862071 0.00862068 0.00862068 0.00862068\n",
      " 0.00862068 0.00862068 0.00862074 0.00862068 0.00862068 0.00862068\n",
      " 0.00862068 0.00862074 0.00862068 0.00862068 0.00862068 0.00862068\n",
      " 0.00862074 0.00862068 0.00862068 0.00862068 0.00862068 0.00862074\n",
      " 0.00862062 0.00862074 0.00862062 0.00862074 0.00862074 0.00862062\n",
      " 0.00862074 0.00862062 0.00862074 0.00862068 0.00862068 0.00862074\n",
      " 0.00862062 0.00862074 0.00862068 0.00862068 0.00862068 0.00862068\n",
      " 0.00862074 0.00862068 0.00862068 0.00862068 0.00862068 0.00862074\n",
      " 0.00862068 0.00862068 0.00862068 0.00862068 0.00862074 0.00862068\n",
      " 0.00862068 0.00862068 0.00862068 0.00862074 0.00862062 0.00862074]\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "path = 'mango/FALCO/e55c959c-533d-4b0e-82b1-4e79decd25e2.npy.gz'\n",
    "\n",
    "with gzip.open('/workspace/melee_project_data/input_np/' + path) as f:\n",
    "    inputs = np.load(f)\n",
    "\n",
    "uniques = np.unique(inputs[0])\n",
    "# print(uniques)\n",
    "# print(uniques.shape)\n",
    "# print(np.diff(uniques))\n",
    "\n",
    "analog_transformed = np.copy(inputs[0])\n",
    "analog_transformed[analog_transformed > 0] -= 0.2875 + 0.0125\n",
    "analog_transformed[analog_transformed < 0] += 0.2875 - 0.0125\n",
    "analog_transformed *= .5 / .725\n",
    "analog_transformed += .5\n",
    "\n",
    "print(.0125 * (.5 / .725))\n",
    "uniques = np.unique(analog_transformed)\n",
    "# print(uniques.shape)\n",
    "# print(uniques)\n",
    "print(np.diff(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4240  5606  5754 ...  9957 10538 10615]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 0.0000e+00, 1.2000e+01, 2.9000e+01,\n",
       "        2.0000e+01, 2.5000e+01, 2.7000e+01, 4.0000e+01, 8.9000e+01,\n",
       "        2.2400e+02, 7.2400e+02, 1.9240e+03, 5.1590e+03, 1.4787e+04,\n",
       "        3.2985e+04, 5.0923e+04, 4.3479e+04, 1.9648e+04, 5.2380e+03,\n",
       "        9.8800e+02, 1.6200e+02]),\n",
       " array([ 5.42626509,  5.71962027,  6.01297545,  6.30633064,  6.59968582,\n",
       "         6.893041  ,  7.18639618,  7.47975136,  7.77310654,  8.06646173,\n",
       "         8.35981691,  8.65317209,  8.94652727,  9.23988245,  9.53323763,\n",
       "         9.82659282, 10.119948  , 10.41330318, 10.70665836, 11.00001354,\n",
       "        11.29336872, 11.5867239 , 11.88007909, 12.17343427, 12.46678945,\n",
       "        12.76014463, 13.05349981, 13.34685499, 13.64021018, 13.93356536,\n",
       "        14.22692054, 14.52027572, 14.8136309 ]),\n",
       " <BarContainer object of 32 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = df['length'].to_numpy()\n",
    "np.sort(lengths)\n",
    "lengths = lengths[:np.argmax(lengths > 60 ** 2 * 8)]\n",
    "print(lengths)\n",
    "log_lenghts = np.log2(lengths + .00001)\n",
    "bins = 32\n",
    "counts, bins = np.histogram(log_lenghts, bins = bins)\n",
    "plt.hist(bins[:-1], bins, weights=counts, log = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.hist(x: 'ArrayLike | Sequence[ArrayLike]', bins: 'int | Sequence[float] | str | None' = None, range: 'tuple[float, float] | None' = None, density: 'bool' = False, weights: 'ArrayLike | None' = None, cumulative: 'bool | float' = False, bottom: 'ArrayLike | float | None' = None, histtype: \"Literal['bar', 'barstacked', 'step', 'stepfilled']\" = 'bar', align: \"Literal['left', 'mid', 'right']\" = 'mid', orientation: \"Literal['vertical', 'horizontal']\" = 'vertical', rwidth: 'float | None' = None, log: 'bool' = False, color: 'ColorType | Sequence[ColorType] | None' = None, label: 'str | Sequence[str] | None' = None, stacked: 'bool' = False, *, data=None, **kwargs) -> 'tuple[np.ndarray | list[np.ndarray], np.ndarray, BarContainer | Polygon | list[BarContainer | Polygon]]'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['stage_name', 'num_players', 'conclusive', 'player_inputs_np_save_path',\n",
      "       'length', 'labels'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage_name                    object\n",
      "num_players                    int64\n",
      "conclusive                    object\n",
      "player_inputs_np_save_path    object\n",
      "length                         int64\n",
      "labels                         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BATTLEFIELD           131748\n",
       "YOSHIS_STORY           89426\n",
       "POKEMON_STADIUM        79372\n",
       "FOUNTAIN_OF_DREAMS     59058\n",
       "FINAL_DESTINATION      49656\n",
       "Name: stage_name, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stage_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_general_filters(df, filters):\n",
    "#     \"\"\"\n",
    "#     Applies filters to the dataframe based on the provided dictionary of filters.\n",
    "\n",
    "#     :param df: The pandas DataFrame to filter.\n",
    "#     :param filters: Dictionary of column names and their desired values.\n",
    "#     :return: The filtered DataFrame.\n",
    "#     \"\"\"\n",
    "#     for feature, values in filters.items():\n",
    "#         if isinstance(values, list):\n",
    "#             df = df[df[feature].isin(values)]\n",
    "#         else:\n",
    "#             df = df[df[feature] == values]\n",
    "#     return df\n",
    "\n",
    "# def identify_and_label_players(df, player_features, opposing_player_features):\n",
    "#     \"\"\"\n",
    "#     Identifies which player (player_1 or player_2) matches the specified features and renames columns accordingly,\n",
    "#     also considering the opposing player features.\n",
    "\n",
    "#     :param df: The merged DataFrame.\n",
    "#     :param player_features: Dictionary of features for the player we are training on.\n",
    "#     :param opposing_player_features: Dictionary of features for the opposing player.\n",
    "#     :return: DataFrame with columns renamed for player and opposing player features, including input paths.\n",
    "#     \"\"\"\n",
    "#     # Reset the index of the DataFrame to ensure alignment\n",
    "#     df = df.reset_index(drop=True)\n",
    "\n",
    "#     # Initialize masks for player 1 and player 2\n",
    "#     player_1_mask = pd.Series([True] * len(df))\n",
    "#     player_2_mask = pd.Series([True] * len(df))\n",
    "\n",
    "#     # Update masks for player features\n",
    "#     for feature, values in player_features.items():\n",
    "#         player_1_mask &= df[f'player_1_{feature}'].isin(values) if isinstance(values, list) else df[f'player_1_{feature}'] == values\n",
    "#         player_2_mask &= df[f'player_2_{feature}'].isin(values) if isinstance(values, list) else df[f'player_2_{feature}'] == values\n",
    "\n",
    "#     # Update masks for opposing player features\n",
    "#     for feature, values in opposing_player_features.items():\n",
    "#         player_1_mask &= df[f'player_2_{feature}'].isin(values) if isinstance(values, list) else df[f'player_2_{feature}'] == values\n",
    "#         player_2_mask &= df[f'player_1_{feature}'].isin(values) if isinstance(values, list) else df[f'player_1_{feature}'] == values\n",
    "\n",
    "#     # Apply the masks to filter the DataFrame\n",
    "#     player_1_df = df[player_1_mask]\n",
    "#     player_2_df = df[player_2_mask]\n",
    "\n",
    "#     # Rename columns for player_1 and player_2 in their respective DataFrames\n",
    "#     player_1_df = player_1_df.rename(columns=lambda col: col.replace('player_1_', 'player_') if 'player_1_' in col else col.replace('player_2_', 'opposing_player_'))\n",
    "#     player_2_df = player_2_df.rename(columns=lambda col: col.replace('player_2_', 'player_') if 'player_2_' in col else col.replace('player_1_', 'opposing_player_'))\n",
    "\n",
    "#     # Concatenate the two DataFrames\n",
    "#     processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n",
    "\n",
    "#     return processed_df\n",
    "\n",
    "\n",
    "\n",
    "# def extract_label(df, label_info):\n",
    "#     \"\"\"\n",
    "#     Extracts the label column from the dataframe based on label_info and renames it to 'label'.\n",
    "\n",
    "#     :param df: The DataFrame to extract the label from.\n",
    "#     :param label_info: Dictionary specifying the source and feature name for the label.\n",
    "#     :return: DataFrame with the label column extracted and renamed to 'label'.\n",
    "#     \"\"\"\n",
    "#     label_source = label_info['source'][0]  # Assuming label_source is passed as a list\n",
    "#     label_feature = label_info['feature'][0]  # Assuming label_feature is passed as a list\n",
    "\n",
    "#     # Construct the full column name based on the source\n",
    "#     if label_source == 'player':\n",
    "#         label_column = f'player_{label_feature}'\n",
    "#     elif label_source == 'opposing_player':\n",
    "#         label_column = f'opposing_player_{label_feature}'\n",
    "#     else:\n",
    "#         label_column = label_feature\n",
    "\n",
    "#     # Check if the column exists after renaming\n",
    "#     if label_column not in df.columns:\n",
    "#         raise KeyError(f\"{label_column} not found in the DataFrame columns\")\n",
    "    \n",
    "#     df['label'] = df[label_column]\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# def prepare_data_for_training(source_data, general_features, player_features, opposing_player_features, label_info):\n",
    "#     \"\"\"\n",
    "#     Prepares data for training based on specified features and filters.\n",
    "\n",
    "#     :param source_data: List of sources to include in the data merge.\n",
    "#     :param general_features: Dictionary of general game features and their desired values.\n",
    "#     :param player_features: Dictionary of features for the player we are training on.\n",
    "#     :param opposing_player_features: Dictionary of features for the opposing player.\n",
    "#     :param label_info: Dictionary specifying the source and feature name for the label.\n",
    "#     :return: A pandas DataFrame with the prepared training data, containing only specified features and the label.\n",
    "#     \"\"\"\n",
    "#     # Merge data from specified sources\n",
    "#     merged_df = create_merged_game_data_df(source_data)\n",
    "\n",
    "#     # Apply filters to general game data\n",
    "#     merged_df = apply_general_filters(merged_df, general_features)\n",
    "\n",
    "#     # Identify and label player and opposing player features\n",
    "#     merged_df = identify_and_label_players(merged_df, player_features, opposing_player_features)\n",
    "\n",
    "#     # Extract and set the label for training\n",
    "#     merged_df = extract_label(merged_df, label_info)\n",
    "\n",
    "#     # Define the order of columns to be selected\n",
    "#     general_feature_columns = list(general_features.keys())\n",
    "#     player_feature_columns = [f'player_{feature}' for feature in player_features.keys()]\n",
    "#     opposing_player_feature_columns = [f'opposing_player_{feature}' for feature in opposing_player_features.keys()]\n",
    "#     input_path_column = ['player_inputs_np_save_path']\n",
    "#     label_column = ['label']\n",
    "\n",
    "#     # Combine all columns in the desired order\n",
    "#     final_columns = general_feature_columns + player_feature_columns + opposing_player_feature_columns + input_path_column + label_column\n",
    "\n",
    "#     # Select only the specified columns from the DataFrame\n",
    "#     final_df = merged_df[final_columns]\n",
    "\n",
    "#     return final_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/extract_all_data/../slp_package/slp_functions.py:107: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "source_data = ['ranked', 'public']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK', 'PEACH', 'JIGGLYPUFF']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}\n",
    "    \n",
    "processed_df = prepare_data_for_training(source_data, general_features, player_features, opposing_player_features, label_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298856, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage_name</th>\n",
       "      <th>num_players</th>\n",
       "      <th>conclusive</th>\n",
       "      <th>player_character_name</th>\n",
       "      <th>player_inputs_np_save_path</th>\n",
       "      <th>length</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FOX</td>\n",
       "      <td>C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\FOX\\ae...</td>\n",
       "      <td>5969</td>\n",
       "      <td>FOX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BATTLEFIELD</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>CAPTAIN_FALCON</td>\n",
       "      <td>C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\CAPTAI...</td>\n",
       "      <td>6073</td>\n",
       "      <td>CAPTAIN_FALCON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FINAL_DESTINATION</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>CAPTAIN_FALCON</td>\n",
       "      <td>C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\CAPTAI...</td>\n",
       "      <td>6989</td>\n",
       "      <td>CAPTAIN_FALCON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BATTLEFIELD</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>SHEIK</td>\n",
       "      <td>C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\SHEIK\\...</td>\n",
       "      <td>6806</td>\n",
       "      <td>SHEIK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOUNTAIN_OF_DREAMS</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>FOX</td>\n",
       "      <td>C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\FOX\\21...</td>\n",
       "      <td>7102</td>\n",
       "      <td>FOX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stage_name  num_players conclusive player_character_name  \\\n",
       "0   FINAL_DESTINATION            2       True                   FOX   \n",
       "1         BATTLEFIELD            2       True        CAPTAIN_FALCON   \n",
       "2   FINAL_DESTINATION            2       True        CAPTAIN_FALCON   \n",
       "3         BATTLEFIELD            2       True                 SHEIK   \n",
       "4  FOUNTAIN_OF_DREAMS            2       True                   FOX   \n",
       "\n",
       "                          player_inputs_np_save_path  length          labels  \n",
       "0  C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\FOX\\ae...    5969             FOX  \n",
       "1  C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\CAPTAI...    6073  CAPTAIN_FALCON  \n",
       "2  C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\CAPTAI...    6989  CAPTAIN_FALCON  \n",
       "3  C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\SHEIK\\...    6806           SHEIK  \n",
       "4  C:\\Users\\jaspa\\Grant ML\\input_np\\ranked\\FOX\\21...    7102             FOX  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_df.shape)\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOX</td>\n",
       "      <td>88233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FALCO</td>\n",
       "      <td>70363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARTH</td>\n",
       "      <td>46190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAPTAIN_FALCON</td>\n",
       "      <td>34869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SHEIK</td>\n",
       "      <td>25971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PEACH</td>\n",
       "      <td>17250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JIGGLYPUFF</td>\n",
       "      <td>15980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           labels  count\n",
       "0             FOX  88233\n",
       "1           FALCO  70363\n",
       "2           MARTH  46190\n",
       "3  CAPTAIN_FALCON  34869\n",
       "4           SHEIK  25971\n",
       "5           PEACH  17250\n",
       "6      JIGGLYPUFF  15980"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(processed_df['labels'].value_counts(), columns=['labels', 'count'])\n",
    "\n",
    "# Get the value counts of the 'labels' column\n",
    "label_counts = processed_df['labels'].value_counts()\n",
    "\n",
    "# Create a DataFrame from the value counts\n",
    "label_counts_df = pd.DataFrame(label_counts).reset_index()\n",
    "label_counts_df.columns = ['labels', 'count']\n",
    "\n",
    "label_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_overlap_info(df, segment_length_power):\n",
    "    unique_labels = df['labels'].unique()\n",
    "    # Get the value counts of the 'labels' column\n",
    "    label_counts = df['labels'].value_counts()\n",
    "\n",
    "    # Create a DataFrame from the value counts\n",
    "    info_df = pd.DataFrame(label_counts).reset_index()\n",
    "    info_df.columns = ['labels', 'value_count']\n",
    "    \n",
    "    segment_length = 2 ** segment_length_power\n",
    "    \n",
    "    for i in range(segment_length_power):\n",
    "        segment_shift = 2 ** (segment_length_power - i)\n",
    "        info_df[f'shift_by_{segment_shift}'] = 0\n",
    "        for label in unique_labels:\n",
    "            game_lengths = df.loc[df['labels'] == label, 'length']\n",
    "            num_segments = sum(game_lengths - segment_length) // segment_shift\n",
    "            info_df.loc[df['labels'] == label, f'shift_by_{segment_shift}'] = num_segments\n",
    "            # info_df[f'shift_by_{segment_shift}'] = num_segments\n",
    "    \n",
    "    return info_df\n",
    "\n",
    "segment_overlap_info(processed_df, 10)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_overlap(data_df, segments_per_label, segment_length_power):\n",
    "    segment_length = 2 ** segment_length_power\n",
    "    data_df.loc[:, 'segment_shift'] = 0\n",
    "\n",
    "    for label in data_df['labels'].unique():\n",
    "        label_indices = data_df['labels'] == label\n",
    "        game_lengths = data_df.loc[label_indices, 'length']\n",
    "        for i in range(segment_length_power):\n",
    "            segment_shift = 2 ** (segment_length_power - i)\n",
    "            num_segments = sum((game_lengths - segment_length) // segment_shift)\n",
    "            if num_segments > segments_per_label:\n",
    "                break\n",
    "        data_df.loc[label_indices, 'segment_shift'] = segment_shift\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "\n",
    "def split_games(data_df, unique_labels, test_ratio, val_ratio, val):\n",
    "    \"\"\"\n",
    "    Splits the data into training, testing, and optionally validation sets based on the number of segments per label.\n",
    "\n",
    "    :param data_df: DataFrame to split.\n",
    "    :param unique_labels: Unique labels in the dataset.\n",
    "    :param test_ratio: Proportion of the data to allocate to the test set.\n",
    "    :param val_ratio: Proportion of the data to allocate to the validation set.\n",
    "    :param val: Boolean indicating whether to create a validation set.\n",
    "    :return: Tuple of DataFrames (test_df, val_df, train_df).\n",
    "    \"\"\"\n",
    "    test_dfs, val_dfs, train_dfs = [], [], []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_df = data_df.loc[data_df['labels'] == label].sample(frac=1).reset_index(drop=True)\n",
    "        num_segments_cumsum = label_df['num_segments'].cumsum()\n",
    "\n",
    "        test_limit = int(num_segments_cumsum.iloc[-1] * test_ratio)\n",
    "        val_limit = test_limit + int(num_segments_cumsum.iloc[-1] * val_ratio)\n",
    "\n",
    "        test_idx = num_segments_cumsum[num_segments_cumsum <= test_limit].last_valid_index() or 0\n",
    "        val_idx = num_segments_cumsum[num_segments_cumsum <= val_limit].last_valid_index() or test_idx\n",
    "\n",
    "        test_dfs.append(label_df.iloc[:test_idx + 1])\n",
    "        if val:\n",
    "            val_dfs.append(label_df.iloc[test_idx + 1:val_idx + 1])\n",
    "        train_dfs.append(label_df.iloc[val_idx + 1:])\n",
    "\n",
    "    test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "    val_df = pd.concat(val_dfs, ignore_index=True) if val else pd.DataFrame()\n",
    "    train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "    \n",
    "    return_columns = ['player_inputs_np_save_path', 'labels', 'length', 'segment_shift', 'num_segments']\n",
    "    # return\n",
    "    # return test_df, val_df, train_df\n",
    "    return test_df[return_columns], val_df[return_columns], train_df[return_columns]\n",
    "\n",
    "\n",
    "\n",
    "def expand_df_vectorized(df):\n",
    "    \"\"\"\n",
    "    Expand the dataframe so that each segment is one row\n",
    "    \"\"\"\n",
    "    # Calculate the repeat count for each row based on 'num_segments'\n",
    "    repeats = df['num_segments'].values\n",
    "    \n",
    "    # Repeat each index according to its corresponding 'num_segments' value\n",
    "    index_repeated = np.repeat(df.index, repeats)\n",
    "    \n",
    "    # Create a new DataFrame by repeating rows\n",
    "    df_repeated = df.loc[index_repeated].reset_index(drop=True)\n",
    "    \n",
    "    # Create a 'segment_index' column that counts up for each group of repeated rows\n",
    "    segment_indices = np.concatenate([np.arange(n,dtype = np.int16) for n in repeats])\n",
    "    \n",
    "    # Assign 'segment_index' to the repeated DataFrame\n",
    "    df_repeated['segment_index'] = segment_indices\n",
    "    \n",
    "    # Optionally, drop the 'num_segments' column if it's no longer needed\n",
    "    df_repeated = df_repeated.drop(columns=['num_segments'])\n",
    "    \n",
    "    return df_repeated\n",
    "\n",
    "def split_games_df(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True):\n",
    "    \"\"\"\n",
    "    data_df should be the output of prepare_data_for_training()\n",
    "    This function splits data_df into two or three three dataframes depending on if we want a validation set or not.\n",
    "    \"\"\"\n",
    "    unique_labels = data_df['labels'].unique()\n",
    "    data_df['length'] -= 123    \n",
    "\n",
    "    segment_length = 2 ** segment_length_power\n",
    "    # Filter out games that are shorter than the segment length\n",
    "    data_df = data_df[data_df['length'] > segment_length]\n",
    "    \n",
    "    # Creating a new DataFrame with these unique labels\n",
    "    \n",
    "    data_df = get_segment_overlap(data_df,  segments_per_label, segment_length_power)\n",
    "\n",
    "    # Calculate 'num_segments' based on the merged information\n",
    "    data_df['num_segments'] = np.ceil((data_df['length'] - segment_length) / data_df['segment_shift']).astype(int)\n",
    "    \n",
    "    test_df, val_df, train_df = split_games(data_df, unique_labels,  test_ratio, val_ratio, val)\n",
    "\n",
    "    return test_df, val_df, train_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_rows_per_label(df, proportion, segments_per_label, encoder):\n",
    "    rows_per_label = int(segments_per_label * proportion)\n",
    "    sampled_df = pd.DataFrame()\n",
    "    \n",
    "    for label in df['labels']:\n",
    "        label_df = df[df['labels'] == label]\n",
    "        sampled_rows = label_df.sample(n=min(rows_per_label, len(label_df)), random_state=1)\n",
    "        sampled_df = pd.concat([sampled_df, sampled_rows], ignore_index=True)\n",
    "    \n",
    "    # Add the 'labels' column using the fitted encoder\n",
    "    # sampled_df['labels'] = encoder.transform(sampled_df['labels'])\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "def sample_rows_per_label(df, proportion, segments_per_label, encoder):\n",
    "    \"\"\"\n",
    "    Sample a proportion of rows from each label category up to a maximum number per label.\n",
    "\n",
    "    :param df: DataFrame containing the data to be sampled.\n",
    "    :param proportion: The proportion of rows to sample from each label category.\n",
    "    :param segments_per_label: Maximum number of segments to sample per label.\n",
    "    :param encoder: An encoder object to transform the label column.\n",
    "    :return: A DataFrame with the sampled rows and transformed labels.\n",
    "    \"\"\"\n",
    "    rows_per_label = int(segments_per_label * proportion)\n",
    "    sampled_dfs = []\n",
    "\n",
    "    # Loop through unique labels to avoid redundant sampling\n",
    "    for label in df['labels'].unique():\n",
    "        label_df = df[df['labels'] == label]\n",
    "        sample_size = min(rows_per_label, len(label_df))\n",
    "        sampled_rows = label_df.sample(n=sample_size, random_state=1)\n",
    "        sampled_dfs.append(sampled_rows)\n",
    "\n",
    "    # Concatenate all sampled rows outside of the loop\n",
    "    sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "    # Transform the 'labels' column using the fitted encoder\n",
    "    sampled_df['labels'] = encoder.transform(sampled_df['labels'])\n",
    "\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "def create_training_segments_df(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True):\n",
    "    \"\"\"\n",
    "    data_df should be the output of prepare_data_for_training()\n",
    "    This function creates three dataframes with a path and segment index that the data loader can use.\n",
    "    \"\"\"\n",
    "    data_df = data_df.copy()\n",
    "    test_df, val_df, train_df = split_games_df(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True)\n",
    "    \n",
    "    expanded_test_df = expand_df_vectorized(test_df)\n",
    "    expanded_val_df = expand_df_vectorized(val_df) if val else  pd.DataFrame()\n",
    "    expanded_train_df = expand_df_vectorized(train_df)\n",
    "    \n",
    "    unique_labels = data_df['labels'].unique()\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(unique_labels)\n",
    "\n",
    "    train_ratio = 1 - test_ratio - val_ratio * val\n",
    "    sampled_test_df = sample_rows_per_label(expanded_test_df, test_ratio, segments_per_label, encoder)\n",
    "    sampled_val_df = sample_rows_per_label(expanded_val_df, val_ratio, segments_per_label, encoder) if val else pd.DataFrame\n",
    "    sampled_train_df = sample_rows_per_label(expanded_train_df, train_ratio, segments_per_label, encoder)\n",
    "\n",
    "\n",
    "    # Shuffle the dataframes so that the labels are mixed up. If we implement a progress bar that keeps track of the\n",
    "    # running loss, we will get a more stable estimate throught the validation process if the labels are shuffled.\n",
    "    sampled_test_df = sampled_test_df.sample(frac=1).reset_index(drop=True)\n",
    "    sampled_val_df = sampled_val_df.sample(frac=1).reset_index(drop=True)\n",
    "    # sampled_train_df = sampled_train_df.sample(frac=1).reset_index(drop=True) \n",
    "    \n",
    "    # return test_df, val_df, train_df\n",
    "    # return expanded_test_df, expanded_val_df, expanded_train_df\n",
    "    \n",
    "    return sampled_test_df, sampled_val_df, sampled_train_df\n",
    "    \n",
    "def create_training_segments_np(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True):\n",
    "    data_df = data_df.copy()\n",
    "    test_df, val_df, train_df = split_games_df(data_df, segments_per_label, segment_length_power = 10, test_ratio = .15, val_ratio = .15, val = True)\n",
    "    \n",
    "    expanded_test_df = expand_df_vectorized(test_df)\n",
    "    expanded_val_df = expand_df_vectorized(val_df) if val else  pd.DataFrame()\n",
    "    expanded_train_df = expand_df_vectorized(train_df)\n",
    "    \n",
    "    unique_labels = data_df['labels'].unique()\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(unique_labels)\n",
    "\n",
    "    train_ratio = 1 - test_ratio - val_ratio * val\n",
    "    \n",
    "    sampled_test_df = sample_rows_per_label(expanded_test_df, test_ratio, segments_per_label, encoder)\n",
    "    sampled_val_df = sample_rows_per_label(expanded_val_df, val_ratio, segments_per_label, encoder) if val else pd.DataFrame\n",
    "    sampled_train_df = sample_rows_per_label(expanded_train_df, train_ratio, segments_per_label, encoder)\n",
    "    \n",
    "    def load_segment(path, segment_index, segment_shift, segment_length):\n",
    "        \"\"\"\n",
    "        Load a specific segment from a file.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to the file.\n",
    "            segment_index (int): Index of the segment within the file.\n",
    "            segment_shift (int): Number of frames to shift for each segment.\n",
    "            segment_length (int): Length of the segment in frames.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The loaded segment.\n",
    "        \"\"\"\n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            game = np.load(f)\n",
    "        start_frame = segment_shift * segment_index\n",
    "        end_frame = start_frame + segment_length\n",
    "        segment = game[:, start_frame:end_frame]\n",
    "        return segment\n",
    "    \n",
    "    # manager = Manager()\n",
    "    # shared_list = manager.list()\n",
    "    \n",
    "    def process_dataframe_parallel(df, segment_length_power):\n",
    "        segment_length = 2 ** segment_length_power\n",
    "        tasks = [(row['player_inputs_np_save_path'], index, row['segment_shift'], segment_length) \n",
    "                for index, row in df.iterrows()]\n",
    "\n",
    "        # Using Parallel and delayed to load segments in parallel\n",
    "        results = Parallel(n_jobs=20, verbose=1)(\n",
    "            delayed(load_segment)(*task) for task in tqdm.tqdm(tasks, desc='Loading segments')\n",
    "        )\n",
    "\n",
    "        return np.array(results)\n",
    "\n",
    "    process_dataframe_parallel(sampled_test_df, segment_length_power)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "    \n",
    "# create_segment_df(processed_df, 1000000, 10)   \n",
    "A_df, B_df, C_df = split_games_df(processed_df, 1000000, 10)\n",
    "print(A_df.describe())\n",
    "print(A_df.shape)\n",
    "print(B_df.shape)\n",
    "print(C_df.shape)\n",
    "print(C_df['labels'].unique())\n",
    "C_df.head()\n",
    "\n",
    "# create_training_segments_df(processed_df, 1000000, 10)   \n",
    "# A_df, B_df, C_df = create_training_segments_df(processed_df, 1000000, 10)\n",
    "# print(A_df.describe())\n",
    "# print(A_df.shape)\n",
    "# print(B_df.shape)\n",
    "# print(C_df.shape)\n",
    "# print(C_df['labels'].unique())\n",
    "# A_df.head()\n",
    "\n",
    "# create_training_segments_np(processed_df, 1000000, 10)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_segments_per_game(df, segment_length_power, num_segments_per_label):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param df: column 'length' should have 123 less already\n",
    "    \"\"\"\n",
    "    segment_length = 2 ** segment_length_power\n",
    "    unique_labels = df['labels'].unique()\n",
    "    \n",
    "    # Eventually, I do want to modify the original dataframe.\n",
    "    df = df.copy()\n",
    "    df = df[df['length'] > segment_length]\n",
    "    df['float_num_segments'] = 0.\n",
    "    # df['int_num_segments'] = 0 \n",
    "    # df['frac_num_segments'] = 0.\n",
    "    \n",
    "    label_info_list = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = df['labels'] == label\n",
    "        adjusted_game_length = df.loc[label_indices, 'length'] - segment_length # A segment must start its own length before the end of the game.\n",
    "        game_length_sum = adjusted_game_length.sum()\n",
    "        shift_estimate = game_length_sum / num_segments_per_label # Idea: Put all the frame data in a (9,-) array, evenly space out segments.\n",
    "        # The number of segments we take from each game will be roughly round(adjusted_game_length / shift_estimate).\n",
    "        # df['int_num_segments'].sum() - num_segments_per_label =~ (number of games with this label) / 2\n",
    "        # If we simply took round(adjusted_game_length / shift_estimate) segments per game, we would be off by a little bit.\n",
    "        # Idea is to sort the games with this label decreasing by df['frac_num_segments'] and take one extra segment from the first\n",
    "        # however many games needed to get the right number of segments.\n",
    "        # Because we want exactly the right number of segments per label in each of test, train, and possibly val, we will calculate the\n",
    "        # number of segments we take from each game after we split the games into those sets.\n",
    "        df.loc[label_indices, 'float_num_segments'] = (adjusted_game_length / shift_estimate)\n",
    "        # df.loc[label_indices, 'int_num_segments'] = adjusted_game_length // shift_estimate \n",
    "        # df.loc[label_indices, 'frac_num_segments'] = adjusted_game_length / shift_estimate - df.loc[label_indices, 'int_num_segments']\n",
    "        \n",
    "        label_info_list.append([label, df.loc[label_indices].shape[0], round(shift_estimate)])\n",
    "    \n",
    "    label_info = pd.DataFrame(label_info_list, columns=['Label', 'Count', 'Estimated Shift'])\n",
    "    \n",
    "    # Sort the label_info DataFrame by 'Count' in descending order\n",
    "    label_info = label_info.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df, label_info\n",
    "\n",
    "def divide_games(df, num_segments_per_label, test_ratio=.15, val_ratio=.15, val=True):\n",
    "    \"\"\"\n",
    "    Splits the games into training, testing, and optionally validation sets based on the approximate number of segments per game we calculated peviously\n",
    "    \n",
    "    :param df: should be the output of number_of_segments_per_game.\n",
    "    \"\"\"\n",
    "    unique_labels = df['labels'].unique()\n",
    "    df = df.copy()\n",
    "    \n",
    "    test_dfs, val_dfs, train_dfs = [], [], []\n",
    "    \n",
    "    num_segments_per_label_test = round(num_segments_per_label * test_ratio)\n",
    "    num_segments_per_label_val = round(num_segments_per_label * val_ratio) * val\n",
    "    num_segments_per_label_train = num_segments_per_label - num_segments_per_label_test - num_segments_per_label_val\n",
    "    \n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_df = df.loc[df['labels'] == label].sample(frac=1).reset_index(drop=True) # Shuffle the games for a randomized split\n",
    "        num_segments_cumsum = label_df['float_num_segments'].cumsum() # The total sum will be num_segments_per_label\n",
    "\n",
    "        \n",
    "        test_idx = num_segments_cumsum[num_segments_cumsum <= num_segments_per_label_test].last_valid_index() \n",
    "        test_label_df = label_df.iloc[:test_idx] \n",
    "        # test_label_df['float_num_segments'].round().sum() won't equal test_limit exactly \n",
    "        # Rather than round 'float_num_segments' we conditionally round 'float_num_segments' so that we get the number of segments that we want in this set.\n",
    "        test_label_df['num_segments'] = test_label_df['float_num_segments'].astype(int) # Start by taking the floor of 'float_num_segments'\n",
    "        test_label_df['frac_part'] = test_label_df['float_num_segments'] - test_label_df['num_segments'] # Compute the fractional part\n",
    "        test_label_df.sort_values(by = ['frac_part'], ascending = False, inplace = True) # sort by fractional part\n",
    "        sum = test_label_df['num_segments'].sum() # Number of segments we get by always rounding down\n",
    "        test_label_df['num_segments'][:num_segments_per_label_test - sum] += 1 # round 'float_num_segments' up or down conditionally based on the number of segments that we want in the set\n",
    "        test_dfs.append(test_label_df)\n",
    "        \n",
    "\n",
    "        val_idx = num_segments_cumsum[num_segments_cumsum <= num_segments_per_label_val + num_segments_per_label_test].last_valid_index()\n",
    "        val_label_df = label_df.iloc[test_idx:val_idx] \n",
    "        # test_label_df['float_num_segments'].round().sum() won't equal test_limit exactly \n",
    "        # Rather than round 'float_num_segments' we conditionally round 'float_num_segments' so that we get the number of segments that we want in this set.\n",
    "        val_label_df['num_segments'] = val_label_df['float_num_segments'].astype(int) # Start by taking the floor of 'float_num_segments'\n",
    "        val_label_df['frac_part'] = val_label_df['float_num_segments'] - val_label_df['num_segments'] # Compute the fractional part\n",
    "        val_label_df.sort_values(by = ['frac_part'], ascending = False, inplace = True) # sort by fractional part\n",
    "        sum = val_label_df['num_segments'].sum() # Number of segments we get by always rounding down\n",
    "        val_label_df['num_segments'][:num_segments_per_label_val - sum] += 1 # round 'float_num_segments' up or down conditionally based on the number of segments that we want in the set\n",
    "        val_dfs.append(val_label_df)\n",
    "        \n",
    "\n",
    "        \n",
    "        train_label_df = label_df.iloc[val_idx:] \n",
    "        # test_label_df['float_num_segments'].round().sum() won't equal test_limit exactly \n",
    "        # Rather than round 'float_num_segments' we conditionally round 'float_num_segments' so that we get the number of segments that we want in this set.\n",
    "        train_label_df['num_segments'] = train_label_df['float_num_segments'].astype(int) # Start by taking the floor of 'float_num_segments'\n",
    "        train_label_df['frac_part'] = train_label_df['float_num_segments'] - train_label_df['num_segments'] # Compute the fractional part\n",
    "        train_label_df.sort_values(by = ['frac_part'], ascending = False, inplace = True) # sort by fractional part\n",
    "        sum = train_label_df['num_segments'].sum() # Number of segments we get by always rounding down\n",
    "        train_label_df['num_segments'][:num_segments_per_label_train - sum] += 1 # round 'float_num_segments' up or down conditionally based on the number of segments that we want in the set\n",
    "        train_dfs.append(train_label_df)\n",
    "        \n",
    "    return_columns = ['player_inputs_np_save_path', 'labels', 'length', 'num_segments']\n",
    "    \n",
    "    test_df = pd.concat(test_dfs, ignore_index=True)[return_columns]\n",
    "    val_df = pd.concat(val_dfs, ignore_index=True)[return_columns] if val else pd.DataFrame(columns = return_columns)\n",
    "    # val_df = pd.concat(val_dfs, ignore_index=True)[return_columns] if val else False\n",
    "    train_df = pd.concat(train_dfs, ignore_index=True)[return_columns]\n",
    "    \n",
    "    # return\n",
    "    # return test_df, val_df, train_df\n",
    "    return test_df, val_df, train_df\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "segments_per_game_df, label_info_df = number_of_segments_per_game(processed_df, 10, 100000)\n",
    "# print(segments_per_game_df.groupby('labels')['float_num_segments'].sum())\n",
    "# print(A_df.describe())\n",
    "# print(A_df.head())\n",
    "\n",
    "test_df, val_df, train_df = divide_games(segments_per_game_df, 100000, test_ratio=.15, val_ratio=.15, val=True)\n",
    "# Sum 'num_segments' for each 'label'\n",
    "print(test_df.groupby('labels')['num_segments'].sum())\n",
    "# Sum 'num_segments' for each 'label'\n",
    "if not val_df.empty:\n",
    "    print(val_df.groupby('labels')['num_segments'].sum())\n",
    "# Sum 'num_segments' for each 'label'\n",
    "print(train_df.groupby('labels')['num_segments'].sum())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def divide_games(df, num_segments_per_label, test_ratio=0.15, val_ratio=0.15, val=True):\n",
    "    \"\"\"\n",
    "    Splits the games into training, testing, and optionally validation sets based on the approximate number of segments per game.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The output of number_of_segments_per_game containing game data with 'labels' and 'float_num_segments'.\n",
    "    num_segments_per_label (int): Total number of segments desired per label.\n",
    "    test_ratio (float): The proportion of data to be used for the test set.\n",
    "    val_ratio (float): The proportion of data to be used for the validation set.\n",
    "    val (bool): Whether to create a validation set.\n",
    "    \n",
    "    Returns:\n",
    "    test_df (DataFrame): Data for testing.\n",
    "    val_df (DataFrame): Data for validation (if val is True, otherwise an empty DataFrame).\n",
    "    train_df (DataFrame): Data for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the dataframe to avoid modifying the original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize empty lists to store split dataframes\n",
    "    test_dfs, val_dfs, train_dfs = [], [], []\n",
    "\n",
    "    # Calculate the number of segments for each split based on the provided ratios\n",
    "    num_segments_test = round(num_segments_per_label * test_ratio)\n",
    "    num_segments_val = round(num_segments_per_label * val_ratio) * val\n",
    "    num_segments_train = num_segments_per_label - num_segments_test - num_segments_val\n",
    "    \n",
    "    # Process each label separately\n",
    "    for label in df['labels'].unique():\n",
    "        # Filter the dataframe for the current label and shuffle\n",
    "        label_df = df[df['labels'] == label].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Calculate cumulative sum to find the cutoff points for splitting\n",
    "        num_segments_cumsum = label_df['float_num_segments'].cumsum()\n",
    "\n",
    "        # Determine the index to split test and train datasets\n",
    "        test_idx = num_segments_cumsum[num_segments_cumsum <= num_segments_test].last_valid_index() or 0\n",
    "        val_idx = num_segments_cumsum[num_segments_cumsum <= num_segments_test + num_segments_val].last_valid_index() or test_idx\n",
    "\n",
    "        # Split the data based on calculated indices\n",
    "        test_label_df = label_df.iloc[:test_idx + 1].copy()\n",
    "        val_label_df = label_df.iloc[test_idx + 1:val_idx + 1].copy() if val else pd.DataFrame()\n",
    "        train_label_df = label_df.iloc[val_idx + 1:].copy()\n",
    "\n",
    "        # Calculate the actual number of segments to extract for each set\n",
    "        # This process adjusts the 'num_segments' by distributing the rounding errors across the segments\n",
    "        # to ensure that the total number of segments remains as close as possible to the desired count\n",
    "        for split_df, num_segments_split in zip(\n",
    "            [test_label_df, val_label_df, train_label_df],\n",
    "            [num_segments_test, num_segments_val, num_segments_train]\n",
    "        ):\n",
    "            # Start with floor values of 'float_num_segments' and calculate the residual fractional part\n",
    "            split_df['num_segments'] = split_df['float_num_segments'].astype(int)\n",
    "            split_df['frac_part'] = split_df['float_num_segments'] - split_df['num_segments']\n",
    "            split_df.sort_values(by='frac_part', ascending=False, inplace=True)\n",
    "\n",
    "            # Distribute rounding residuals to match the total segment count precisely\n",
    "            residual_count = num_segments_split - split_df['num_segments'].sum()\n",
    "            split_df.iloc[:residual_count, split_df.columns.get_loc('num_segments')] += 1\n",
    "\n",
    "        # Append the processed dataframes to their respective lists\n",
    "        test_dfs.append(test_label_df)\n",
    "        val_dfs.append(val_label_df)\n",
    "        train_dfs.append(train_label_df)\n",
    "\n",
    "    # Concatenate all the dataframes in each list to create the final splits\n",
    "    return_columns = ['player_inputs_np_save_path', 'labels', 'length', 'num_segments']\n",
    "    test_df = pd.concat(test_dfs, ignore_index=True)[return_columns]\n",
    "    val_df = pd.concat(val_dfs, ignore_index=True)[return_columns] if val else pd.DataFrame(columns=return_columns)\n",
    "    train_df = pd.concat(train_dfs, ignore_index=True)[return_columns]\n",
    "\n",
    "    return test_df, val_df, train_df\n",
    "\n",
    "test_df, val_df, train_df = divide_games(segments_per_game_df, 100000, test_ratio=.15, val_ratio=.15, val=False)\n",
    "# Sum 'num_segments' for each 'label'\n",
    "print(test_df.groupby('labels')['num_segments'].sum())\n",
    "# Sum 'num_segments' for each 'label'\n",
    "if not val_df.empty:\n",
    "    print(val_df.groupby('labels')['num_segments'].sum())\n",
    "# Sum 'num_segments' for each 'label'\n",
    "print(train_df.groupby('labels')['num_segments'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in df['labels'].unique():\n",
    "    label_df = df[df['labels'] == label].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Ensure 'float_num_segments' is in label_df before proceeding\n",
    "    if 'float_num_segments' not in label_df.columns:\n",
    "        print(f\"'float_num_segments' column is missing in label_df for label {label}\")\n",
    "        continue  # Skip this label if the required column is missing\n",
    "\n",
    "    num_segments_cumsum = label_df['float_num_segments'].cumsum()\n",
    "    \n",
    "    test_idx = num_segments_cumsum[num_segments_cumsum <= num_segments_test].last_valid_index() or 0\n",
    "    val_idx = num_segments_cumsum[num_segments_cumsum <= num_segments_test + num_segments_val].last_valid_index() or test_idx\n",
    "\n",
    "    test_label_df = label_df.iloc[:test_idx + 1].copy()\n",
    "    val_label_df = label_df.iloc[test_idx + 1:val_idx + 1].copy() if val else pd.DataFrame()\n",
    "    train_label_df = label_df.iloc[val_idx + 1:].copy()\n",
    "\n",
    "    for split_df in [test_label_df, val_label_df, train_label_df]:\n",
    "        if not split_df.empty:\n",
    "            split_df['num_segments'] = split_df['float_num_segments'].astype(int)\n",
    "            split_df['frac_part'] = split_df['float_num_segments'] - split_df['num_segments']\n",
    "            split_df.sort_values(by='frac_part', ascending=False, inplace=True)\n",
    "\n",
    "            residual_count = num_segments_per_label - split_df['num_segments'].sum()\n",
    "            index_for_update = split_df.iloc[:residual_count].index\n",
    "            split_df.loc[index_for_update, 'num_segments'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataframe(df, segment_length_power):\n",
    "    \"\"\"\n",
    "    Returns a data frame listing the segments. One row per segment.\n",
    "    :param df: should be one of the three outputs of divide_games\n",
    "    \"\"\"\n",
    "    segment_length = 2 ** segment_length_power\n",
    "    \n",
    "    # Calculate the repeat count for each row based on 'num_segments'\n",
    "    repeats = df['num_segments'].values\n",
    "\n",
    "    \n",
    "    # Repeat each index according to its corresponding 'num_segments' value\n",
    "    index_repeated = np.repeat(df.index, repeats)\n",
    "    \n",
    "    # Create a new DataFrame by repeating rows\n",
    "    df_repeated = df.loc[index_repeated].reset_index(drop=True)\n",
    "    \n",
    "    # Create a 'segment_index' column that counts up for each group of repeated rows\n",
    "    segment_indices = np.concatenate([np.arange(n,dtype = np.int16) for n in repeats])\n",
    "    \n",
    "    # Assign 'segment_index' to the repeated DataFrame\n",
    "    df_repeated['segment_index'] = segment_indices\n",
    "    \n",
    "    df_repeated['segment_start_index'] = ( (df_repeated['length'] - segment_length) // df_repeated['num_segments']) * df_repeated['segment_index']\n",
    "    \n",
    "    df_repeated = df_repeated.drop(columns=['length', 'num_segments', 'segment_index'])\n",
    "    \n",
    "    return df_repeated\n",
    "\n",
    "train_segments_df = create_training_dataframe(train_df, 10)\n",
    "print(train_segments_df.value_counts('labels'))\n",
    "   \n",
    "test_segments_df = create_training_dataframe(test_df, 10)\n",
    "print(test_segments_df.value_counts('labels'))\n",
    "\n",
    "if not val_df.empty:\n",
    "    val_segments_df = create_training_dataframe(val_df, 10)\n",
    "    print(val_segments_df.value_counts('labels'))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_numpy(df, segment_length_power):\n",
    "    \"\"\"\n",
    "    Returns a numpy with all the segments in the dataframe. \n",
    "    :param df: should be one of the three outputs of divide_games\n",
    "    \"\"\"    \n",
    "    def process_game(path, label, length, num_segments, segment_length):\n",
    "        if num_segments == 0:\n",
    "            return\n",
    "        \n",
    "        with gzip.open(path, 'rb') as f:\n",
    "            inputs_array = np.load(f)\n",
    "            \n",
    "        segments_array = np.empty((num_segments, 9, segment_length), dtype=np.single)\n",
    "        \n",
    "        segment_shift = (length - segment_length) // num_segments\n",
    "        \n",
    "        for i in range(num_segments):\n",
    "            segments_array[i,:,:] = inputs_array[:, segment_shift * i : segment_shift * i + segment_length]\n",
    "        \n",
    "        shared_list.append((segments_array, [label] * num_segments))  # Append as a tuple\n",
    "        \n",
    "    segment_length = 2 ** segment_length_power\n",
    "    \n",
    "    # Zip the columns\n",
    "    tasks = [(row['player_inputs_np_save_path'], row['labels'], row['length'],  row['num_segments']) \n",
    "            for index, row in df.iterrows()]\n",
    "    \n",
    "    manager = Manager()\n",
    "    shared_list = manager.list()\n",
    "    \n",
    "    # Using Parallel and delayed to load segments in parallel\n",
    "    Parallel(n_jobs=-1, verbose=1)(delayed(process_game)(task[0], task[1], task[2], task[3], segment_length) for task in tqdm.tqdm(tasks))\n",
    "    \n",
    "    # Then in your main function after the parallel computation:\n",
    "    input_arrays, label_lists = zip(*list(shared_list))\n",
    "    input_array = np.concatenate(input_arrays, axis=0)\n",
    "    labels = np.concatenate(label_lists)\n",
    "\n",
    "    return input_array, labels\n",
    "\n",
    "input_array, labels = create_training_numpy(train_df, 10)\n",
    "print(input_array.shape)\n",
    "print(labels.shape)\n",
    "input_array, labels = create_training_numpy(test_df, 10)\n",
    "print(input_array.shape)\n",
    "print(labels.shape)\n",
    "if not val_df.empty:\n",
    "    input_array, labels = create_training_numpy(val_df, 10)\n",
    "    print(input_array.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

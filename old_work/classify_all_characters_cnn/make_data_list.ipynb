{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import shutil\n",
    "# import os\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "import pyarrow\n",
    "# Paths of datasets\n",
    "\n",
    "\n",
    "\n",
    "ranked_full_game_save_path = 'C:/Users/jaspa/Grant ML/ranked_full_subfolders'\n",
    "ranked_segment_save_path_subfolder = 'C:/Users/jaspa/Grant ML/ranked_segments_subfolders'\n",
    "ranked_segment_save_path_bulk = 'C:/Users/jaspa/Grant ML/ranked_segments_bulk'\n",
    "\n",
    "public_full_game_save_path = 'C:/Users/jaspa/Grant ML/public_full_subfolders'\n",
    "public_segment_save_path_subfolder = 'C:/Users/jaspa/Grant ML/public_segments_subfolders'\n",
    "public_segment_save_path_bulk = 'C:/Users/jaspa/Grant ML/public_segments_bulk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all slp game input arrays we want to process. Make a dataframe with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_files(directory):\n",
    "#     for root, dirs, files in os.walk(directory):\n",
    "#         for file in files:\n",
    "#             yield os.path.join(root, file)\n",
    "\n",
    "# def parse_file_path(file_path):\n",
    "#     parts = file_path.split('\\\\')\n",
    "#     # Extracting necessary details from the file path\n",
    "#     file_name = parts[-1]\n",
    "#     character = parts[-3]\n",
    "#     opponent = parts[-2]\n",
    "#     return {\n",
    "#         'file': file_name,\n",
    "#         'path': '\\\\'.join(parts[:-1]),\n",
    "#         'character': character,\n",
    "#         'opponent': opponent,\n",
    "#         'game_length': 0,  # Placeholder values\n",
    "#         'segment_shift': 0,\n",
    "#         'num_segments': 0\n",
    "#     }\n",
    "\n",
    "# def generate_dataframe_from_folders(folders):\n",
    "#     files_data = []\n",
    "#     for folder in folders:\n",
    "#         for file_path in list_files(folder):\n",
    "#             files_data.append(parse_file_path(file_path))\n",
    "#     return pd.DataFrame(files_data)\n",
    "\n",
    "# ranked_full_game_save_path = 'C:/Users/jaspa/Grant ML/ranked_full_subfolders'\n",
    "# public_full_game_save_path = 'C:/Users/jaspa/Grant ML/public_full_subfolders'\n",
    "# folders = [ranked_full_game_save_path, public_full_game_save_path]\n",
    "\n",
    "# df = generate_dataframe_from_folders(folders)\n",
    "# df  # Display the first few rows to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Joblib to open the game arrays and get the length of the game in frames. Save the data frame. About 40 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_array_length(file_path):\n",
    "#     # Update this function based on how you want to handle the file reading,\n",
    "#     # For gzipped numpy files, you would do something like this:\n",
    "#     with gzip.open(file_path, 'rb') as f:\n",
    "#         arr = np.load(f)\n",
    "#         return arr.shape[1]\n",
    "\n",
    "# def process_row(row):\n",
    "#     # Call the get_array_length function with the path of the file\n",
    "#     length = get_array_length(row['path'] + '\\\\' + row['file'])\n",
    "#     return length\n",
    "\n",
    "\n",
    "# num_cores = -1  # Or however many cores you want to use\n",
    "# lengths = Parallel(n_jobs=num_cores,verbose = 0)(delayed(process_row)(row) for index, row in tqdm.tqdm(df.iterrows()))\n",
    "# df['game_length'] = lengths\n",
    "\n",
    "# df.to_feather('C:/Users/jaspa/Grant ML/slp/data/path_info_df.feather')\n",
    "\n",
    "\n",
    "# print(df.head())  # To check the updated DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your feather file\n",
    "file_path = 'C:/Users/jaspa/Grant ML/slp/data/path_info_df.feather'\n",
    "\n",
    "# Load the feather file\n",
    "df = pd.read_feather(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of segments you want to use (same for each character). Check what the maximum power of two you need to shift each segment by to get the right number of segments. Can shift by as little as 64 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_segments =300000\n",
    "segment_length = 1024\n",
    "characters = [\n",
    "    'FOX', \n",
    "    'FALCO', \n",
    "    'MARTH', \n",
    "    'SHEIK', \n",
    "    'CAPTAIN_FALCON', \n",
    "    'PEACH', \n",
    "    'JIGGLYPUFF', \n",
    "    'SAMUS', \n",
    "    'ICE_CLIMBERS', \n",
    "    'GANONDORF', \n",
    "    'YOSHI', \n",
    "    'LUIGI', \n",
    "    'PIKACHU', \n",
    "    'DR_MARIO', \n",
    "    'NESS', \n",
    "    'LINK', \n",
    "    'MEWTWO', \n",
    "    'GAME_AND_WATCH', \n",
    "    'DONKEY_KONG', \n",
    "    'YOUNG_LINK', \n",
    "    'MARIO', \n",
    "    'ROY', \n",
    "    'BOWSER', \n",
    "    'ZELDA', \n",
    "    'KIRBY', \n",
    "    'PICHU'\n",
    "    ]\n",
    "for character in characters:\n",
    "    game_lengths = df.loc[df['character'] == character, 'game_length']\n",
    "    for i in range(10):\n",
    "        segment_shift = 2 ** (10-i)\n",
    "        num_segments = 0\n",
    "        for game_length in game_lengths:\n",
    "            num_segments += (game_length - segment_length) // segment_shift\n",
    "        if num_segments > min_segments:\n",
    "            break\n",
    "    print(character, ' ',segment_shift)\n",
    "    # Correct way to set values to avoid SettingWithCopyWarning\n",
    "    df.loc[df['character'] == character, 'segment_shift'] = segment_shift\n",
    "\n",
    "df.head() \n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many segments you will get from each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_segments'] = (df['game_length']- segment_length) // df['segment_shift']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how many extra segments you have for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_segments = 20000\n",
    "# segment_length = 1024\n",
    "for character in characters:\n",
    "    character_df = df.loc[df['character'] == character]\n",
    "    print(character, ': ', sum(character_df['num_segments'])-min_segments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize the games from each character. Cound how many segments you are going to get for each character. Split the games up so that 15% of the segments end up in the test set and the validation and 70% of the segments end up in the train set. No segments from the same game will end up in different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize empty DataFrames for test, validation, and training sets\n",
    "test_df = pd.DataFrame()\n",
    "val_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "\n",
    "# min_segments = 20000\n",
    "# target_segments_for_test_and_val = min_segments * 0.15\n",
    "\n",
    "for character in characters:\n",
    "    character_df = df.loc[df['character'] == character].sample(frac=1).reset_index(drop=True)  # Shuffle the rows\n",
    "    sum_segments = 0\n",
    "    test_idx = 0\n",
    "    val_idx = 0\n",
    "    \n",
    "    #split the games of each character by .15, .15, and .7\n",
    "    target_segments_for_test_and_val = int(sum(character_df['num_segments']) * 0.15)\n",
    "    # Determine the split indices for test and validation sets\n",
    "    for i, row in character_df.iterrows():\n",
    "        sum_segments += row['num_segments']\n",
    "        if sum_segments >= target_segments_for_test_and_val:\n",
    "            if test_idx == 0:  # First batch for test set\n",
    "                test_idx = i\n",
    "                sum_segments = 0  # Reset sum for validation set calculation\n",
    "            elif val_idx == 0:  # Next batch for validation set\n",
    "                val_idx = i\n",
    "                break\n",
    "    \n",
    "    # Split the character_df into test, val, and train based on the indices\n",
    "    test_rows = character_df.iloc[:test_idx+1]\n",
    "    val_rows = character_df.iloc[test_idx+1:val_idx+1]\n",
    "    train_rows = character_df.iloc[val_idx+1:]\n",
    "\n",
    "    # Append to respective DataFrames\n",
    "    test_df = pd.concat([test_df, test_rows], ignore_index=True)\n",
    "    val_df = pd.concat([val_df, val_rows], ignore_index=True)\n",
    "    train_df = pd.concat([train_df, train_rows], ignore_index=True)\n",
    "    \n",
    "print(test_df.shape)\n",
    "print(val_df.shape)\n",
    "print(train_df.shape)\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the data frames so that there is one path for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "def expand_df_vectorized(df):\n",
    "    # Calculate the repeat count for each row based on 'num_segments'\n",
    "    repeats = df['num_segments'].values\n",
    "    \n",
    "    # Repeat each index according to its corresponding 'num_segments' value\n",
    "    index_repeated = np.repeat(df.index, repeats)\n",
    "    \n",
    "    # Create a new DataFrame by repeating rows\n",
    "    df_repeated = df.loc[index_repeated].reset_index(drop=True)\n",
    "    \n",
    "    # Create a 'segment_index' column that counts up for each group of repeated rows\n",
    "    segment_indices = np.concatenate([np.arange(n,dtype = np.int16) for n in repeats])\n",
    "    \n",
    "    # Assign 'segment_index' to the repeated DataFrame\n",
    "    df_repeated['segment_index'] = segment_indices\n",
    "    \n",
    "    # Optionally, drop the 'num_segments' column if it's no longer needed\n",
    "    df_repeated = df_repeated.drop(columns=['num_segments'])\n",
    "    \n",
    "    return df_repeated\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "expanded_test_df = expand_df_vectorized(test_df)\n",
    "expanded_val_df = expand_df_vectorized(val_df)\n",
    "expanded_train_df = expand_df_vectorized(train_df)\n",
    "\n",
    "print(expanded_test_df.shape)\n",
    "print(expanded_val_df.shape)\n",
    "print(expanded_train_df.shape)\n",
    "print(expanded_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sample the right number of segments to create each data set.  Encode the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rows_per_character(df, proportion, min_segments, encoder,characters):\n",
    "    rows_per_character = int(min_segments * proportion)\n",
    "    sampled_df = pd.DataFrame()\n",
    "    \n",
    "    for character in characters:\n",
    "        character_df = df[df['character'] == character]\n",
    "        sampled_rows = character_df.sample(n=min(rows_per_character, len(character_df)), random_state=1)\n",
    "        sampled_df = pd.concat([sampled_df, sampled_rows], ignore_index=True)\n",
    "    \n",
    "    # Add the 'labels' column using the fitted encoder\n",
    "    sampled_df['labels'] = encoder.transform(sampled_df['character'])\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "# Assuming 'characters' is your array of unique character names\n",
    "characters =  [\n",
    "                'FOX', \n",
    "                'FALCO', \n",
    "                'MARTH', \n",
    "                'SHEIK', \n",
    "                'CAPTAIN_FALCON', \n",
    "                'PEACH', \n",
    "                'JIGGLYPUFF', \n",
    "                'SAMUS', \n",
    "                'ICE_CLIMBERS', \n",
    "                'GANONDORF', \n",
    "                'YOSHI', \n",
    "                'LUIGI', \n",
    "                'PIKACHU', \n",
    "                'DR_MARIO', \n",
    "                'NESS', \n",
    "                'LINK', \n",
    "                'MEWTWO', \n",
    "                'GAME_AND_WATCH', \n",
    "                'DONKEY_KONG', \n",
    "                'YOUNG_LINK', \n",
    "                'MARIO', \n",
    "                'ROY', \n",
    "                'BOWSER', \n",
    "                'ZELDA', \n",
    "                'KIRBY', \n",
    "                'PICHU'\n",
    "                ]\n",
    "\n",
    "# Initialize and fit the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(characters)\n",
    "\n",
    "sampled_test_df = sample_rows_per_character(expanded_test_df, 0.15, min_segments, encoder,characters)\n",
    "sampled_val_df = sample_rows_per_character(expanded_val_df, 0.15, min_segments, encoder,characters)\n",
    "sampled_train_df = sample_rows_per_character(expanded_train_df, 0.70, min_segments, encoder,characters)\n",
    "\n",
    "\n",
    "\n",
    "sampled_test_df = sampled_test_df.sample(frac=1).reset_index(drop=True)\n",
    "sampled_val_df = sampled_val_df.sample(frac=1).reset_index(drop=True)\n",
    "# sampled_train_df = sampled_train_df.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "\n",
    "print(sampled_test_df.shape)\n",
    "print(sampled_val_df.shape)\n",
    "print(sampled_train_df.shape)\n",
    "\n",
    "sampled_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_test_df.to_feather('C:/Users/jaspa/Grant ML/slp/data/sample_test_df.feather')\n",
    "sampled_val_df.to_feather('C:/Users/jaspa/Grant ML/slp/data/sample_val_df.feather')\n",
    "sampled_train_df.to_feather('C:/Users/jaspa/Grant ML/slp/data/sample_train_df.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['character'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch_tensorrt\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asses_model(model_name, y_pred, y_test, labels_order):\n",
    "    print()\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_pred, y_test)\n",
    "\n",
    "    # Print accuracy and Cohen Kappa score with explanations\n",
    "    print(f'Accuracy of {model_name}: {accuracy:.4f}')\n",
    "    print(f'Cohen Kappa Score of {model_name}: {kappa:.4f}')\n",
    "\n",
    "    # Calculate the normalized predicted label count\n",
    "    unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
    "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "    normalized_counts_pred = {k: v / counts_test[np.where(unique_test == k)[0][0]] for k, v in zip(unique_pred, counts_pred)}\n",
    "    \n",
    "    # Calculate the percent the model over or under predicted the labels using the specified label order\n",
    "    sorted_values = [normalized_counts_pred[k] - 1 if k in normalized_counts_pred else 0 for k in labels_order]\n",
    "\n",
    "    # Plotting the percent the model over or under predicted the labels\n",
    "    plt.figure(figsize=(2*len(labels_order)+10, 4))\n",
    "    plt.bar(labels_order, sorted_values, color=['green' if x > 0 else 'blue' for x in sorted_values])\n",
    "    plt.title(f'Percent Model {model_name} Over or Under Predicted Labels')\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Percent Over/Under Prediction')\n",
    "    \n",
    "    # Center y-axis and set equal extension above and below\n",
    "    max_extent = max(abs(min(sorted_values)), abs(max(sorted_values))) * 1.05\n",
    "    plt.ylim(-max_extent, max_extent)\n",
    "    plt.axhline(y=0, color='gray', linewidth=0.8)\n",
    "    plt.show()\n",
    "\n",
    "    # Display each confusion matrix on its own row\n",
    "    for norm in [None, 'true', 'pred']:\n",
    "        plt.figure(figsize=(len(labels_order)+4, len(labels_order)+4))\n",
    "        ax = plt.gca()\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test, y_pred, normalize=norm, ax=ax,\n",
    "            xticks_rotation='vertical', labels=labels_order\n",
    "        )\n",
    "        ax.title.set_text(f'{model_name} Confusion Matrix ({\"Not Normalized\" if norm is None else \"Normalized by \" + norm})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = ['ranked','public','mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True],\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FALCO'],\n",
    "    'character_name': ['PIKACHU', 'PICHU'],\n",
    "    'character_name': ['DR_MARIO', 'MARIO'],\n",
    "    # 'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'PEACH', 'LUIGI', 'DONKEY_KONG'],\n",
    "    'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR_MARIO    4202\n",
      "MARIO       1713\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/random_conv_resnet/../slp_package/input_dataset.py:95: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stage_name</th>\n",
       "      <th>num_players</th>\n",
       "      <th>conclusive</th>\n",
       "      <th>player_character_name</th>\n",
       "      <th>player_type_name</th>\n",
       "      <th>opposing_player_type_name</th>\n",
       "      <th>player_inputs_np_sub_path</th>\n",
       "      <th>length</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOSHIS_STORY</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>DR_MARIO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\DR_MARIO\\14760f74-d6d0-479f-8439-fdf5adc...</td>\n",
       "      <td>10660</td>\n",
       "      <td>DR_MARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BATTLEFIELD</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>DR_MARIO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\DR_MARIO\\c9f01df5-e378-4135-941d-e3d1689...</td>\n",
       "      <td>8127</td>\n",
       "      <td>DR_MARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BATTLEFIELD</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>DR_MARIO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\DR_MARIO\\66bfa1d1-f744-4745-84b0-e796c67...</td>\n",
       "      <td>11120</td>\n",
       "      <td>DR_MARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOUNTAIN_OF_DREAMS</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>DR_MARIO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\DR_MARIO\\9d97358f-afc9-4074-a8d8-2658dab...</td>\n",
       "      <td>6988</td>\n",
       "      <td>DR_MARIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POKEMON_STADIUM</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>DR_MARIO</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>mango\\DR_MARIO\\55113736-8401-46a1-b1dc-a3258f1...</td>\n",
       "      <td>9511</td>\n",
       "      <td>DR_MARIO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stage_name  num_players conclusive player_character_name  \\\n",
       "0        YOSHIS_STORY            2       True              DR_MARIO   \n",
       "1         BATTLEFIELD            2       True              DR_MARIO   \n",
       "2         BATTLEFIELD            2       True              DR_MARIO   \n",
       "3  FOUNTAIN_OF_DREAMS            2       True              DR_MARIO   \n",
       "4     POKEMON_STADIUM            2       True              DR_MARIO   \n",
       "\n",
       "  player_type_name opposing_player_type_name  \\\n",
       "0            HUMAN                     HUMAN   \n",
       "1            HUMAN                     HUMAN   \n",
       "2            HUMAN                     HUMAN   \n",
       "3            HUMAN                     HUMAN   \n",
       "4            HUMAN                     HUMAN   \n",
       "\n",
       "                           player_inputs_np_sub_path  length    labels  \n",
       "0  mango\\DR_MARIO\\14760f74-d6d0-479f-8439-fdf5adc...   10660  DR_MARIO  \n",
       "1  mango\\DR_MARIO\\c9f01df5-e378-4135-941d-e3d1689...    8127  DR_MARIO  \n",
       "2  mango\\DR_MARIO\\66bfa1d1-f744-4745-84b0-e796c67...   11120  DR_MARIO  \n",
       "3  mango\\DR_MARIO\\9d97358f-afc9-4074-a8d8-2658dab...    6988  DR_MARIO  \n",
       "4  mango\\DR_MARIO\\55113736-8401-46a1-b1dc-a3258f1...    9511  DR_MARIO  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "\n",
    "print(dataset.dataset['labels'].value_counts())\n",
    "dataset.dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Label  Count  Shift\n",
      "0  DR_MARIO   4177   3807\n",
      "1     MARIO   1710   1633\n"
     ]
    }
   ],
   "source": [
    "labels_order =  dataset.number_of_segments_per_game(3600, 8000)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           player_inputs_np_sub_path  length  num_segments  \\\n",
      "0  mango\\DR_MARIO\\80bce95c-b480-4eed-b282-184f207...    7400             1   \n",
      "1  ranked\\DR_MARIO\\016036ac-5f30-445b-846e-a06b8b...   11205             2   \n",
      "2  ranked\\DR_MARIO\\2010efeb-a9f0-4c22-8c57-a0fd7c...   15012             3   \n",
      "3  public\\DR_MARIO\\7687db8c-99ff-4d8e-881c-714cce...    7397             1   \n",
      "4  public\\DR_MARIO\\0dc06526-b116-4d79-90ce-0bcab6...    7391             1   \n",
      "\n",
      "     labels  encoded_labels  \n",
      "0  DR_MARIO               0  \n",
      "1  DR_MARIO               0  \n",
      "2  DR_MARIO               0  \n",
      "3  DR_MARIO               0  \n",
      "4  DR_MARIO               0  \n",
      "(12800, 6)\n",
      "(3200, 6)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df  = dataset.train_test_split_dataframes(test_ratio = .20, val = False)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading game segments from compressed numpy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.file_paths = df['player_inputs_np_sub_path'].to_numpy()\n",
    "        self.encoded_labels = df['encoded_labels'].to_numpy()\n",
    "        self.segment_start_index = df['segment_start_index'].to_numpy()\n",
    "        # self.segment_index = df['segment_index'].to_numpy()\n",
    "        self.segment_length = df['segment_length'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads and returns a sample from the dataset at the specified index.\"\"\"\n",
    "        with gzip.open('/workspace/melee_project_data/input_np/' + self.file_paths[idx].replace('\\\\','/'), 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "        \n",
    "        # Start and end of the segment\n",
    "        segment_start = self.segment_start_index[idx]\n",
    "        segment_end = self.segment_start_index[idx] + self.segment_length[idx]\n",
    "        \n",
    "        segment = segment[:,int(segment_start):int(segment_end)]\n",
    "        \n",
    "\n",
    "        segment[-5] = (segment[-5] > .5)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        segment_tensor = torch.from_numpy(segment).float()\n",
    "        label_tensor = torch.tensor(self.encoded_labels[idx], dtype=torch.long)\n",
    "        return segment_tensor, label_tensor\n",
    "    \n",
    "def prepare_data_loaders(train_df, test_df, batch_size, num_workers):\n",
    "    # Initialize datasets\n",
    "    train_dataset = TrainingDataset(train_df,True)\n",
    "    # val_dataset = TrainingDataset(file_paths_val, labels_val)\n",
    "    test_dataset = TrainingDataset(test_df,True)\n",
    "\n",
    "    # Initialize data loaders\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True,persistent_workers=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True,persistent_workers=True),\n",
    "        # 'val': DataLoader(val_dataset, batch_size=2**9, num_workers=num_workers, shuffle=False, pin_memory=True,persistent_workers=True)\n",
    "    }\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, loaders, device, random_conv, num_epochs=1):\n",
    "    # Initialize progress bar for the epochs\n",
    "    epoch_progress = tqdm(range(num_epochs), desc='Training Progress', unit='epoch')\n",
    "    for epoch in epoch_progress:\n",
    "        model.train()\n",
    "        epoch_loss_sum = 0\n",
    "        epoch_total = 0\n",
    "\n",
    "        for _, input_cpu, target_cpu in enumerate(loaders['train']):\n",
    "            # Move data to the appropriate device\n",
    "            input_gpu = input_cpu.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_gpu = model(random_conv(input_gpu))\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(pred_gpu, target_cpu.to(device)) \n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to avoid exploding gradients\n",
    "            # clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Perform a single optimization step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss sum for accurate average calculation\n",
    "            epoch_loss_sum += loss.item() * target_cpu.size(0)\n",
    "            epoch_total += target_cpu.size(0)\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        epoch_loss = epoch_loss_sum / epoch_total\n",
    "\n",
    "        # Update progress bar with the final loss of the epoch\n",
    "        epoch_progress.set_postfix(Loss=f'{epoch_loss:.10f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(model, loaders, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loader_tqdm = tqdm(loaders[loader], unit='batch')\n",
    "        \n",
    "        for _, target_cpu in enumerate(eval_loader_tqdm):\n",
    "            target_gpu = target_cpu.to(device)\n",
    "            output_gpu = model(target_gpu)\n",
    "            # output_gpu = torch.sigmoid(output_gpu)\n",
    "            \n",
    "            predictions.append(torch.sigmoid(output_gpu).cpu().numpy())\n",
    "            targets.append(target_cpu.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomConv1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RandomConv1d, self).__init__()\n",
    "        # Randomly set the number of input and output channels\n",
    "        in_channels = 9  # Arbitrary range from 1 to 9\n",
    "        out_channels = 9 # Arbitrary range from 1 to 9\n",
    "\n",
    "        # Random kernel size, usually a small odd number is practical\n",
    "        kernel_size = np.random.choice([3, 5, 7, 11, 13])\n",
    "\n",
    "\n",
    "\n",
    "        # Random dilation, usually 1 or a small number\n",
    "        dilation = np.random.randint(1,10)\n",
    "\n",
    "        # Random padding, usually related to the kernel size\n",
    "        padding = np.random.randint(0, kernel_size // 2 + 1)\n",
    "        # Create the Conv1d layer with random parameters\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               padding=padding, dilation=dilation)\n",
    "\n",
    "        # Randomly initialize weights and biases\n",
    "        nn.init.uniform_(self.conv1.weight, -0.1, 0.1)\n",
    "        if self.conv1.bias is not None:\n",
    "            nn.init.uniform_(self.conv1.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "# Example of using the class\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# model = torch.compile(model,mode = 'max-autotune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading game segments from compressed numpy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.file_paths = df['player_inputs_np_sub_path'].to_numpy()\n",
    "        self.encoded_labels = df['encoded_labels'].to_numpy()\n",
    "        self.segment_start_index = df['segment_start_index'].to_numpy()\n",
    "        self.segment_length = df['segment_length'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads and returns a sample from the dataset at the specified index.\"\"\"\n",
    "        with gzip.open('/workspace/melee_project_data/input_np/' + self.file_paths[idx].replace('\\\\','/'), 'rb') as f:\n",
    "            segment = np.load(f)\n",
    "        \n",
    "        segment_start = self.segment_start_index[idx]\n",
    "        segment_end = segment_start + self.segment_length[idx]\n",
    "        \n",
    "        segment = segment[:, int(segment_start):int(segment_end)]\n",
    "        segment[-5] = (segment[-5] > .5)\n",
    "\n",
    "        segment_tensor = torch.from_numpy(segment).float()\n",
    "        label_tensor = torch.tensor(self.encoded_labels[idx], dtype=torch.long)\n",
    "\n",
    "        return segment_tensor, label_tensor\n",
    "\n",
    "def prepare_data_loaders(train_df, test_df, batch_size, num_workers):\n",
    "    train_dataset = TrainingDataset(train_df, True)\n",
    "    test_dataset = TrainingDataset(test_df, True)\n",
    "\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True, persistent_workers=True),\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "def train_model(model, criterion, optimizer, loaders, device, conv_layer, num_epochs=1):\n",
    "    epoch_progress = tqdm(range(num_epochs), desc='Training Progress', unit='epoch')\n",
    "    for epoch in epoch_progress:\n",
    "        model.train()\n",
    "        epoch_loss_sum = 0\n",
    "        epoch_total = 0\n",
    "\n",
    "        for _, (input_cpu, target_cpu) in enumerate(loaders['train']):  # Correct unpacking\n",
    "            input_gpu = conv_layer(input_cpu.to(device))  # Preprocess with random_conv\n",
    "            target_gpu = target_cpu.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output_gpu = model(input_gpu)\n",
    "            # print(target_gpu)\n",
    "            # print(output_gpu)\n",
    "            loss = criterion(output_gpu, target_gpu)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss_sum += loss.item() * target_cpu.size(0)\n",
    "            epoch_total += target_cpu.size(0)\n",
    "\n",
    "        epoch_loss = epoch_loss_sum / epoch_total\n",
    "        epoch_progress.set_postfix(Loss=f'{epoch_loss:.10f}')\n",
    "\n",
    "    return\n",
    "\n",
    "def predict(model, loaders, loader, conv_layer, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # eval_loader_tqdm = tqdm(loaders[loader], unit='batch')\n",
    "        \n",
    "        for _, (input_cpu, labels_cpu) in enumerate(loaders[loader]):\n",
    "            target_gpu = conv_layer(input_cpu.to(device))\n",
    "            output_gpu = model(target_gpu)\n",
    "            # output_gpu = torch.sigmoid(output_gpu)\n",
    "            \n",
    "            predictions.append(torch.sigmoid(output_gpu).cpu().numpy())\n",
    "            targets.append(labels_cpu.numpy())\n",
    "    \n",
    "    return  np.concatenate(predictions, axis=0), np.concatenate(targets, axis=0)\n",
    "    # targets = np.concatenate(targets, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encode_ResNet_Model import ResNet50\n",
    "\n",
    "channels = 9\n",
    "batch_size =  8\n",
    "num_workers = 16\n",
    "loaders = prepare_data_loaders(train_df, test_df, batch_size, num_workers)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "epochs = 10\n",
    "# model = torch.compile(model,mode = 'max-autotune')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet50(2, channels).to('cuda')\n",
    "# optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "# random_conv = RandomConv1d().to('cuda').eval()\n",
    "# train_model(model, criterion, optimizer, loaders, 'cuda', random_conv , num_epochs=epochs)\n",
    "# pred = predict(model, loaders, 'test', random_conv, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_preds = np.zeros((3200,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:23<00:00, 26.30s/epoch, Loss=0.1098772552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0: 0.8678\n",
      "Cohen Kappa Score of 0: 0.7356\n",
      "Running Accuracy 0: 0.8678\n",
      "Running Cohen Kappa Score 0: 0.7356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:53<00:00, 29.32s/epoch, Loss=0.0745252047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1: 0.8778\n",
      "Cohen Kappa Score of 1: 0.7556\n",
      "Running Accuracy 1: 0.9306\n",
      "Running Cohen Kappa Score 1: 0.8612\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:37<00:00, 27.77s/epoch, Loss=0.0854723208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 2: 0.9325\n",
      "Cohen Kappa Score of 2: 0.8650\n",
      "Running Accuracy 2: 0.9381\n",
      "Running Cohen Kappa Score 2: 0.8762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:49<00:00, 28.99s/epoch, Loss=0.1245645602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3: 0.8681\n",
      "Cohen Kappa Score of 3: 0.7363\n",
      "Running Accuracy 3: 0.9334\n",
      "Running Cohen Kappa Score 3: 0.8669\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:34<00:00, 27.44s/epoch, Loss=0.1499652916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 4: 0.7744\n",
      "Cohen Kappa Score of 4: 0.5488\n",
      "Running Accuracy 4: 0.9350\n",
      "Running Cohen Kappa Score 4: 0.8700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:42<00:00, 28.26s/epoch, Loss=0.0661999931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 5: 0.9266\n",
      "Cohen Kappa Score of 5: 0.8531\n",
      "Running Accuracy 5: 0.9425\n",
      "Running Cohen Kappa Score 5: 0.8850\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:43<00:00, 28.34s/epoch, Loss=0.1567060117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 6: 0.8297\n",
      "Cohen Kappa Score of 6: 0.6594\n",
      "Running Accuracy 6: 0.9381\n",
      "Running Cohen Kappa Score 6: 0.8762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:52<00:00, 29.22s/epoch, Loss=0.0703003116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 7: 0.9228\n",
      "Cohen Kappa Score of 7: 0.8456\n",
      "Running Accuracy 7: 0.9447\n",
      "Running Cohen Kappa Score 7: 0.8894\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:48<00:00, 28.86s/epoch, Loss=0.0732241109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 8: 0.8525\n",
      "Cohen Kappa Score of 8: 0.7050\n",
      "Running Accuracy 8: 0.9416\n",
      "Running Cohen Kappa Score 8: 0.8831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:39<00:00, 27.97s/epoch, Loss=0.1205959884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 9: 0.8881\n",
      "Cohen Kappa Score of 9: 0.7762\n",
      "Running Accuracy 9: 0.9416\n",
      "Running Cohen Kappa Score 9: 0.8831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:51<00:00, 29.14s/epoch, Loss=0.0788355491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10: 0.9291\n",
      "Cohen Kappa Score of 10: 0.8581\n",
      "Running Accuracy 10: 0.9466\n",
      "Running Cohen Kappa Score 10: 0.8931\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:39<00:00, 27.95s/epoch, Loss=0.0788041658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 11: 0.9394\n",
      "Cohen Kappa Score of 11: 0.8788\n",
      "Running Accuracy 11: 0.9459\n",
      "Running Cohen Kappa Score 11: 0.8919\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:46<00:00, 28.66s/epoch, Loss=0.1210339753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 12: 0.8928\n",
      "Cohen Kappa Score of 12: 0.7856\n",
      "Running Accuracy 12: 0.9469\n",
      "Running Cohen Kappa Score 12: 0.8938\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:39<00:00, 27.97s/epoch, Loss=0.1103187756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 13: 0.8525\n",
      "Cohen Kappa Score of 13: 0.7050\n",
      "Running Accuracy 13: 0.9459\n",
      "Running Cohen Kappa Score 13: 0.8919\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:36<00:00, 27.65s/epoch, Loss=0.1720805582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 14: 0.8094\n",
      "Cohen Kappa Score of 14: 0.6188\n",
      "Running Accuracy 14: 0.9444\n",
      "Running Cohen Kappa Score 14: 0.8888\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:51<00:00, 29.15s/epoch, Loss=0.0756255774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 15: 0.8834\n",
      "Cohen Kappa Score of 15: 0.7669\n",
      "Running Accuracy 15: 0.9481\n",
      "Running Cohen Kappa Score 15: 0.8962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:47<00:00, 28.78s/epoch, Loss=0.1286727302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 16: 0.8734\n",
      "Cohen Kappa Score of 16: 0.7469\n",
      "Running Accuracy 16: 0.9491\n",
      "Running Cohen Kappa Score 16: 0.8981\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:27<00:00, 26.77s/epoch, Loss=0.1298744783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 17: 0.8750\n",
      "Cohen Kappa Score of 17: 0.7500\n",
      "Running Accuracy 17: 0.9469\n",
      "Running Cohen Kappa Score 17: 0.8938\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:53<00:00, 29.40s/epoch, Loss=0.0769932266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 18: 0.9459\n",
      "Cohen Kappa Score of 18: 0.8919\n",
      "Running Accuracy 18: 0.9484\n",
      "Running Cohen Kappa Score 18: 0.8969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:44<00:00, 28.49s/epoch, Loss=0.0928680218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 19: 0.6047\n",
      "Cohen Kappa Score of 19: 0.2094\n",
      "Running Accuracy 19: 0.9500\n",
      "Running Cohen Kappa Score 19: 0.9000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:51<00:00, 29.13s/epoch, Loss=0.1120561275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 20: 0.8844\n",
      "Cohen Kappa Score of 20: 0.7688\n",
      "Running Accuracy 20: 0.9519\n",
      "Running Cohen Kappa Score 20: 0.9038\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:43<00:00, 28.31s/epoch, Loss=0.0937152866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 21: 0.9181\n",
      "Cohen Kappa Score of 21: 0.8362\n",
      "Running Accuracy 21: 0.9516\n",
      "Running Cohen Kappa Score 21: 0.9031\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:42<00:00, 28.25s/epoch, Loss=0.0845111176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 22: 0.9356\n",
      "Cohen Kappa Score of 22: 0.8712\n",
      "Running Accuracy 22: 0.9506\n",
      "Running Cohen Kappa Score 22: 0.9012\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:19<00:00, 25.99s/epoch, Loss=0.0900467203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 23: 0.8256\n",
      "Cohen Kappa Score of 23: 0.6512\n",
      "Running Accuracy 23: 0.9519\n",
      "Running Cohen Kappa Score 23: 0.9038\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:35<00:00, 27.54s/epoch, Loss=0.0720354769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 24: 0.9316\n",
      "Cohen Kappa Score of 24: 0.8631\n",
      "Running Accuracy 24: 0.9541\n",
      "Running Cohen Kappa Score 24: 0.9081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [05:00<00:00, 30.09s/epoch, Loss=0.1223658780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 25: 0.8728\n",
      "Cohen Kappa Score of 25: 0.7456\n",
      "Running Accuracy 25: 0.9525\n",
      "Running Cohen Kappa Score 25: 0.9050\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:48<00:00, 28.83s/epoch, Loss=0.0990355125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 26: 0.8250\n",
      "Cohen Kappa Score of 26: 0.6500\n",
      "Running Accuracy 26: 0.9516\n",
      "Running Cohen Kappa Score 26: 0.9031\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:52<00:00, 29.27s/epoch, Loss=0.0818678711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 27: 0.9256\n",
      "Cohen Kappa Score of 27: 0.8513\n",
      "Running Accuracy 27: 0.9522\n",
      "Running Cohen Kappa Score 27: 0.9044\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:50<00:00, 29.09s/epoch, Loss=0.0827462502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 28: 0.8509\n",
      "Cohen Kappa Score of 28: 0.7019\n",
      "Running Accuracy 28: 0.9500\n",
      "Running Cohen Kappa Score 28: 0.9000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:41<00:00, 28.15s/epoch, Loss=0.1458336860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 29: 0.8050\n",
      "Cohen Kappa Score of 29: 0.6100\n",
      "Running Accuracy 29: 0.9525\n",
      "Running Cohen Kappa Score 29: 0.9050\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:36<00:00, 27.67s/epoch, Loss=0.1080943365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 30: 0.9062\n",
      "Cohen Kappa Score of 30: 0.8125\n",
      "Running Accuracy 30: 0.9513\n",
      "Running Cohen Kappa Score 30: 0.9025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:40<00:00, 28.01s/epoch, Loss=0.1148756626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 31: 0.8441\n",
      "Cohen Kappa Score of 31: 0.6881\n",
      "Running Accuracy 31: 0.9531\n",
      "Running Cohen Kappa Score 31: 0.9062\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:34<00:00, 27.48s/epoch, Loss=0.1105702900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 32: 0.9053\n",
      "Cohen Kappa Score of 32: 0.8106\n",
      "Running Accuracy 32: 0.9528\n",
      "Running Cohen Kappa Score 32: 0.9056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:46<00:00, 28.65s/epoch, Loss=0.1348568210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 33: 0.8087\n",
      "Cohen Kappa Score of 33: 0.6175\n",
      "Running Accuracy 33: 0.9513\n",
      "Running Cohen Kappa Score 33: 0.9025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10/10 [04:34<00:00, 27.43s/epoch, Loss=0.1200689620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 34: 0.8556\n",
      "Cohen Kappa Score of 34: 0.7112\n",
      "Running Accuracy 34: 0.9513\n",
      "Running Cohen Kappa Score 34: 0.9025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████  | 8/10 [03:44<00:55, 27.99s/epoch, Loss=0.0903039929]"
     ]
    }
   ],
   "source": [
    "for n in range(100):\n",
    "    model = ResNet50(2, channels).to('cuda')\n",
    "    optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "    # model = torch.compile(model)\n",
    "    random_conv = RandomConv1d().to('cuda').eval()\n",
    "    train_model(model, criterion, optimizer, loaders, 'cuda', random_conv , num_epochs=epochs)\n",
    "    pred, target = predict(model, loaders, 'test', random_conv, 'cuda')\n",
    "    average_preds = (average_preds * (n) + pred) / (n+1)\n",
    "    \n",
    "    pred_label = pred > .5\n",
    "    average_pred_label = average_preds > .5\n",
    "    kappa = cohen_kappa_score(pred_label, target)\n",
    "    accuracy = accuracy_score(pred_label, target)\n",
    "    print(f'Accuracy of {n}: {accuracy:.4f}')\n",
    "    print(f'Cohen Kappa Score of {n}: {kappa:.4f}')\n",
    "    \n",
    "    kappa = cohen_kappa_score(average_pred_label, target)\n",
    "    accuracy = accuracy_score(average_pred_label, target)\n",
    "    print(f'Running Accuracy {n}: {accuracy:.4f}')\n",
    "    print(f'Running Cohen Kappa Score {n}: {kappa:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Accuracy 281: 0.9300\n",
      "Running Cohen Kappa Score 281: 0.8600\n"
     ]
    }
   ],
   "source": [
    "    # pred_label = pred > .5\n",
    "    average_pred_label = average_preds > .5\n",
    "    # kappa = cohen_kappa_score(pred_label, target)\n",
    "    # accuracy = accuracy_score(pred_label, target)\n",
    "    # print(f'Accuracy of {n}: {accuracy:.4f}')\n",
    "    # print(f'Cohen Kappa Score of {n}: {kappa:.4f}')\n",
    "    \n",
    "    kappa = cohen_kappa_score(average_pred_label, target)\n",
    "    accuracy = accuracy_score(average_pred_label, target)\n",
    "    print(f'Running Accuracy {n}: {accuracy:.4f}')\n",
    "    print(f'Running Cohen Kappa Score {n}: {kappa:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch_tensorrt\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from slp_package.slp_functions import create_merged_game_data_df\n",
    "from slp_package.input_dataset import InputDataSet\n",
    "import slp_package.pytorch_functions as slp_pytorch_functions\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using CUDA\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We classify 5 characters on competitive stages\n",
    "\n",
    "source_data = ['public','ranked','mango']\n",
    "\n",
    "general_features = {\n",
    "    'stage_name': ['FOUNTAIN_OF_DREAMS','FINAL_DESTINATION','BATTLEFIELD','YOSHIS_STORY','POKEMON_STADIUM','DREAMLAND'],\n",
    "    'num_players': [2],\n",
    "    'conclusive': [True]\n",
    "}\n",
    "player_features = {\n",
    "    # 'netplay_code': ['MANG#0'],\n",
    "    # 'character_name': ['FOX', 'FALCO', 'MARTH', 'CAPTAIN_FALCON', 'SHEIK'],\n",
    "    # 'character_name': ['PIKACHU','PICHU'],\n",
    "    # 'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'PEACH', 'LUIGI', 'DONKEY_KONG'],\n",
    "    'character_name': ['FOX', 'CAPTAIN_FALCON', 'SHEIK', 'FALCO', 'GAME_AND_WATCH', 'MARTH', 'LINK', 'ICE_CLIMBERS', 'SAMUS', 'GANONDORF', 'BOWSER', 'MEWTWO', 'YOSHI', 'PIKACHU', 'JIGGLYPUFF', 'NESS', 'DR_MARIO', 'MARIO', 'PEACH', 'ROY', 'LUIGI', 'YOUNG_LINK', 'DONKEY_KONG', 'PICHU', 'KIRBY'], # No ZELDA\n",
    "    'type_name': ['HUMAN']\n",
    "    \n",
    "}\n",
    "opposing_player_features = {\n",
    "    # 'character_name': ['MARTH'],\n",
    "    # 'netplay_code': ['KOD#0', 'ZAIN#0']\n",
    "    'type_name': ['HUMAN']\n",
    "}\n",
    "label_info = {\n",
    "    'source': ['player'], # Can be 'general', 'player\n",
    "    # 'feature': ['netplay_code']\n",
    "    'feature': ['character_name']\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappedDataset(Dataset):\n",
    "    def __init__(self, file_path, dtype, shape):\n",
    "        self.data = np.memmap(file_path, dtype=dtype, mode='r', shape=shape)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def prepare_data_loaders(batch_size, num_workers):\n",
    "    # Initialize datasets\n",
    "    save_path = \"/workspace/melee_project_data/MiniRocket_Transform/\"\n",
    "    \n",
    "    # Train dataset\n",
    "    filename = 'minirocket_all_characters_60s_5000_segments_per_character_train.dat'\n",
    "    full_path = os.path.join(save_path, filename)\n",
    "    dtype = 'float32'  # Example dtype, adjust as necessary\n",
    "    shape = (5000, 60, 2048)  # Example shape, adjust as necessary\n",
    "    train_dataset = MappedDataset(full_path, dtype, shape)\n",
    "    \n",
    "    # Test dataset\n",
    "    filename = 'minirocket_all_characters_60s_5000_segments_per_character_test.dat'\n",
    "    full_path = os.path.join(save_path, filename)\n",
    "    test_dataset = MappedDataset(full_path, dtype, shape)\n",
    "\n",
    "    # Initialize data loaders\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True, drop_last=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "def init(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.constant_(layer.weight.data, 0)\n",
    "        nn.init.constant_(layer.bias.data, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/slp_jaspar/rocket_classifier_all_data/../slp_package/input_dataset.py:95: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  processed_df = pd.concat([player_1_df, player_2_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOX               103069\n",
      "FALCO              90719\n",
      "MARTH              53728\n",
      "CAPTAIN_FALCON     38006\n",
      "SHEIK              27623\n",
      "PEACH              17438\n",
      "JIGGLYPUFF         16374\n",
      "SAMUS               9524\n",
      "ICE_CLIMBERS        6849\n",
      "GANONDORF           6655\n",
      "YOSHI               5725\n",
      "LUIGI               5230\n",
      "DR_MARIO            4202\n",
      "PIKACHU             4096\n",
      "LINK                2502\n",
      "NESS                2306\n",
      "DONKEY_KONG         2026\n",
      "GAME_AND_WATCH      1967\n",
      "MEWTWO              1775\n",
      "MARIO               1713\n",
      "YOUNG_LINK          1447\n",
      "ROY                 1272\n",
      "BOWSER               940\n",
      "KIRBY                556\n",
      "PICHU                230\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = InputDataSet(source_data, general_features, player_features, opposing_player_features, label_info)\n",
    "\n",
    "print(dataset.dataset['labels'].value_counts())\n",
    "# print(list(dataset.dataset['labels'].unique()))\n",
    "# dataset.dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Label   Count   Shift\n",
      "0              FOX  102551  120601\n",
      "1            FALCO   90263  103954\n",
      "2            MARTH   53538   68557\n",
      "3   CAPTAIN_FALCON   37820   43246\n",
      "4            SHEIK   27536   39604\n",
      "5            PEACH   17367   27064\n",
      "6       JIGGLYPUFF   16214   24015\n",
      "7            SAMUS    9489   16294\n",
      "8     ICE_CLIMBERS    6820   10776\n",
      "9        GANONDORF    6611    8103\n",
      "10           YOSHI    5704    8177\n",
      "11           LUIGI    5210    7765\n",
      "12        DR_MARIO    4177    6091\n",
      "13         PIKACHU    4067    6097\n",
      "14            LINK    2489    3829\n",
      "15            NESS    2291    4183\n",
      "16     DONKEY_KONG    2009    2903\n",
      "17  GAME_AND_WATCH    1949    2305\n",
      "18          MEWTWO    1758    3257\n",
      "19           MARIO    1710    2612\n",
      "20      YOUNG_LINK    1430    2256\n",
      "21             ROY    1262    1787\n",
      "22          BOWSER     934    1532\n",
      "23           KIRBY     531     851\n",
      "24           PICHU     227     330\n"
     ]
    }
   ],
   "source": [
    "labels_order =  dataset.number_of_segments_per_game(3600,5000)\n",
    "print(labels_order)\n",
    "labels_order = labels_order['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 323140/323140 [00:02<00:00, 160985.50it/s]\n",
      "100%|██████████| 80817/80817 [00:00<00:00, 146935.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# _, _, y_train, y_test  = dataset.train_test_split_numpy(test_ratio = .20, val = False)\n",
    "# save_path = \"/workspace/melee_project_data/MiniRocket_Transform/\"\n",
    "\n",
    "# with gzip.open(save_path + 'minirocket_all_characters_60s_5000_segments_per_character_y_test', 'wb') as f:\n",
    "#     np.save(f, y_test)\n",
    "# with gzip.open(save_path + 'minirocket_all_characters_60s_5000_segments_per_character_y_train', 'wb') as f:\n",
    "#     np.save(f, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FALCO' 'FALCO' 'FALCO' ... 'SAMUS' 'SAMUS' 'SAMUS']\n"
     ]
    }
   ],
   "source": [
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import gzip\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch import nn\n",
    "\n",
    "# class MappedDataset(Dataset):\n",
    "#     def __init__(self, data_path, label_path, dtype, shape):\n",
    "#         # Memory-mapped data for features\n",
    "#         self.data = np.memmap(data_path, dtype=dtype, mode='r', shape=shape)\n",
    "#         # Load labels from compressed file\n",
    "#         with gzip.open(label_path, 'rb') as f:\n",
    "#             self.labels = np.load(f)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Return both the feature and label for the given index\n",
    "#         return self.data[idx], self.labels[idx]\n",
    "\n",
    "# def prepare_data_loaders(batch_size, num_workers):\n",
    "#     save_path = \"/workspace/melee_project_data/MiniRocket_Transform/\"\n",
    "    \n",
    "#     # Train dataset\n",
    "#     train_data_path = os.path.join(save_path, 'minirocket_all_characters_60s_5000_segments_per_character_train.dat')\n",
    "#     train_label_path = os.path.join(save_path, 'minirocket_all_characters_60s_5000_segments_per_character_y_train')\n",
    "#     dtype = 'float32'  # Example dtype, adjust as necessary\n",
    "#     shape = (5000, 60, 2048)  # Example shape, adjust as necessary\n",
    "#     train_dataset = MappedDataset(train_data_path, train_label_path, dtype, shape)\n",
    "    \n",
    "#     # Test dataset\n",
    "#     test_data_path = os.path.join(save_path, 'minirocket_all_characters_60s_5000_segments_per_character_test.dat')\n",
    "#     test_label_path = os.path.join(save_path, 'minirocket_all_characters_60s_5000_segments_per_character_y_test')\n",
    "#     test_dataset = MappedDataset(test_data_path, test_label_path, dtype, shape)\n",
    "\n",
    "#     # Initialize data loaders\n",
    "#     loaders = {\n",
    "#         'train': DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True, drop_last=True),\n",
    "#         'test': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "#     }\n",
    "#     return loaders\n",
    "\n",
    "# def init(layer):\n",
    "#     if isinstance(layer, nn.Linear):\n",
    "#         nn.init.constant_(layer.weight.data, 0)\n",
    "#         nn.init.constant_(layer.bias.data, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and encode labels\n",
    "def load_and_encode_labels(label_path):\n",
    "    with gzip.open(label_path, 'rb') as f:\n",
    "        labels = np.load(f)\n",
    "    return labels\n",
    "\n",
    "# Prepare and encode labels before initializing datasets\n",
    "def prepare_encoded_labels(train_label_path, test_label_path):\n",
    "    # Load labels\n",
    "    y_train = load_and_encode_labels(train_label_path)\n",
    "    y_test = load_and_encode_labels(test_label_path)\n",
    "    \n",
    "    # Create a label encoder and fit it to all possible labels (train + test)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(np.concatenate((y_train, y_test), axis=0))\n",
    "    \n",
    "    # Transform labels to encoded version\n",
    "    encoded_y_train = encoder.transform(y_train)\n",
    "    encoded_y_test = encoder.transform(y_test)\n",
    "    \n",
    "    return encoded_y_train, encoded_y_test\n",
    "\n",
    "class MappedDataset(Dataset):\n",
    "    def __init__(self, data_path, labels, shape):\n",
    "        # Memory-mapped data for features\n",
    "        self.data = np.memmap(data_path, dtype='float32', mode='r', shape=shape)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return both the feature and label for the given index\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def prepare_data_loaders(batch_size, num_workers):\n",
    "    save_path = \"/workspace/melee_project_data/MiniRocket_Transform/\"\n",
    "    \n",
    "    # Paths\n",
    "    train_data_path = os.path.join(save_path, 'minirocket_all_characters_60s_5000_segments_per_character_train.dat')\n",
    "    train_label_path = os.path.join(save_path, 'minirocket_all_characters_60s_5000_segments_per_character_y_train')\n",
    "    test_data_path = os.path.join(save_path, 'minirocket_all_characters_60s_5000_segments_per_character_test.dat')\n",
    "    test_label_path = os.path.join(save_path, 'minirocket_all_characters_60s_5000_segments_per_character_y_test')\n",
    "    \n",
    "    # Encode labels\n",
    "    encoded_y_train, encoded_y_test = prepare_encoded_labels(train_label_path, test_label_path)\n",
    "    \n",
    "    # Datasets with updated shapes\n",
    "    train_dataset = MappedDataset(train_data_path, encoded_y_train, shape=(100000, 127932))\n",
    "    test_dataset = MappedDataset(test_data_path, encoded_y_test, shape=(25000, 127932))\n",
    "\n",
    "    # Initialize data loaders\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, persistent_workers=True, drop_last=True),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "# Example of initializing and using the data loaders\n",
    "loaders = prepare_data_loaders(batch_size=32, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, loaders, loss_function, optimizer, scheduler, num_epochs=10):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         running_loss = 0.0\n",
    "        \n",
    "#         # Initialize tqdm with the desired output format\n",
    "#         with tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as tbar:\n",
    "#             for inputs, labels in tbar:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "#                 optimizer.zero_grad()  # Zero the parameter gradients\n",
    "#                 outputs = model(inputs)  # Forward pass\n",
    "#                 loss = loss_function(outputs, labels)  # Compute loss\n",
    "#                 loss.backward()  # Backpropagation\n",
    "#                 optimizer.step()  # Optimize the model\n",
    "\n",
    "#                 running_loss += loss.item() * inputs.size(0)  # Update total loss\n",
    "\n",
    "#                 # Set the post description of tqdm for showing batches per second\n",
    "#                 tbar.set_postfix(loss=(running_loss / (tbar.n + 1)), bps=f'{1/tbar.avg:.2f}')\n",
    "\n",
    "#         epoch_loss = running_loss / len(loaders['train'].dataset)\n",
    "#         print(f'Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "#         # Validation phase (if you have a validation loader)\n",
    "#         model.eval()  # Set model to evaluate mode\n",
    "#         with torch.no_grad(), tqdm(loaders['test'], desc='Validation', unit='batch') as vbar:\n",
    "#             valid_loss = 0.0\n",
    "#             for inputs, labels in vbar:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = loss_function(outputs, labels)\n",
    "#                 valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#                 # Set the post description of tqdm for showing batches per second in validation\n",
    "#                 vbar.set_postfix(bps=f'{1/vbar.avg:.2f}')\n",
    "\n",
    "#             valid_loss /= len(loaders['test'].dataset)\n",
    "#             print(f'Validation Loss: {valid_loss:.4f}')\n",
    "        \n",
    "#         scheduler.step(valid_loss)  # Adjust learning rate based on validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loaders, loss_function, optimizer, scheduler, num_epochs=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with tqdm(loaders['train'], desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as tbar:\n",
    "            for data in tbar:\n",
    "                inputs, labels = data\n",
    "                # Convert inputs and labels to PyTorch tensors and move them to the device\n",
    "                inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "                # print(inputs.shape)\n",
    "                \n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "                outputs = model(inputs)  # Forward pass\n",
    "                loss = loss_function(outputs, labels)  # Compute loss\n",
    "                loss.backward()  # Backpropagation\n",
    "                optimizer.step()  # Optimize the model\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)  # Update total loss\n",
    "                \n",
    "                # Update the progress bar\n",
    "                tbar.set_postfix(loss=(running_loss / (tbar.n + 1)))\n",
    "\n",
    "        epoch_loss = running_loss / len(loaders['train'].dataset)\n",
    "        print(f'Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluate mode\n",
    "        with torch.no_grad(), tqdm(loaders['test'], desc='Validation', unit='batch') as vbar:\n",
    "            valid_loss = 0.0\n",
    "            for data in vbar:\n",
    "                inputs, labels = data\n",
    "                inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Update the validation progress bar\n",
    "                vbar.set_postfix(loss=(valid_loss / (vbar.n + 1)))\n",
    "\n",
    "            valid_loss /= len(loaders['test'].dataset)\n",
    "            print(f'Validation Loss: {valid_loss:.4f}')\n",
    "        \n",
    "        scheduler.step(valid_loss)  # Adjust learning rate based on validation loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=127932, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(127932, 25))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loaders = prepare_data_loaders(batch_size=32, num_workers=16)\n",
    "optimizer = Adam(model.parameters(), lr = .0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor = 0.5, min_lr = 1e-8, patience = 5)\n",
    "model.apply(init)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/3125 [00:00<?, ?batch/s]/tmp/ipykernel_53056/3092518281.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_53056/3092518281.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
      "Epoch 1/10: 100%|██████████| 3125/3125 [00:15<00:00, 208.26batch/s, loss=117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/781 [00:00<?, ?batch/s]/tmp/ipykernel_53056/3092518281.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_53056/3092518281.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
      "Validation: 100%|██████████| 781/781 [00:03<00:00, 233.31batch/s, loss=246]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.3763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3125/3125 [00:15<00:00, 205.90batch/s, loss=117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 781/781 [00:03<00:00, 232.54batch/s, loss=286]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3125/3125 [00:15<00:00, 205.16batch/s, loss=116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 781/781 [00:03<00:00, 234.61batch/s, loss=237] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.3868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3125/3125 [00:15<00:00, 205.92batch/s, loss=115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 781/781 [00:03<00:00, 230.57batch/s, loss=280]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.5772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  29%|██▊       | 891/3125 [00:04<00:10, 204.45batch/s, loss=115]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loaders, loss_function, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert inputs and labels to PyTorch tensors and move them to the device\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(inputs.shape)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, loaders, loss_function, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

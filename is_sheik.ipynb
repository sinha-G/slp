{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Detecting Whether the Inputs are a Sheik </h1>\n",
    "\n",
    "We want to train a binary classifier to accurately predict whether a multi-channel time series (representing a Super Smash Bros. Melee player's inputs) was produced by a Sheik player. We first load our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import slippi as slp\n",
    "\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preliminary Functions </h2>\n",
    "\n",
    "We use these functions to one-hot encode the button bitmask and get the frame data for a given port number and frames object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of time steps in the model inputs\n",
    "frames_per_input = 60 * 12     # 12 seconds of gameplay\n",
    "\n",
    "\n",
    "def one_hot_encode(bitmask):\n",
    "    labels = ['DPAD_LEFT', 'DPAD_RIGHT', 'DPAD_DOWN', 'DPAD_UP', 'Z', 'R', 'L', 'A', 'B', 'X', 'Y', 'START']\n",
    "    encoded_values = [1, 2, 4, 8, 16, 32, 64, 256, 512, 1024, 2048, 4096]\n",
    "\n",
    "    # Create a dictionary mapping labels to their encoded values\n",
    "    label_to_value = dict(zip(labels, encoded_values))\n",
    "\n",
    "    # Initialize a list to store the one-hot encoded values\n",
    "    one_hot_encoded = [0] * len(labels)\n",
    "\n",
    "    # Iterate through labels and set the corresponding one-hot encoded value\n",
    "    for label, value in label_to_value.items():\n",
    "        if bitmask & value:\n",
    "            one_hot_encoded[labels.index(label)] = 1\n",
    "\n",
    "    return one_hot_encoded\n",
    "\n",
    "\n",
    "# # # Append a new row to a numpy list\n",
    "# def get_frame_data(frames, port):\n",
    "#     sheik_inputs = np.empty((0, 18))  # Initialize an empty NumPy array\n",
    "#     # sheik_inputs = []  # Initialize an empty NumPy array\n",
    "#     # for i, frame in enumerate(frames[300: 300 + frames_per_input]):   # Take frames_per_input frames. skips first 5 seconds.\n",
    "#     for i, frame in enumerate(frames[300: ]):   # Takes all the frames. skips first 5 seconds.\n",
    "#         buttons = one_hot_encode(frame.ports[port].leader.pre.buttons.physical.value)\n",
    "#         j_x = frame.ports[port].leader.pre.joystick.x\n",
    "#         j_y = frame.ports[port].leader.pre.joystick.y\n",
    "#         c_x = frame.ports[port].leader.pre.cstick.x\n",
    "#         c_y = frame.ports[port].leader.pre.cstick.y\n",
    "#         t_l = frame.ports[port].leader.pre.triggers.physical.l\n",
    "#         t_r = frame.ports[port].leader.pre.triggers.physical.r\n",
    "\n",
    "#         frame_data = np.array(buttons + [j_x, j_y, c_x, c_y, t_l, t_r]).reshape(1, -1)\n",
    "#         sheik_inputs = np.vstack((sheik_inputs, frame_data))\n",
    "\n",
    "#     return sheik_inputs\n",
    "\n",
    "# Create a numpy list that is the correct size and fill it with a loop\n",
    "def get_frame_data(frames, port):\n",
    "    \n",
    "    # sheik_inputs = np.empty((frames_per_input, 18))  # Initialize an empty NumPy array\n",
    "    # for i, frame in enumerate(frames[300:  300 + frames_per_input]):   # Take frames_per_input frames. skips first 5 seconds.\n",
    "        \n",
    "    sheik_inputs = np.empty((len(frames)-300, 18))  # Initialize an empty NumPy array    \n",
    "    for i, frame in enumerate(frames[300: ]):   # Takes all the frames. skips first 5 seconds.\n",
    "        buttons = one_hot_encode(frame.ports[port].leader.pre.buttons.physical.value)\n",
    "        j_x = frame.ports[port].leader.pre.joystick.x\n",
    "        j_y = frame.ports[port].leader.pre.joystick.y\n",
    "        c_x = frame.ports[port].leader.pre.cstick.x\n",
    "        c_y = frame.ports[port].leader.pre.cstick.y\n",
    "        t_l = frame.ports[port].leader.pre.triggers.physical.l\n",
    "        t_r = frame.ports[port].leader.pre.triggers.physical.r\n",
    "\n",
    "        frame_data = buttons + [j_x, j_y, c_x, c_y, t_l, t_r]\n",
    "        sheik_inputs[i] = frame_data\n",
    "\n",
    "    return sheik_inputs\n",
    "\n",
    "# # List comprehension\n",
    "# def get_frame_data(frames, port):\n",
    "#     return np.array([\n",
    "#         one_hot_encode(frame.ports[port].leader.pre.buttons.physical.value) +\n",
    "#         [frame.ports[port].leader.pre.joystick.x,\n",
    "#          frame.ports[port].leader.pre.joystick.y,\n",
    "#          frame.ports[port].leader.pre.cstick.x,\n",
    "#          frame.ports[port].leader.pre.cstick.y,\n",
    "#          frame.ports[port].leader.pre.triggers.physical.l,\n",
    "#          frame.ports[port].leader.pre.triggers.physical.r]\n",
    "#         # for frame in frames[300: 300 + frames_per_input] # Take frames_per_input frames. skips first 5 seconds.\n",
    "#         for frame in frames[300:] # Takes all the frames. skips first 5 seconds.\n",
    "#     ])\n",
    "\n",
    "# Append to a numpy list is vastly slower than the other two.\n",
    "# Getting the frames in the second two algorithms take the same amount of time,\n",
    "# but the third option takes longer because it seems to have more to do after it is done to get the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Loading </h2>\n",
    "\n",
    "We begin by iterating through the Slippi Public Dataset, extracting replays of Sheik-Fox games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2362 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "  2%|▏         | 48/2362 [00:00<00:32, 70.91it/s][Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.2s\n",
      "  3%|▎         | 72/2362 [00:01<00:43, 53.01it/s][Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    1.5s\n",
      "  4%|▍         | 96/2362 [00:01<00:45, 49.27it/s][Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    2.1s\n",
      "  5%|▌         | 120/2362 [00:02<00:48, 46.50it/s][Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    3.0s\n",
      "  6%|▌         | 144/2362 [00:03<00:53, 41.64it/s][Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:    3.3s\n",
      "  7%|▋         | 168/2362 [00:03<00:52, 41.61it/s][Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:    3.9s\n",
      "  8%|▊         | 192/2362 [00:04<00:52, 41.20it/s][Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    4.3s\n",
      "  9%|▉         | 216/2362 [00:04<00:50, 42.08it/s][Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:    4.9s\n",
      " 10%|█         | 240/2362 [00:05<00:51, 40.90it/s][Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:    6.1s\n",
      " 11%|█         | 264/2362 [00:06<00:54, 38.36it/s][Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:    6.5s\n",
      " 12%|█▏        | 288/2362 [00:06<00:51, 40.63it/s][Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:    7.2s\n",
      " 14%|█▍        | 336/2362 [00:07<00:49, 40.96it/s][Parallel(n_jobs=-1)]: Done 290 tasks      | elapsed:    7.8s\n",
      " 15%|█▌        | 360/2362 [00:08<00:45, 43.69it/s][Parallel(n_jobs=-1)]: Done 317 tasks      | elapsed:    8.4s\n",
      " 16%|█▋        | 384/2362 [00:08<00:47, 41.32it/s][Parallel(n_jobs=-1)]: Done 344 tasks      | elapsed:    9.0s\n",
      " 17%|█▋        | 408/2362 [00:09<00:46, 42.05it/s][Parallel(n_jobs=-1)]: Done 373 tasks      | elapsed:    9.7s\n",
      " 18%|█▊        | 432/2362 [00:10<00:45, 41.98it/s][Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:   10.3s\n",
      " 19%|█▉        | 456/2362 [00:10<00:44, 42.96it/s][Parallel(n_jobs=-1)]: Done 433 tasks      | elapsed:   11.2s\n",
      " 21%|██▏       | 504/2362 [00:11<00:46, 39.63it/s][Parallel(n_jobs=-1)]: Done 464 tasks      | elapsed:   12.0s\n",
      " 22%|██▏       | 528/2362 [00:12<00:48, 37.60it/s][Parallel(n_jobs=-1)]: Done 497 tasks      | elapsed:   13.1s\n",
      " 24%|██▍       | 576/2362 [00:13<00:48, 37.07it/s][Parallel(n_jobs=-1)]: Done 530 tasks      | elapsed:   13.9s\n",
      " 25%|██▌       | 600/2362 [00:14<00:46, 37.50it/s][Parallel(n_jobs=-1)]: Done 565 tasks      | elapsed:   14.8s\n",
      " 26%|██▋       | 624/2362 [00:15<00:46, 37.26it/s][Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed:   15.7s\n",
      " 28%|██▊       | 672/2362 [00:16<00:45, 37.38it/s][Parallel(n_jobs=-1)]: Done 637 tasks      | elapsed:   16.8s\n",
      " 30%|███       | 720/2362 [00:17<00:45, 36.11it/s][Parallel(n_jobs=-1)]: Done 674 tasks      | elapsed:   17.8s\n",
      " 31%|███▏      | 744/2362 [00:18<00:43, 36.91it/s][Parallel(n_jobs=-1)]: Done 713 tasks      | elapsed:   18.9s\n",
      " 34%|███▎      | 792/2362 [00:19<00:41, 37.53it/s][Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:   19.9s\n",
      " 35%|███▍      | 816/2362 [00:20<00:41, 37.24it/s][Parallel(n_jobs=-1)]: Done 793 tasks      | elapsed:   21.0s\n",
      " 37%|███▋      | 864/2362 [00:21<00:40, 36.81it/s][Parallel(n_jobs=-1)]: Done 834 tasks      | elapsed:   22.1s\n",
      " 39%|███▊      | 912/2362 [00:22<00:37, 38.28it/s][Parallel(n_jobs=-1)]: Done 877 tasks      | elapsed:   23.2s\n",
      " 41%|████      | 960/2362 [00:24<00:36, 38.30it/s][Parallel(n_jobs=-1)]: Done 920 tasks      | elapsed:   24.3s\n",
      " 43%|████▎     | 1008/2362 [00:25<00:35, 38.39it/s][Parallel(n_jobs=-1)]: Done 965 tasks      | elapsed:   25.5s\n",
      " 45%|████▍     | 1056/2362 [00:26<00:35, 36.30it/s][Parallel(n_jobs=-1)]: Done 1010 tasks      | elapsed:   26.7s\n",
      " 46%|████▌     | 1080/2362 [00:27<00:35, 35.99it/s][Parallel(n_jobs=-1)]: Done 1057 tasks      | elapsed:   28.1s\n",
      " 48%|████▊     | 1128/2362 [00:28<00:31, 39.24it/s][Parallel(n_jobs=-1)]: Done 1104 tasks      | elapsed:   29.2s\n",
      " 50%|████▉     | 1176/2362 [00:29<00:30, 38.76it/s][Parallel(n_jobs=-1)]: Done 1153 tasks      | elapsed:   30.3s\n",
      " 53%|█████▎    | 1248/2362 [00:31<00:28, 38.73it/s][Parallel(n_jobs=-1)]: Done 1202 tasks      | elapsed:   31.7s\n",
      " 55%|█████▍    | 1296/2362 [00:32<00:27, 39.11it/s][Parallel(n_jobs=-1)]: Done 1253 tasks      | elapsed:   33.0s\n",
      " 57%|█████▋    | 1344/2362 [00:34<00:25, 39.99it/s][Parallel(n_jobs=-1)]: Done 1304 tasks      | elapsed:   34.2s\n",
      " 59%|█████▉    | 1392/2362 [00:35<00:24, 39.33it/s][Parallel(n_jobs=-1)]: Done 1357 tasks      | elapsed:   35.5s\n",
      " 61%|██████    | 1440/2362 [00:36<00:23, 39.03it/s][Parallel(n_jobs=-1)]: Done 1410 tasks      | elapsed:   36.9s\n",
      " 63%|██████▎   | 1488/2362 [00:37<00:22, 39.20it/s][Parallel(n_jobs=-1)]: Done 1465 tasks      | elapsed:   38.3s\n",
      " 66%|██████▌   | 1560/2362 [00:39<00:20, 38.91it/s][Parallel(n_jobs=-1)]: Done 1520 tasks      | elapsed:   39.7s\n",
      " 68%|██████▊   | 1608/2362 [00:40<00:19, 37.82it/s][Parallel(n_jobs=-1)]: Done 1577 tasks      | elapsed:   41.2s\n",
      " 71%|███████   | 1680/2362 [00:42<00:18, 37.72it/s][Parallel(n_jobs=-1)]: Done 1634 tasks      | elapsed:   42.8s\n",
      " 73%|███████▎  | 1728/2362 [00:44<00:17, 37.16it/s][Parallel(n_jobs=-1)]: Done 1693 tasks      | elapsed:   44.4s\n",
      " 75%|███████▌  | 1776/2362 [00:45<00:15, 38.78it/s][Parallel(n_jobs=-1)]: Done 1752 tasks      | elapsed:   46.0s\n",
      " 78%|███████▊  | 1848/2362 [00:47<00:13, 37.75it/s][Parallel(n_jobs=-1)]: Done 1813 tasks      | elapsed:   47.7s\n",
      " 81%|████████▏ | 1920/2362 [00:49<00:11, 36.89it/s][Parallel(n_jobs=-1)]: Done 1874 tasks      | elapsed:   49.3s\n",
      " 83%|████████▎ | 1968/2362 [00:50<00:10, 37.22it/s][Parallel(n_jobs=-1)]: Done 1937 tasks      | elapsed:   51.0s\n",
      " 86%|████████▋ | 2040/2362 [00:52<00:08, 38.28it/s][Parallel(n_jobs=-1)]: Done 2000 tasks      | elapsed:   52.6s\n",
      " 88%|████████▊ | 2088/2362 [00:53<00:07, 36.39it/s][Parallel(n_jobs=-1)]: Done 2065 tasks      | elapsed:   54.5s\n",
      " 91%|█████████▏| 2160/2362 [00:55<00:05, 38.45it/s][Parallel(n_jobs=-1)]: Done 2130 tasks      | elapsed:   56.1s\n",
      " 94%|█████████▍| 2232/2362 [00:57<00:03, 43.22it/s][Parallel(n_jobs=-1)]: Done 2197 tasks      | elapsed:   57.6s\n",
      " 98%|█████████▊| 2304/2362 [00:59<00:01, 33.98it/s][Parallel(n_jobs=-1)]: Done 2264 tasks      | elapsed:   59.7s\n",
      "100%|██████████| 2362/2362 [01:01<00:00, 38.69it/s]\n",
      "[Parallel(n_jobs=-1)]: Done 2362 out of 2362 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             TimeSeries  Label  \\\n",
      "13    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "25    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      1   \n",
      "21    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "40    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      1   \n",
      "10    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "...                                                 ...    ...   \n",
      "4498  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      1   \n",
      "4508  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "4506  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...      1   \n",
      "4511  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "4509  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      1   \n",
      "\n",
      "                                                  FName  \n",
      "13            00_37_01.564Z [314] Fox + Sheik (FoD).slp  \n",
      "25            00_37_01.564Z [314] Fox + Sheik (FoD).slp  \n",
      "21             00_40_07.217Z [314] Fox + Sheik (DL).slp  \n",
      "40             00_40_07.217Z [314] Fox + Sheik (DL).slp  \n",
      "10     00_45_18.826Z [NELL] Fox + [LUST] Sheik (BF).slp  \n",
      "...                                                 ...  \n",
      "4498         Sheik vs Fox [YS] Game_20210222T125144.slp  \n",
      "4508  Sheik vs Fox [YS] Game_8C56C529AEAA_20200311T2...  \n",
      "4506  Sheik vs Fox [YS] Game_8C56C529AEAA_20200311T2...  \n",
      "4511  Sheik vs Fox [YS] Game_8C56C529AEAA_20200311T2...  \n",
      "4509  Sheik vs Fox [YS] Game_8C56C529AEAA_20200311T2...  \n",
      "\n",
      "[4516 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import slp\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "\n",
    "# Function to process a single SLP file and append to shared lists\n",
    "def process_slp_file(slp_file, dataset_path, time_series_list, label_list, ids):\n",
    "    try:\n",
    "        file_path = os.path.join(dataset_path, slp_file)\n",
    "        game = slp.Game(file_path)\n",
    "        frames = game.frames\n",
    "\n",
    "        if len(frames) < 300 + frames_per_input:  # Ignore games that are <3600 frames (i.e. <60 seconds)\n",
    "            return\n",
    "\n",
    "        # List occupied ports\n",
    "        occupied_ports = [i for i, port in enumerate(game.start.players) if port is not None]\n",
    "        port_1 = occupied_ports[0]\n",
    "        port_2 = occupied_ports[1]\n",
    "\n",
    "        if len(occupied_ports) > 2:  # Ignore games that aren't singles\n",
    "            return\n",
    "\n",
    "        # Determine characters playing\n",
    "        occupied_ports = [i for i, port in enumerate(game.start.players) if port is not None]\n",
    "        port_1_character = game.start.players[port_1].character.name\n",
    "        port_2_character = game.start.players[port_2].character.name\n",
    "\n",
    "\n",
    "        frame_data = get_frame_data(frames, port_1)\n",
    "        time_series_list.append(frame_data)\n",
    "        label_list.append(1 if port_1_character == 'SHEIK' else 0)\n",
    "        ids.append(slp_file)\n",
    "        frame_data = get_frame_data(frames, port_2)\n",
    "        time_series_list.append(frame_data)\n",
    "        label_list.append(1 if port_2_character == 'SHEIK' else 0)\n",
    "        ids.append(slp_file)   \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {slp_file}: {str(e)}\")\n",
    "\n",
    "# Set your dataset_path and frames_per_input\n",
    "dataset_path = './Slippi_Public_Dataset_v3/'\n",
    "# frames_per_input = ...\n",
    "\n",
    "slp_files = [file for file in os.listdir(dataset_path) if file.endswith('.slp') and 'Sheik' in file and 'Fox' in file]\n",
    "print(len(slp_files))\n",
    "\n",
    "# Create shared lists to store results\n",
    "manager = Manager()\n",
    "time_series_list = manager.list()\n",
    "label_list = manager.list()\n",
    "ids = manager.list()\n",
    "\n",
    "# Use joblib to parallelize processing of SLP files\n",
    "Parallel(n_jobs=-1, verbose=10)(delayed(process_slp_file)(slp_file, dataset_path, time_series_list, label_list, ids) for slp_file in tqdm.tqdm(slp_files))\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame({\"TimeSeries\": list(time_series_list), \"Label\": list(label_list), \"FName\": list(ids)})\n",
    "\n",
    "\n",
    "df.sort_values(by=['FName','Label'],inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get memory usage for each column in bytes\n",
    "# memory_usage = df.memory_usage(deep=True)\n",
    "\n",
    "# # Sum the memory usage values to get the total memory usage of the DataFrame\n",
    "# total_memory_usage = memory_usage.sum()\n",
    "\n",
    "# print(f\"Total memory usage of the DataFrame: {total_memory_usage} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Specify the file path where you want to save the pickle file\n",
    "# pickle_file_path = './data/Sheik_vs_Fox_full_input_data.pkl'\n",
    "\n",
    "# # Save the DataFrame as a pickle file\n",
    "# df.to_pickle(pickle_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Visualization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate_rows = df[df.duplicated(subset = 'TimeSeries', keep = False)]\n",
    "\n",
    "# print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Preprocessing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to PyTorch tensors\n",
    "# time_series_tensor = torch.tensor(np.array(time_series_list), dtype=torch.float32)\n",
    "# label_tensor = torch.tensor(label_list, dtype=torch.float32)\n",
    "\n",
    "# channels = 18\n",
    "\n",
    "# # Normalize each channel individually\n",
    "# scaler = StandardScaler()\n",
    "# time_series_normalized = torch.zeros(time_series_tensor.shape)\n",
    "\n",
    "# # Iterate over channels\n",
    "# # for i in range(channels):\n",
    "# #     time_series_normalized[:, :, i] = torch.tensor(scaler.fit_transform(time_series_tensor[:, :, i]))\n",
    "\n",
    "# # print(time_series_sensor.shape)\n",
    "# # print(time_series_normalized.shape)\n",
    "\n",
    "# train_data, test_data, train_labels, test_labels = train_test_split(time_series_normalized, label_tensor, test_size = 0.2, shuffle = True, stratify = label_tensor)\n",
    "\n",
    "# print(torch.isnan(time_series_normalized).any())\n",
    "# print(torch.isnan(label_tensor).any())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

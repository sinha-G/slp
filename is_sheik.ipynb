{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Detecting Whether the Inputs are a Sheik </h1>\n",
    "\n",
    "We want to train a binary classifier to accurately predict whether a multi-channel time series (representing a Super Smash Bros. Melee player's inputs) was produced by a Sheik player. We first load our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import slippi as slp\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preliminary Functions </h2>\n",
    "\n",
    "We use these functions to one-hot encode the button bitmask and get the frame data for a given port number and frames object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of time steps in the model inputs\n",
    "frames_per_input = 60 * 12     # 12 seconds of gameplay\n",
    "\n",
    "def one_hot_encode(bitmask):\n",
    "    labels = ['DPAD_LEFT', 'DPAD_RIGHT', 'DPAD_DOWN', 'DPAD_UP', 'Z', 'R', 'L', 'A', 'B', 'X', 'Y', 'START']\n",
    "    encoded_values = [1, 2, 4, 8, 16, 32, 64, 256, 512, 1024, 2048, 4096]\n",
    "\n",
    "    # Create a dictionary mapping labels to their encoded values\n",
    "    label_to_value = dict(zip(labels, encoded_values))\n",
    "\n",
    "    # Initialize a list to store the one-hot encoded values\n",
    "    one_hot_encoded = [0] * len(labels)\n",
    "\n",
    "    # Iterate through labels and set the corresponding one-hot encoded value\n",
    "    for label, value in label_to_value.items():\n",
    "        if bitmask & value:\n",
    "            one_hot_encoded[labels.index(label)] = 1\n",
    "\n",
    "    return one_hot_encoded\n",
    "\n",
    "def get_frame_data(frames, port):\n",
    "    sheik_inputs = np.empty((0, 18))  # Initialize an empty NumPy array\n",
    "\n",
    "    for i, frame in enumerate(frames[300: 300 + frames_per_input]):   # Take frames_per_input frames. skips first 5 seconds.\n",
    "        buttons = one_hot_encode(frame.ports[port].leader.pre.buttons.physical.value)\n",
    "        j_x = frame.ports[port].leader.pre.joystick.x\n",
    "        j_y = frame.ports[port].leader.pre.joystick.y\n",
    "        c_x = frame.ports[port].leader.pre.cstick.x\n",
    "        c_y = frame.ports[port].leader.pre.cstick.y\n",
    "        t_l = frame.ports[port].leader.pre.triggers.physical.l\n",
    "        t_r = frame.ports[port].leader.pre.triggers.physical.r\n",
    "\n",
    "        frame_data = np.array(buttons + [j_x, j_y, c_x, c_y, t_l, t_r]).reshape(1, -1)\n",
    "        sheik_inputs = np.vstack((sheik_inputs, frame_data))\n",
    "\n",
    "    return sheik_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Loading </h2>\n",
    "\n",
    "We begin by iterating through the Slippi Public Dataset, extracting replays of Sheik-Fox games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 290 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 317 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 344 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 373 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 433 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 464 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 497 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 530 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 565 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 637 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 674 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 713 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 793 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 834 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 877 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 920 tasks      | elapsed:   16.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m ids \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mlist()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Use joblib to parallelize processing of SLP files\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_slp_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_series_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mslp_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslp_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from the results\u001b[39;00m\n\u001b[0;32m     58\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(time_series_list), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(label_list), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(ids)})\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\slp\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\slp\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaspa\\.conda\\envs\\slp\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "# import slp\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Manager\n",
    "\n",
    "# Function to process a single SLP file and append to shared lists\n",
    "def process_slp_file(slp_file, dataset_path, time_series_list, label_list, ids):\n",
    "    try:\n",
    "        file_path = os.path.join(dataset_path, slp_file)\n",
    "        game = slp.Game(file_path)\n",
    "        frames = game.frames\n",
    "\n",
    "        if len(frames) < 300 + frames_per_input:  # Ignore games that are <3600 frames (i.e. <60 seconds)\n",
    "            return\n",
    "\n",
    "        # List occupied ports\n",
    "        occupied_ports = [i for i, port in enumerate(game.start.players) if port is not None]\n",
    "        port_1 = occupied_ports[0]\n",
    "        port_2 = occupied_ports[1]\n",
    "\n",
    "        if len(occupied_ports) > 2:  # Ignore games that aren't singles\n",
    "            return\n",
    "\n",
    "        # Determine characters playing\n",
    "        port_1_character = game.start.players[port_1].character.name\n",
    "        port_2_character = game.start.players[port_2].character.name\n",
    "\n",
    "        frame_data = get_frame_data(frames, port_1)\n",
    "        time_series_list.append(frame_data)\n",
    "        label_list.append(1 if port_1_character == 'SHEIK' else 0)\n",
    "        ids.append(slp_file)\n",
    "        frame_data = get_frame_data(frames, port_2)\n",
    "        time_series_list.append(frame_data)\n",
    "        label_list.append(1 if port_2_character == 'SHEIK' else 0)\n",
    "        ids.append(slp_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {slp_file}: {str(e)}\")\n",
    "\n",
    "# Set your dataset_path and frames_per_input\n",
    "dataset_path = './Slippi_Public_Dataset_v3/'\n",
    "# frames_per_input = ...\n",
    "\n",
    "slp_files = [file for file in os.listdir(dataset_path) if file.endswith('.slp') and 'Sheik' in file and 'Fox' in file]\n",
    "print(len(slp_files))\n",
    "\n",
    "# Create shared lists to store results\n",
    "manager = Manager()\n",
    "time_series_list = manager.list()\n",
    "label_list = manager.list()\n",
    "ids = manager.list()\n",
    "\n",
    "# Use joblib to parallelize processing of SLP files\n",
    "Parallel(n_jobs=-1, verbose=10)(delayed(process_slp_file)(slp_file, dataset_path, time_series_list, label_list, ids) for slp_file in tqdm.tqdm(slp_files))\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame({\"TimeSeries\": list(time_series_list), \"Label\": list(label_list), \"FName\": list(ids)})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(time_series_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            TimeSeries  Label  \\\n",
      "0    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "1    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      1   \n",
      "2    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "3    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      1   \n",
      "4    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "..                                                 ...    ...   \n",
      "195  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "196  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      1   \n",
      "197  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "198  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0   \n",
      "199  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...      1   \n",
      "\n",
      "                                                FName  \n",
      "0           00_37_01.564Z [314] Fox + Sheik (FoD).slp  \n",
      "1           00_37_01.564Z [314] Fox + Sheik (FoD).slp  \n",
      "2            00_40_07.217Z [314] Fox + Sheik (DL).slp  \n",
      "3            00_40_07.217Z [314] Fox + Sheik (DL).slp  \n",
      "4    00_45_18.826Z [NELL] Fox + [LUST] Sheik (BF).slp  \n",
      "..                                                ...  \n",
      "195                     12_56_25 Sheik + Fox (FD).slp  \n",
      "196                     12_56_34 Sheik + Fox (PS).slp  \n",
      "197                     12_56_34 Sheik + Fox (PS).slp  \n",
      "198                    12_56_38 Fox + Sheik (FoD).slp  \n",
      "199                    12_56_38 Fox + Sheik (FoD).slp  \n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = './Slippi_Public_Dataset_v3/'\n",
    "\n",
    "# List files in the dataset with Sheik\n",
    "slp_files = [file for file in os.listdir(dataset_path) if file.endswith('.slp') and 'Sheik' in file and 'Fox' in file][:100]\n",
    "print(len(slp_files))\n",
    "\n",
    "time_series_list =[]\n",
    "label_list = []\n",
    "ids = []\n",
    "\n",
    "# Load the .slp files\n",
    "for i, slp_file in enumerate(tqdm.tqdm(slp_files)):\n",
    "    \n",
    "    # Get file path and store game variable\n",
    "    file_path = os.path.join(dataset_path, slp_file)\n",
    "    game = slp.Game(file_path)\n",
    "    frames = game.frames\n",
    "\n",
    "    if len(frames) < 300 + frames_per_input:          # Ignore games that are <3600 frames (i.e. <60 seconds)\n",
    "        continue\n",
    "    \n",
    "    # List occupied ports\n",
    "    occupied_ports = [i for i, port in enumerate(game.start.players) if port is not None]\n",
    "    port_1 = occupied_ports[0]\n",
    "    port_2 = occupied_ports[1]    \n",
    "\n",
    "    if (len(occupied_ports)) > 2:   # Ignore games that aren't singles\n",
    "            continue\n",
    "\n",
    "    # Determine characters playing\n",
    "    port_1_character = game.start.players[port_1].character.name\n",
    "    port_2_character = game.start.players[port_2].character.name\n",
    "\n",
    "    frame_data = get_frame_data(frames, port_1)\n",
    "    time_series_list.append(frame_data)\n",
    "    label_list.append(1 if port_1_character == 'SHEIK' else 0)\n",
    "    ids.append(slp_file)\n",
    "    frame_data = get_frame_data(frames, port_2)\n",
    "    time_series_list.append(frame_data)\n",
    "    label_list.append(1 if port_2_character == 'SHEIK' else 0)\n",
    "    ids.append(slp_file)\n",
    "    \n",
    "df = pd.DataFrame({\"TimeSeries\": time_series_list, \"Label\": label_list, \"FName\": ids})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Visualization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2362 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import slp\n",
    "# import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "# Function to process each slp file\n",
    "def process_slp_file(slp_file):\n",
    "    file_path = os.path.join(dataset_path, slp_file)\n",
    "    game = slp.Game(file_path)\n",
    "    frames = game.frames\n",
    "\n",
    "    if len(frames) < 300 + frames_per_input:          # Ignore games that are <3600 frames (i.e. <60 seconds)\n",
    "        print(f\"Ignoring {slp_file} - Game length is less than required.\")\n",
    "        return None\n",
    "    \n",
    "    # List occupied ports\n",
    "    occupied_ports = [i for i, port in enumerate(game.start.players) if port is not None]\n",
    "    port_1 = occupied_ports[0]\n",
    "    port_2 = occupied_ports[1]    \n",
    "\n",
    "    if len(occupied_ports) > 2:   # Ignore games that aren't singles\n",
    "        print(f\"Ignoring {slp_file} - Not a singles game.\")\n",
    "        return None\n",
    "\n",
    "    # Determine characters playing\n",
    "    port_1_character = game.start.players[port_1].character.name\n",
    "    port_2_character = game.start.players[port_2].character.name\n",
    "\n",
    "    frame_data1 = get_frame_data(frames, port_1)\n",
    "    frame_data2 = get_frame_data(frames, port_2)\n",
    "\n",
    "    print(f\"Processed {slp_file}\")\n",
    "    return frame_data1, port_1_character, frame_data2, port_2_character, slp_file\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_path = './Slippi_Public_Dataset_v3/'\n",
    "    frames_per_input = 300  # You'll need to define this variable\n",
    "\n",
    "    slp_files = [file for file in os.listdir(dataset_path) if file.endswith('.slp') and 'Sheik' in file and 'Fox' in file]\n",
    "    print(f\"Total files to process: {len(slp_files)}\")\n",
    "\n",
    "    time_series_list = []\n",
    "    label_list = []\n",
    "    ids = []\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "\n",
    "    # Process slp files in parallel\n",
    "    results = pool.map(process_slp_file, slp_files)\n",
    "\n",
    "    # Close the pool of worker processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Filter out the None values from the results and extract the data\n",
    "    results = [result for result in results if result is not None]\n",
    "    for frame_data1, port_1_character, frame_data2, port_2_character, slp_file in results:\n",
    "        time_series_list.append(frame_data1)\n",
    "        label_list.append(1 if port_1_character == 'SHEIK' else 0)\n",
    "        ids.append(slp_file)\n",
    "        time_series_list.append(frame_data2)\n",
    "        label_list.append(1 if port_2_character == 'SHEIK' else 0)\n",
    "        ids.append(slp_file)\n",
    "\n",
    "    df = pd.DataFrame({\"TimeSeries\": time_series_list, \"Label\": label_list, \"FName\": ids})\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df[df.duplicated(subset = 'TimeSeries', keep = False)]\n",
    "\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Preprocessing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "time_series_tensor = torch.tensor(np.array(time_series_list), dtype=torch.float32)\n",
    "label_tensor = torch.tensor(label_list, dtype=torch.float32)\n",
    "\n",
    "channels = 18\n",
    "\n",
    "# Normalize each channel individually\n",
    "scaler = StandardScaler()\n",
    "time_series_normalized = torch.zeros(time_series_tensor.shape)\n",
    "\n",
    "# Iterate over channels\n",
    "# for i in range(channels):\n",
    "#     time_series_normalized[:, :, i] = torch.tensor(scaler.fit_transform(time_series_tensor[:, :, i]))\n",
    "\n",
    "# print(time_series_sensor.shape)\n",
    "# print(time_series_normalized.shape)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(time_series_normalized, label_tensor, test_size = 0.2, shuffle = True, stratify = label_tensor)\n",
    "\n",
    "print(torch.isnan(time_series_normalized).any())\n",
    "print(torch.isnan(label_tensor).any())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
